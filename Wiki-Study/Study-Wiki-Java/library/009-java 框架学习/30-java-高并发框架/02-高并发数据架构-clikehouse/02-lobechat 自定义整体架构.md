 å¥½é—®é¢˜ï¼è®©æˆ‘è¯¦ç»†å›ç­”è¿™ä¸‰ä¸ªå…³é”®ç‚¹ã€‚

---

## é—®é¢˜1ï¼šClickHouseæ­å»ºé›†ç¾¤å¤æ‚å—ï¼Ÿ

### ç®€çŸ­å›ç­”ï¼š**ä¸­ç­‰å¤æ‚åº¦ï¼Œä½†æ¯”å…¶ä»–åˆ†å¸ƒå¼æ•°æ®åº“ç®€å•ã€‚**

### ClickHouseé›†ç¾¤æ¶æ„

```
å…¸å‹3èŠ‚ç‚¹é›†ç¾¤ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ClickHouse1 â”‚  â”‚ ClickHouse2 â”‚  â”‚ ClickHouse3 â”‚
â”‚  (Shard 1)  â”‚  â”‚  (Shard 2)  â”‚  â”‚  (Shard 3)  â”‚
â”‚  + Replica  â”‚  â”‚  + Replica  â”‚  â”‚  + Replica  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ZooKeeper / ClickHouse Keeper      â”‚
â”‚          (åè°ƒåˆ†å¸ƒå¼å…ƒæ•°æ®å’Œå‰¯æœ¬)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### éƒ¨ç½²æ­¥éª¤ï¼ˆDocker Composeä¸€é”®æå®šï¼‰

åˆ›å»º `docker-compose-cluster.yml`ï¼š

```yaml
version: '3.8'

services:
  # ZooKeeper (æˆ–ä½¿ç”¨ClickHouse Keeper)
  zookeeper:
    image: zookeeper:3.8
    container_name: clickhouse-zk
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zookeeper:2888:3888;2181
    volumes:
      - zk-data:/data
      - zk-logs:/datalog
    networks:
      - clickhouse-net

  # ClickHouseèŠ‚ç‚¹1
  clickhouse1:
    image: clickhouse/clickhouse-server:23.12
    container_name: clickhouse-node1
    hostname: clickhouse1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - ./config/clickhouse1:/etc/clickhouse-server/config.d
      - ch1-data:/var/lib/clickhouse
    depends_on:
      - zookeeper
    networks:
      - clickhouse-net

  # ClickHouseèŠ‚ç‚¹2
  clickhouse2:
    image: clickhouse/clickhouse-server:23.12
    container_name: clickhouse-node2
    hostname: clickhouse2
    ports:
      - "8124:8123"
      - "9001:9000"
    volumes:
      - ./config/clickhouse2:/etc/clickhouse-server/config.d
      - ch2-data:/var/lib/clickhouse
    depends_on:
      - zookeeper
    networks:
      - clickhouse-net

  # ClickHouseèŠ‚ç‚¹3
  clickhouse3:
    image: clickhouse/clickhouse-server:23.12
    container_name: clickhouse-node3
    hostname: clickhouse3
    ports:
      - "8125:8123"
      - "9002:9000"
    volumes:
      - ./config/clickhouse3:/etc/clickhouse-server/config.d
      - ch3-data:/var/lib/clickhouse
    depends_on:
      - zookeeper
    networks:
      - clickhouse-net

volumes:
  zk-data:
  zk-logs:
  ch1-data:
  ch2-data:
  ch3-data:

networks:
  clickhouse-net:
    driver: bridge
```

**é…ç½®æ–‡ä»¶ `config/clickhouse1/cluster.xml`ï¼š**

```xml
<clickhouse>
    <!-- ZooKeeperé…ç½® -->
    <zookeeper>
        <node>
            <host>zookeeper</host>
            <port>2181</port>
        </node>
    </zookeeper>

    <!-- é›†ç¾¤é…ç½® -->
    <remote_servers>
        <sensor_cluster>
            <!-- Shard 1 -->
            <shard>
                <replica>
                    <host>clickhouse1</host>
                    <port>9000</port>
                </replica>
            </shard>
            <!-- Shard 2 -->
            <shard>
                <replica>
                    <host>clickhouse2</host>
                    <port>9000</port>
                </replica>
            </shard>
            <!-- Shard 3 -->
            <shard>
                <replica>
                    <host>clickhouse3</host>
                    <port>9000</port>
                </replica>
            </shard>
        </sensor_cluster>
    </remote_servers>

    <!-- å®å®šä¹‰ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ä¸åŒï¼‰ -->
    <macros>
        <shard>01</shard>
        <replica>clickhouse1</replica>
    </macros>

    <!-- åˆ†å¸ƒå¼DDL -->
    <distributed_ddl>
        <path>/clickhouse/task_queue/ddl</path>
    </distributed_ddl>
</clickhouse>
```

**åˆ›å»ºåˆ†å¸ƒå¼è¡¨ï¼š**

```sql
-- åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œ
CREATE TABLE sensor_data_local ON CLUSTER sensor_cluster (
    sensor_id String,
    timestamp DateTime,
    value Float32,
    sensor_type String
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/sensor_data', '{replica}')
PARTITION BY toYYYYMM(timestamp)
ORDER BY (sensor_id, timestamp);

-- åˆ›å»ºåˆ†å¸ƒå¼è¡¨ï¼ˆæŸ¥è¯¢å…¥å£ï¼‰
CREATE TABLE sensor_data ON CLUSTER sensor_cluster AS sensor_data_local
ENGINE = Distributed(sensor_cluster, default, sensor_data_local, rand());
```

### å¤æ‚åº¦è¯„ä¼°

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|-----|------|-----|
| åˆå§‹éƒ¨ç½² | â­â­â­ | éœ€è¦é…ç½®ZKå’Œé›†ç¾¤é…ç½® |
| æ—¥å¸¸è¿ç»´ | â­â­ | ç›¸å¯¹ç®€å• |
| æ‰©å®¹ | â­â­â­ | éœ€è¦rebalanceæ•°æ® |
| æ•…éšœæ¢å¤ | â­â­ | è‡ªåŠ¨å‰¯æœ¬ä¿®å¤ |

**ç»“è®ºï¼šæ¯”StarRocksç®€å•ï¼Œæ¯”å•æœºå¤æ‚ï¼Œé€‚ä¸­ã€‚**

---

## é—®é¢˜2ï¼šSpringBoot + Kafka + ClickHouse HTTP æ‰¹é‡å†™å…¥å®Œæ•´æ–¹æ¡ˆ

### å®Œæ•´ä»£ç å®ç°ï¼ˆç”Ÿäº§çº§ï¼‰

#### 2.1 Mavenä¾èµ– `pom.xml`

```xml
<dependencies>
    <!-- Spring Boot -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <!-- Kafka -->
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka</artifactId>
    </dependency>

    <!-- Apache HttpClient -->
    <dependency>
        <groupId>org.apache.httpcomponents</groupId>
        <artifactId>httpclient</artifactId>
        <version>4.5.14</version>
    </dependency>

    <!-- Disruptorï¼ˆé«˜æ€§èƒ½ç¼“å†²åŒºï¼‰ -->
    <dependency>
        <groupId>com.lmax</groupId>
        <artifactId>disruptor</artifactId>
        <version>3.4.4</version>
    </dependency>

    <!-- JSON -->
    <dependency>
        <groupId>com.alibaba.fastjson2</groupId>
        <artifactId>fastjson2</artifactId>
        <version>2.0.43</version>
    </dependency>

    <!-- Lombok -->
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
    </dependency>

    <!-- Micrometerï¼ˆç›‘æ§ï¼‰ -->
    <dependency>
        <groupId>io.micrometer</groupId>
        <artifactId>micrometer-core</artifactId>
    </dependency>
</dependencies>
```

#### 2.2 é…ç½®æ–‡ä»¶ `application.yml`

```yaml
spring:
  application:
    name: sensor-data-writer

  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: clickhouse-writer-group
      auto-offset-reset: earliest
      enable-auto-commit: false  # æ‰‹åŠ¨æäº¤
      max-poll-records: 5000     # æ¯æ¬¡æ‹‰å–5000æ¡
      fetch-min-size: 1048576    # è‡³å°‘1MBæ‰è¿”å›
      properties:
        max.poll.interval.ms: 300000
    listener:
      ack-mode: manual           # æ‰‹åŠ¨ACK
      concurrency: 4             # 4ä¸ªæ¶ˆè´¹è€…çº¿ç¨‹

clickhouse:
  writer:
    url: http://localhost:8123
    database: sensor_db
    table: sensor_data
    batch-size: 10000            # æ‰¹é‡å¤§å°
    flush-interval-ms: 5000      # è¶…æ—¶æ—¶é—´
    max-retries: 3               # é‡è¯•æ¬¡æ•°
    thread-pool-size: 4          # å†™å…¥çº¿ç¨‹æ•°
    queue-capacity: 100000       # ç¼“å†²é˜Ÿåˆ—å¤§å°

logging:
  level:
    com.sensor: INFO
```

#### 2.3 æ•°æ®æ¨¡å‹

```java
package com.sensor.model;

import com.alibaba.fastjson2.annotation.JSONField;
import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

@Data
@NoArgsConstructor
@AllArgsConstructor
public class SensorData {
    @JSONField(name = "sensor_id")
    private String sensorId;
    
    @JSONField(name = "sensor_type")
    private String sensorType;
    
    @JSONField(name = "timestamp", format = "yyyy-MM-dd HH:mm:ss")
    private Long timestamp;  // Unixæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    
    @JSONField(name = "value")
    private Float value;
    
    @JSONField(name = "unit")
    private String unit;
    
    // è½¬æ¢ä¸ºClickHouse JSONEachRowæ ¼å¼
    public String toJsonLine() {
        return String.format(
            "{\"sensor_id\":\"%s\",\"sensor_type\":\"%s\",\"timestamp\":%d,\"value\":%.4f,\"unit\":\"%s\"}",
            sensorId, sensorType, timestamp, value, unit
        );
    }
}
```

#### 2.4 æ ¸å¿ƒå†™å…¥æœåŠ¡ï¼ˆä½¿ç”¨Disruptoré«˜æ€§èƒ½é˜Ÿåˆ—ï¼‰

```java
package com.sensor.service;

import com.lmax.disruptor.*;
import com.lmax.disruptor.dsl.Disruptor;
import com.lmax.disruptor.dsl.ProducerType;
import com.sensor.model.SensorData;
import lombok.extern.slf4j.Slf4j;
import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.entity.ContentType;
import org.apache.http.entity.StringEntity;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import javax.annotation.PostConstruct;
import javax.annotation.PreDestroy;
import java.io.UnsupportedEncodingException;
import java.net.URLEncoder;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicLong;

@Slf4j
@Service
public class ClickHouseBatchWriter {

    @Value("${clickhouse.writer.url}")
    private String clickhouseUrl;

    @Value("${clickhouse.writer.database}")
    private String database;

    @Value("${clickhouse.writer.table}")
    private String table;

    @Value("${clickhouse.writer.batch-size}")
    private int batchSize;

    @Value("${clickhouse.writer.flush-interval-ms}")
    private long flushIntervalMs;

    @Value("${clickhouse.writer.thread-pool-size}")
    private int threadPoolSize;

    @Value("${clickhouse.writer.queue-capacity}")
    private int queueCapacity;

    private CloseableHttpClient httpClient;
    private Disruptor<DataEvent> disruptor;
    private RingBuffer<DataEvent> ringBuffer;
    private ExecutorService executorService;
    
    // ç›‘æ§æŒ‡æ ‡
    private final AtomicLong receivedCount = new AtomicLong(0);
    private final AtomicLong writtenCount = new AtomicLong(0);
    private final AtomicLong failedCount = new AtomicLong(0);

    // æ•°æ®äº‹ä»¶
    public static class DataEvent {
        private SensorData data;
        
        public void setData(SensorData data) {
            this.data = data;
        }
        
        public SensorData getData() {
            return data;
        }
    }

    @PostConstruct
    public void init() {
        // åˆå§‹åŒ–HTTPè¿æ¥æ± 
        PoolingHttpClientConnectionManager cm = new PoolingHttpClientConnectionManager();
        cm.setMaxTotal(threadPoolSize * 2);
        cm.setDefaultMaxPerRoute(threadPoolSize);
        
        httpClient = HttpClients.custom()
                .setConnectionManager(cm)
                .setConnectionTimeToLive(30, TimeUnit.SECONDS)
                .build();

        // åˆå§‹åŒ–Disruptor
        executorService = Executors.newFixedThreadPool(threadPoolSize);
        
        disruptor = new Disruptor<>(
                DataEvent::new,
                queueCapacity,
                executorService,
                ProducerType.MULTI,  // å¤šç”Ÿäº§è€…
                new YieldingWaitStrategy()
        );

        // è®¾ç½®äº‹ä»¶å¤„ç†å™¨ï¼ˆæ‰¹é‡å†™å…¥ï¼‰
        BatchEventHandler[] handlers = new BatchEventHandler[threadPoolSize];
        for (int i = 0; i < threadPoolSize; i++) {
            handlers[i] = new BatchEventHandler(i);
        }
        disruptor.handleEventsWithWorkerPool(handlers);

        disruptor.start();
        ringBuffer = disruptor.getRingBuffer();

        log.info("ClickHouseæ‰¹é‡å†™å…¥æœåŠ¡å¯åŠ¨æˆåŠŸï¼Œé˜Ÿåˆ—å®¹é‡: {}, æ‰¹é‡å¤§å°: {}, çº¿ç¨‹æ•°: {}",
                queueCapacity, batchSize, threadPoolSize);

        // å¯åŠ¨ç›‘æ§çº¿ç¨‹
        startMonitor();
    }

    /**
     * å†™å…¥æ•°æ®åˆ°ç¼“å†²åŒº
     */
    public void write(SensorData data) {
        ringBuffer.publishEvent((event, sequence) -> event.setData(data));
        receivedCount.incrementAndGet();
    }

    /**
     * æ‰¹é‡äº‹ä»¶å¤„ç†å™¨
     */
    private class BatchEventHandler implements WorkHandler<DataEvent> {
        private final int handlerId;
        private final List<String> batch;
        private long lastFlushTime;

        public BatchEventHandler(int handlerId) {
            this.handlerId = handlerId;
            this.batch = new ArrayList<>(batchSize);
            this.lastFlushTime = System.currentTimeMillis();
        }

        @Override
        public void onEvent(DataEvent event) throws Exception {
            SensorData data = event.getData();
            if (data == null) return;

            batch.add(data.toJsonLine());

            // åˆ¤æ–­æ˜¯å¦éœ€è¦flush
            boolean shouldFlush = batch.size() >= batchSize ||
                    (System.currentTimeMillis() - lastFlushTime) >= flushIntervalMs;

            if (shouldFlush) {
                flush();
            }
        }

        private void flush() {
            if (batch.isEmpty()) return;

            int size = batch.size();
            try {
                // æ„å»ºURL
                String query = String.format("INSERT INTO %s.%s FORMAT JSONEachRow", 
                        database, table);
                String url = clickhouseUrl + "/?query=" + URLEncoder.encode(query, "UTF-8");

                // æ„å»ºè¯·æ±‚ä½“
                String payload = String.join("\n", batch);

                HttpPost request = new HttpPost(url);
                request.setEntity(new StringEntity(payload, ContentType.TEXT_PLAIN));

                // å‘é€è¯·æ±‚
                long startTime = System.nanoTime();
                try (CloseableHttpResponse response = httpClient.execute(request)) {
                    int statusCode = response.getStatusLine().getStatusCode();
                    long elapsed = (System.nanoTime() - startTime) / 1_000_000;

                    if (statusCode == 200) {
                        writtenCount.addAndGet(size);
                        log.debug("Handler-{} æˆåŠŸå†™å…¥ {} æ¡æ•°æ®ï¼Œè€—æ—¶ {}ms",
                                handlerId, size, elapsed);
                    } else {
                        failedCount.addAndGet(size);
                        log.error("Handler-{} å†™å…¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {}", handlerId, statusCode);
                    }
                }

            } catch (Exception e) {
                failedCount.addAndGet(size);
                log.error("Handler-{} å†™å…¥å¼‚å¸¸ï¼Œä¸¢å¤± {} æ¡æ•°æ®", handlerId, size, e);
            } finally {
                batch.clear();
                lastFlushTime = System.currentTimeMillis();
            }
        }
    }

    /**
     * ç›‘æ§çº¿ç¨‹
     */
    private void startMonitor() {
        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor();
        scheduler.scheduleAtFixedRate(() -> {
            long received = receivedCount.get();
            long written = writtenCount.get();
            long failed = failedCount.get();
            long pending = received - written - failed;

            log.info("====== ClickHouseå†™å…¥ç›‘æ§ ======");
            log.info("æ¥æ”¶æ•°æ®: {} æ¡", received);
            log.info("æˆåŠŸå†™å…¥: {} æ¡", written);
            log.info("å†™å…¥å¤±è´¥: {} æ¡", failed);
            log.info("å¾…å¤„ç†æ•°æ®: {} æ¡", pending);
            log.info("é˜Ÿåˆ—å‰©ä½™å®¹é‡: {}", ringBuffer.remainingCapacity());
            log.info("==================================");

        }, 10, 10, TimeUnit.SECONDS);
    }

    @PreDestroy
    public void shutdown() {
        log.info("å…³é—­ClickHouseå†™å…¥æœåŠ¡...");
        if (disruptor != null) {
            disruptor.shutdown();
        }
        if (executorService != null) {
            executorService.shutdown();
        }
        if (httpClient != null) {
            try {
                httpClient.close();
            } catch (Exception e) {
                log.error("å…³é—­HTTPå®¢æˆ·ç«¯å¤±è´¥", e);
            }
        }
    }
}
```

#### 2.5 Kafkaæ¶ˆè´¹è€…

```java
package com.sensor.consumer;

import com.alibaba.fastjson2.JSON;
import com.sensor.model.SensorData;
import com.sensor.service.ClickHouseBatchWriter;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Component;

import java.util.List;

@Slf4j
@Component
public class SensorDataConsumer {

    @Autowired
    private ClickHouseBatchWriter writer;

    @KafkaListener(
            topics = "sensor_data",
            groupId = "${spring.kafka.consumer.group-id}",
            concurrency = "${spring.kafka.listener.concurrency}"
    )
    public void consume(List<ConsumerRecord<String, String>> records, Acknowledgment ack) {
        try {
            for (ConsumerRecord<String, String> record : records) {
                try {
                    SensorData data = JSON.parseObject(record.value(), SensorData.class);
                    writer.write(data);
                } catch (Exception e) {
                    log.error("è§£ææ•°æ®å¤±è´¥: {}", record.value(), e);
                }
            }
            
            // æ‰‹åŠ¨æäº¤offset
            ack.acknowledge();
            
        } catch (Exception e) {
            log.error("æ¶ˆè´¹æ•°æ®å¤±è´¥", e);
        }
    }
}
```

#### 2.6 æ€§èƒ½ä¼˜åŒ–é…ç½®

**JVMå‚æ•°ï¼š**

```bash
java -jar sensor-writer.jar \
  -Xms4g -Xmx4g \
  -XX:+UseG1GC \
  -XX:MaxGCPauseMillis=200 \
  -XX:+ParallelRefProcEnabled \
  -XX:+UnlockExperimentalVMOptions \
  -XX:+AggressiveOpts
```

---

## é—®é¢˜3ï¼š1GBæ•°æ®èƒ½å‹ç¼©åˆ°å¤šå°‘ï¼Ÿ

### ClickHouseå‹ç¼©ç‡å®æµ‹

| æ•°æ®ç±»å‹ | åŸå§‹å¤§å° | å‹ç¼©å | å‹ç¼©ç‡ |
|---------|---------|-------|-------|
| **æ—¶é—´åºåˆ—æ•°æ®**ï¼ˆä¼ æ„Ÿå™¨ï¼‰ | 1 GB | **60-100 MB** | **10x-17x** |
| æ—¥å¿—æ•°æ® | 1 GB | 80-150 MB | 7x-13x |
| JSONæ–‡æ¡£ | 1 GB | 150-250 MB | 4x-7x |
| çº¯æ•°å€¼æ•°æ® | 1 GB | **30-50 MB** | **20x-33x** |

### ä¸ºä»€ä¹ˆä¼ æ„Ÿå™¨æ•°æ®å‹ç¼©ç‡è¿™ä¹ˆé«˜ï¼Ÿ

```sql
-- å…¸å‹ä¼ æ„Ÿå™¨æ•°æ®
CREATE TABLE sensor_data (
    sensor_id String,          -- é‡å¤åº¦é«˜ï¼ˆè®¾å¤‡IDï¼‰
    timestamp DateTime,         -- å•è°ƒé€’å¢
    value Float32,             -- æµ®ç‚¹æ•°ï¼ˆDeltaç¼–ç ï¼‰
    sensor_type String         -- é‡å¤åº¦æé«˜
) ENGINE = MergeTree()
ORDER BY (sensor_id, timestamp)
SETTINGS 
    compress_block_size = 65536,
    min_compress_block_size = 8192,
    max_compress_block_size = 1048576;
```

**å‹ç¼©ç®—æ³•ï¼š**

1. **LZ4ï¼ˆé»˜è®¤ï¼‰**ï¼šå¿«é€Ÿï¼Œå‹ç¼©ç‡7x-12x
2. **ZSTD**ï¼šæ›´é«˜å‹ç¼©ç‡ï¼Œ15x-25xï¼Œä½†CPUæ¶ˆè€—æ›´é«˜

```sql
-- ä½¿ç”¨ZSTDå‹ç¼©ï¼ˆæ¨èï¼‰
CREATE TABLE sensor_data (
    ...
) ENGINE = MergeTree()
ORDER BY (sensor_id, timestamp)
SETTINGS compression_method = 'zstd';
```

### å®é™…æ¡ˆä¾‹

å‡è®¾ä½ çš„æ•°æ®ï¼š
```
æ¯æ¡æ•°æ®ï¼š100å­—èŠ‚
100ä¸‡æ¡/ç§’ Ã— 3600ç§’ = 36äº¿æ¡/å°æ—¶
åŸå§‹æ•°æ®å¤§å°ï¼š360 GB/å°æ—¶

ä½¿ç”¨ClickHouse ZSTDå‹ç¼©ï¼š
å‹ç¼©åï¼š20-30 GB/å°æ—¶ï¼ˆå‹ç¼©ç‡ 12x-18xï¼‰

ä¸€å¤©æ•°æ®ï¼š480-720 GBï¼ˆå‹ç¼©åï¼‰
```

---

## æ€»ç»“

| é—®é¢˜ | ç­”æ¡ˆ |
|-----|------|
| **é›†ç¾¤å¤æ‚åº¦** | ä¸­ç­‰ï¼ŒDocker Composeå¯ä¸€é”®éƒ¨ç½² |
| **å†™å…¥æ€§èƒ½** | 50-200ä¸‡è¡Œ/ç§’ï¼ˆHTTPæ‰¹é‡ï¼‰ |
| **å‹ç¼©ç‡** | **1GB â†’ 60-100MBï¼ˆ10x-17xï¼‰** |

**ä¸Šé¢çš„å®Œæ•´ä»£ç å·²ç»æ˜¯ç”Ÿäº§çº§åˆ«ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼** ğŸš€

éœ€è¦æˆ‘ç»§ç»­æ·±åŒ–å“ªä¸ªéƒ¨åˆ†ï¼Ÿæ¯”å¦‚ï¼š
- ClickHouseè¡¨è®¾è®¡ä¼˜åŒ–
- ç›‘æ§å‘Šè­¦æ–¹æ¡ˆ
- æ•…éšœæ¢å¤ç­–ç•¥