# 记录公共总结



##   大体总结


###2018.07.22_0 > 2018.07.22_1 周日

```
1. confluence、Jira、git
2.  开发环境考虑建3个sftp，模拟运营商边界服务器，公安网边界服务器和公安网内部ftp服务器
3 .所有研发同学，从10.0开始使用git作为代码版本管理工具，jira作为项目管理工具
```


### 2018.07.09_0 > 2018.07.09_1 周一


```
1. whoTextFiles()返回的是javaPairRDD, key 是每个文件所在的路径
2. 为解决Agent功能（虚拟资源分配）需求，华为UAP计划进行补丁升级。
3. BDI调度 影响集群
```


### 2018.07.05_0 > 2018.07.05_1 周一

```
1. gitlab 代码服务器
```

### 2018.07.04_0 > 2018.07.04_1 周一

```
1. pom 文档中junit存在则没有办法 plugins 插件打包！
2. maven-tomcat
3. maven 找不到包：https://blog.csdn.net/chengmeng_123/article/details/78641698
```

#### 2018.07.04_1 周一

```

```


### 2018.07.02_0 > 2018.07.02_1 周一

```
1. 使用maven 打包报错 ：https://blog.csdn.net/u012027337/article/details/51741372
    maven-surefire-plugin  这个插件先关的话可以吧项目中有关test Test这样命名的类给修改一下名字，或者修改一下配置，具体博客如上
    >> 结论 mvn 打包 方式如果有test 之类的明明会有问题解决
      1. 删除类似test的文件
      2. 换一种方式打包 mvn Plugins方式

```
#### 2018.07.02_1

surefire-plugin 插件
```
1. surefire-plugin 插件

1.maven-surefire-plugin简介
Maven本身并不是一个单元测试框架，它只是在构建执行到特定生命周期阶段的时候，通过插件来执行JUnit或者TestNG的测试用例。这个插件就是maven-surefire-plugin，也可以称为测试运行器(Test Runner)，它能兼容JUnit 3、JUnit 4以及TestNG。

在默认情况下，maven-surefire-plugin的test目标会自动执行测试源码路径（默认为src/test/Java/）下所有符合一组命名模式的测试类。

默认情况下，surefire会执行文件名以Test开头或结尾的测试用例，或者是以TestCase结尾的测试用例。
这组模式为：
**/Test*.java：任何子目录下所有命名以Test开关的Java类。
**/*Test.java：任何子目录下所有命名以Test结尾的Java类。
**/*TestCase.java：任何子目录下所有命名以TestCase结尾的Java类。
2.maven-surefire-plugin的使用
如果说maven意见有了maven-surefire-plugin的默认配置，我们还有必要了解maven-surefire-plugin的配置么？答案是肯定的。虽说maven-surefire-plugin有默认配置，但是当需要修改一些测试执行的策略时，就有必要我们去重新配置这个插件了。

2.1.配置JUnit
2.1.1.插件自动匹配
最简单的配置方式就不配置或者是只声明插件。

    <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-surefire-plugin</artifactId>
        <version>2.19</version>
    </plugin>
2.1.2.插件手动匹配
当然，如果你明确用的是JUnit4.7及以上版本，可以明确声明：

<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-surefire-plugin</artifactId>
    <version>2.19</version>
    <dependencies>
        <dependency>
            <groupId>org.apache.maven.surefire</groupId>
            <artifactId>surefire-junit47</artifactId>
            <version>2.19</version>
        </dependency>
    </dependencies>
</plugin>
3.包含与排除测试用例
如果由于历史原因，测试类不符合默认的三种命名模式，可以通过pom.xml设置maven-surefire-plugin插件添加命名模式或排除一些命名模式。
[html] view plain copy

<plugin>  
    <groupId>org.apache.maven.plugins</groupId>  
    <artifactId>maven-surefire-plugin</artifactId>  
    <version>2.5</version>  
    <configuration>  
        <includes>  
            <include>**/*Tests.java</include>  
        </includes>  
        <excludes>  
            <exclude>**/*ServiceTest.java</exclude>  
            <exclude>**/TempDaoTest.java</exclude>  
        </excludes>  
    </configuration>  
</plugin>  

```


### 2018.06.29_0 > 2018.06.29_1 周五

```
* maven lifecylc / plugs  ： http://lixh1986.iteye.com/blog/2383960
* base64 编码解码：content = new String(Base64.decodeBase64(content.getBytes("UTF-8")));
```

### 2018.06.28_0 > 2018.06.28_1 周四

```
1. java环境变量再改无效 win/system32s中的优先于java_home中设置的环境变量
2. 阿里代码规约检测，造成了if 后面一行代码也报错
3. idea 打开多个项目:原文地址（https://blog.csdn.net/zht666/article/details/47831893/）
4. idea 多个项目共同调用代码
5. 重定向linux输入输出 :https://blog.csdn.net/u012656834/article/details/41310237
```

### 2018.06.28_1 详解

#### 1-java 环境变量问题：

原文地址：https://blog.csdn.net/jijianshuai/article/details/72638804
```
当使用安装版本JDK后，想要更改系统环境变量时，直接更改JAVA_HOME无效

原因：

当使用安装版本的JDK程序时（一般是1.7版本以上），在安装结束后安装程序会自动将java.exe、javaw.exe、javaws.exe三个可执行文件复制到C:\Windows\System32目录，这个目录在WINDOWS环境变量中的优先级高于JAVA_HOME设置的环境变量优先级，故此直接更改JAVA_HOME会无效。

另外，JDK1.8安装版本，还会在C:\ProgramData\Oracle\Java目录中生成一些配置文件，并同时将此目录写到环境变量中的Path中。

解决方案：

    删除C:\Windows\System32目录下java.exe、javaw.exe、javaws.exe三个文件。

    删除Path中C:\ProgramData\Oracle\Java\javapath配置

```


### 2018.06.27_0 > 2018.06.27_1 周三

```
1. 1. java.lang.AutoCloseable 类的使用
2. nginx反向代理+ssl? 没搞过啊
```


#### 2018.06.27_1

##### 1- AutoCloseable 类的使用
```Java
/**
 * 加载log4j配置
 */
public class LoadLogConfig {

    public static void loadByLocal(String path) {
        FileInputStream logConfigInputStream = null;
        try {
            //加载log4j配置
            logConfigInputStream = new FileInputStream(path);
            PropertyConfigurator.configure(logConfigInputStream);
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            BaseCloseable.close(logConfigInputStream);
        }
    }
}
通用关闭类：
/**
 * 通用关闭类
 * Created by jado on 2017/8/4.
 */
public class BaseCloseable {
    /**
     * 关闭操作
     *
     * @param closeable 实现的接口类
     */
    public static void close(AutoCloseable closeable) {
        if (closeable != null) {
            try {
                closeable.close();
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }
}


```




### 2018.06.26_0 > 2018.06.26_1 周二
```
1. String.format 用法

```

### 2018.06.26_1

#### 1-Strng.format

```
tmp = String.format("%s|%s|%s|%s|%s|%s|%s|%s|%s|%s|%s|%s|%s|%s",
        							task, msg_code, content, cir_lng, cir_lat, r, start_time, end_time, choose, sport,
									road, td_list, event_lng, event_lat);

```

### 2018.06.25_0 > 2018.06.25_1  周一

```
1. RSAUtils.class.getClassLoader().getResourceAsStream(fileName)

2. mysql 备份，查看版本

```



### 2018.06.25_1  周一

#### 1-类名.class.getClassLoader().getResourceAsStream()  解释

[博客地址1 ](http://www.mamicode.com/info-detail-292312.html)

#### 2-mysql 备份方式、查看版本


[mysql 三种备份方式](https://www.cnblogs.com/frankielf0921/p/5924365.html)

[mysql备份命令](https://www.cnblogs.com/Cherie/p/3309456.html)

```
一、备份常用操作基本命令

1、备份命令mysqldump格式

   格式：mysqldump -h主机名  -P端口 -u用户名 -p密码 –database 数据库名 > 文件名.sql

2、备份MySQL数据库为带删除表的格式

备份MySQL数据库为带删除表的格式，能够让该备份覆盖已有数据库而不需要手动删除原有数据库。

mysqldump  --add-drop-table -uusername -ppassword -database databasename > backupfile.sql

3、直接将MySQL数据库压缩备份

mysqldump -hhostname -uusername -ppassword -database databasename | gzip > backupfile.sql.gz

4、备份MySQL数据库某个(些)表

mysqldump -hhostname -uusername -ppassword databasename specific_table1 specific_table2 > backupfile.sql

5、同时备份多个MySQL数据库

mysqldump -hhostname -uusername -ppassword –databases databasename1 databasename2 databasename3 > multibackupfile.sql仅仅备6、仅备份份数据库结构

mysqldump –no-data –databases databasename1 databasename2 databasename3 > structurebackupfile.sql

7、备份服务器上所有数据库

mysqldump –all-databases > allbackupfile.sql

8、还原MySQL数据库的命令

mysql -hhostname -uusername -ppassword databasename < backupfile.sql

9、还原压缩的MySQL数据库

gunzip < backupfile.sql.gz | mysql -uusername -ppassword databasename

10、将数据库转移到新服务器

mysqldump -uusername -ppassword databasename | mysql –host=*.*.*.* -C databasename

11、--master-data 和--single-transaction

   在mysqldump中使用--master-data=2，会记录binlog文件和position的信息 。--single-transaction会将隔离级别设置成repeatable-commited

12、导入数据库

常用source命令，用use进入到某个数据库，mysql>source d:\test.sql，后面的参数为脚本文件。

13、查看binlog日志

查看binlog日志可用用命令 mysqlbinlog  binlog日志名称|more



14、general_log

General_log记录数据库的任何操作，查看general_log 的状态和位置可以用命令show variables like "general_log%"  ,开启general_log可以用命令set global general_log=on
二、增量备份

小量的数据库可以每天进行完整备份，因为这也用不了多少时间，但当数据库很大时，就不太可能每天进行一次完整备份了，这时候就可以使用增量备份。增量备份的原理就是使用了mysql的binlog志。



1、首先做一次完整备份：

mysqldump -h10.6.208.183 -utest2 -p123  -P3310 --single-transaction  --master-data=2  test>test.sql这时候就会得到一个全备文件test.sql

在sql文件中我们会看到：
-- CHANGE MASTER TO MASTER_LOG_FILE='bin-log.000002', MASTER_LOG_POS=107;是指备份后所有的更改将会保存到bin-log.000002二进制文件中。
2、在test库的t_student表中增加两条记录，然后执行flush logs命令。这时将会产生一个新的二进制日志文件bin-log.000003，bin-log.000002则保存了全备过后的所有更改，既增加记录的操作也保存在了bin-log.00002中。

3、再在test库中的a表中增加两条记录，然后误删除t_student表和a表。a中增加记录的操作和删除表a和t_student的操作都记录在bin-log.000003中。


三、恢复

1、首先导入全备数据

mysql -h10.6.208.183 -utest2 -p123  -P3310 < test.sql，也可以直接在mysql命令行下面用source导入

2、恢复bin-log.000002

   mysqlbinlog bin-log.000002 |mysql -h10.6.208.183 -utest2 -p123  -P3310  

3、恢复部分 bin-log.000003

   在general_log中找到误删除的时间点，然后更加对应的时间点到bin-log.000003中找到相应的position点，需要恢复到误删除的前面一个position点。

可以用如下参数来控制binlog的区间

--start-position 开始点 --stop-position 结束点

--start-date 开始时间  --stop-date  结束时间

找到恢复点后，既可以开始恢复。

  mysqlbinlog mysql-bin.000003 --stop-position=208 |mysql -h10.6.208.183 -utest2 -p123  -P3310

```
___


```

在Java Web等工程中，我们可以使用下面的方式来加载一个文件流

InputStream in=DBUtil.class.getResourceAsStream("database.properties")


此时，database.properties 和 DBUtil 在同一级目录下面。
中文解释：
通过一个名称找到资源。搜索规则跟给定的类有关系，它是通过定义这个类的类加载器来实现的。
这个方法会委派给这个对象的类加载器，如果这个对象是由启动类加载器加载的，那么方法就会为派给ClassLoader.getSystemResourceAsStream
在委派之前，一个资源路径是由给定的资源名称通过下面的算法来构造的：
如果资源的名称由/开头，那么资源的绝对名称就是/后面的那一部分，
否则的话，绝对名称就是：modified_package_name/name
modified_package_name是这个对象的包名用’/’替代’.’，因为包的形式是com.abc.ClassA.class这样的形式，实际上是com/abc/ClassA.class

总结：
也就是说，只要资源和类在同一级目录下面，无论资源名称的前面加不加/或./都可以找到。

```
##### 查看mysql版本
[查看mysql版本](https://www.jb51.net/article/36370.htm)
```

```


### 2018.06.22_0 > 2018.06.22_1  周五

```
1. idea 自动导入包， setting——》 auto inport -> 找到java 勾选auto fly 那个选项


```
### 2018.06.22_1

```


```


### 2018.06.21_0 > 2018.06.21_1
```

----------------------
1. System.setProperty("")

3. spark调优： spark_default_parallelism
4. System.geroperty（）使用 user.dir 项目所在目录
5. Preconditions.checkArgument(StringUtils.isNotBlank(brokerList), "kafka brokerList is blank...");
6. 生产者生产
```

---
分割线
---

----------------------
### 2018.06.20_0 -> 2018.06.20_1
----------------------

```
1. oracel 去除javascritp 从未来的jdk中链接地址 （http://www.infoq.com/cn/news/2018/06/deprecate-nashorn?utm_source=notification_email&utm_campaign=notifications&utm_medium=link&utm_content=content_in_followed_topic&utm_term=daily）
不过有替换方案 ：graalVM

2. hbase list 有问题

[博主地址](https://www.cnblogs.com/zlslch/p/6556870.html)
3. postgrasql
4. quartz 任务调度学习

```


## 分总结


### 2018.06.21_1

```
 1. spark spark_default_parallelism 调优配置

 控制生成的task数量，一个分区一个task

 博主原地址（https://blog.csdn.net/bbaiggey/article/details/51984753）
 spark中有partition的概念（和slice是同一个概念，在spark1.2中官网已经做出了说明），一般每个partition对应一个task。在我的测试过程中，如果没有设置spark.default.parallelism参数，spark计算出来的partition非常巨大，与我的cores非常不搭。我在两台机器上（8cores *2 +6g * 2）上，spark计算出来的partition达到2.8万个，也就是2.9万个tasks，每个task完成时间都是几毫秒或者零点几毫秒，执行起来非常缓慢。在我尝试设置了 spark.default.parallelism 后，任务数减少到10，执行一次计算过程从minute降到20second。

Clusters will not be fully utilized unless you set the level of parallelism for each operation high enough. Spark automatically sets the number of “map” tasks to run on each file according to its size (though you can control it through optional parameters to SparkContext.textFile, etc), and for distributed “reduce” operations, such as groupByKey and reduceByKey, it uses the largest parent RDD’s number of partitions. You can pass the level of parallelism as a second argument (see the spark.PairRDDFunctions documentation), or set the config propertyspark.default.parallelism to change the default. In general, we recommend 2-3 tasks per CPU core in your cluster.（除非您将每个操作的并行度设置得足够高，否则不会充分利用集群。Spark根据每个文件的大小自动设置要在其上运行的“map”任务的数量(尽管您可以通过可选的参数来控制它，以SparkContext。对于分布式的“减少”操作，例如groupByKey和reduceByKey，它使用最大的父RDD的分区数。您可以将并行级别作为第二个参数传递(请参阅spark。或者设置config propertyspark.default.parallelism可以更改默认值。通常，我们建议您的集群中每个CPU内核执行2-3个任务。）

参数可以通过spark_home/conf/spark-default.conf配置文件设置。

2. System.getproperty()

1,System.getProperty返回的数值,比如java.version,java.home,os.name,user.home以及user.dir等等.
2,getProperties
public static Properties getProperties()确定当前的系统属性.
首先,如果有安全管理器,则不带参数直接调用其 checkPropertiesAccess 方法.这可能导致一个安全性异常.
将 getProperty(String) 方法使用的当前系统属性集合作为 Properties 对象返回.如果没有当前系统属性集合,则先创建并初始化一个系统属性集合.这个系统属性集合总是包含以下键的值: 键 相关值的描述
java.version Java 运行时环境版本
java.vendor Java 运行时环境供应商
java.vendor.url Java 供应商的 URL
java.home Java 安装目录
java.vm.specification.version Java 虚拟机规范版本
java.vm.specification.vendor Java 虚拟机规范供应商
java.vm.specification.name Java 虚拟机规范名称
java.vm.version Java 虚拟机实现版本
java.vm.vendor Java 虚拟机实现供应商
java.vm.name Java 虚拟机实现名称
java.specification.version Java 运行时环境规范版本
java.specification.vendor Java 运行时环境规范供应商
java.specification.name Java 运行时环境规范名称
java.class.version Java 类格式版本号
java.class.path Java 类路径
java.library.path 加载库时搜索的路径列表
java.io.tmpdir 默认的临时文件路径
java.compiler 要使用的 JIT 编译器的名称
java.ext.dirs 一个或多个扩展目录的路径
os.name 操作系统的名称
os.arch 操作系统的架构
os.version 操作系统的版本
file.separator 文件分隔符(在 UNIX 系统中是"/")
path.separator 路径分隔符(在 UNIX 系统中是":")
line.separator 行分隔符(在 UNIX 系统中是"/n")
user.name 用户的账户名称
user.home 用户的主目录
user.dir 用户的当前工作目录
系统属性值中的多个路径是用平台的路径分隔符分隔的.
注意,即使安全管理器不允许执行 getProperties 操作,它可能也会选择允许执行 getProperty(String) 操作.


```
3.

4. Preconditions.checkArgument(StringUtils.isNotBlank(brokerList), "kafka brokerList is blank...");

```Java
/**
   * Ensures the truth of an expression involving one or more parameters to the
   * calling method.
   *
   * @param expression a boolean expression
   * @param errorMessage the exception message to use if the check fails; will
   *     be converted to a string using {@link String#valueOf(Object)}
   * @throws IllegalArgumentException if {@code expression} is false
   */
  public static void checkArgument(
      boolean expression, @Nullable Object errorMessage) {
    if (!expression) {
      throw new IllegalArgumentException(String.valueOf(errorMessage));
    }
  }
```

5. StringUtils.isNotBlank

```Java
判断字符是否为不为空
* StringUtils.isNotBlank(null)      = false
* StringUtils.isNotBlank("")        = false
* StringUtils.isNotBlank(" ")       = false
* StringUtils.isNotBlank("bob")     = true
* StringUtils.isNotBlank("  bob  ") = true
StringUtils.isNotBlank(brokerList)
```

### 2018.06.20_1 -> 2018.06.20_0

```
1. ConcurrentHashMap 的学习
2. # 5- jdk 1.7 /1.8 特性
3. # 6- maven 本地库setting.xml 的配置 (https://www.cnblogs.com/DreamDrive/p/5571916.html)

```


### ( 2018.06.19 记录)

```
xml 、config、 hdfs utils 工具类 、
```

###  2018.06.19_0 -> 2018.06.19_1

```
1. 工具类、反射、xml 、log4j 、hdfs utils 工具类

```


### 2- log4j 参数详解以及使用

[博主log4j 总结](https://blog.csdn.net/azheng270/article/details/2173430/)

#### log4j的使用

1 -使用流程

* 设置config.xml 路径

项目路径下有conf/log4j-config.xml (conf是项目路径的一级子目录：项目路径/conf/log4j-config.xml)
log4j 配置详解路径:
[log4j 配置博主总结](https://blog.csdn.net/azheng270/article/details/2173430/)

* 使用

使用之前需要加载log4j的配置文件

control 控制类
 ```Java

import org.apache.commons.logging.LogFactory;
import org.apache.log4j.xml.DOMConfigurator;

private final Log logger = LogFactory.getLog(controller.class);

DOMConfigurator.configure("conf/log4j-config.xml");
 ```

* 然后每个类中都可以有一个log 日志类创建，如下:参数为本类字节码对象

ParseConfig.java
 ```Java
 import org.apache.commons.logging.LogFactory;
 import org.apache.commons.configuration.XMLConfiguration;
 import org.apache.log4j.Logger;
 public class controller {
     public static final Logger logger = Logger.getLogger(ParseConfig.class);
     // private final Log logger = LogFactory.getLog(controller.class);
     logger.info("init log4j.xml ok!");
     logger.debug("调试");
     logger.error("错误！");
   }

 ```

 * 这里多废话加载xml文件

 测试xml
注意xml中严格禁止使用 < & 这两种符号，所以

```
&(逻辑与)  &amp;       
<(小于)    &lt;       
>(大于)    &gt;        
"(双引号)  &quot;     
'(单引号)  &apos;
```
 ```xml
 <?xml version="1.0" encoding="UTF-8"?>
 <config>
     <sys_setting>
         <net_type>11</net_type>
         <!--jdbc配置-->
 		     <url>jdbc:mysql://ip/nc_yd?autoReconnectForPools=true&amp;useUnicode=true&amp;characterEncoding=utf-8</url>
         <username>username</username>
         <password>password</password>
         <!--任务输入配置 -->
         <input></input>
 	       <local>路径</local>
           <!--任务执行间隔 -->
 	       <intervalTime>0 */1 * * * ?</intervalTime>
     </sys_setting>
 </config>

 ```
* 使用 xml 的配置

 ```Java
 // import org.apache.commons.configuration.XMLConfiguration;

 XMLConfiguration parser = new XMLConfiguration();
 parser.setFile(new File(filename));
parser.load();
// 获取对应的配置
Config.url = parser.getString("sys_setting.url");

 ```


####
挺详细的:[博主总结](https://blog.csdn.net/azheng270/article/details/2173430/)

* 以下是部分摘录

```

```
### 3- 资源调度的使用 quartz
* 资源调度配置

[quartz作业调度配置博主地址](https://www.cnblogs.com/lpc-xx/p/8550364.html)
[对比linux作业调度](https://www.cnblogs.com/intval/p/5763929.html)
#### 资源调度的使用 quartz

参数配置说明quartz

```
// 一下是一秒执行一次
*/1 * * * * ?
* 0/1 * * * * ?
* */1 * * * ?

// 每分钟执行一次
0 */1 * * * ?
0 0/1 * * * ?


.quartz 时间配置规则
格式: [秒] [分] [小时] [日] [月] [周] [年]
 序号 	说明 	 是否必填 	 允许填写的值 	允许的通配符
 1 	 秒 	 是 	 0-59  	  , - * /
 2 	 分 	 是 	 0-59 	  , - * /
 3 	小时 	 是 	 0-23 	  , - * /
 4 	 日 	 是 	 1-31 	  , - * ? / L W
 5 	 月 	 是 	 1-12 or JAN-DEC 	  , - * /
 6 	 周 	 是 	 1-7 or SUN-SAT 	  , - * ? / L #
 7 	 年 	 否 	 empty 或 1970-2099 	 , - * /

 0 0 12 * * ? 	每天12点触发
0 15 10 ? * * 	每天10点15分触发
0 15 10 * * ? 	每天10点15分触发
0 15 10 * * ? * 	每天10点15分触发
0 15 10 * * ? 2005 	2005年每天10点15分触发
0 * 14 * * ? 	每天下午的 2点到2点59分每分触发
0 0/5 14 * * ? 	每天下午的 2点到2点59分(整点开始，每隔5分触发)
0 0/5 14,18 * * ? 	每天下午的 2点到2点59分(整点开始，每隔5分触发) 、每天下午的 18点到18点59分(整点开始，每隔5分触发)
0 0-5 14 * * ? 	每天下午的 2点到2点05分每分触发
0 10,44 14 ? 3 WED 	3月分每周三下午的 2点10分和2点44分触发
0 15 10 ? * MON-FRI 	从周一到周五每天上午的10点15分触发
0 15 10 15 * ? 	每月15号上午10点15分触发
0 15 10 L * ? 	每月最后一天的10点15分触发
0 15 10 ? * 6L 	每月最后一周的星期五的10点15分触发
0 15 10 ? * 6L 2002-2005 	从2002年到2005年每月最后一周的星期五的10点15分触发
0 15 10 ? * 6#3 	每月的第三周的星期五开始触发
0 0 12 1/5 * ? 	每月的第一个中午开始每隔5天触发一次
0 11 11 11 11 ? 	每年的11月11号 11点11分触发(光棍节)

```

```Java
import org.quartz.Job;
import org.quartz.JobExecutionContext;
import org.quartz.JobExecutionException;

// 首先，必需要取得一个Scheduler的引用
           SchedulerFactory sf = new StdSchedulerFactory();
           Scheduler sched = sf.getScheduler();
           //jobs可以在scheduled的sched.start()方法前被调用

           JobDetail job = newJob(contro.class).withIdentity("job1", "group1").build();
           CronTrigger trigger = newTrigger().withIdentity("trigger1", "group1").withSchedule(cronSchedule(Config.intervalTime)).build();

           sched.scheduleJob(job, trigger);

           sched.start(); //
           // 这里执行start 的话，name只要实现Job接口的类 ，并在execut （）  函数中执行了函数 的话那么 都会进行执行

```
#### 部分记录

* pom 配置文件
```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>OTV_Quartz</groupId>
    <artifactId>OTV_Quartz</artifactId>
    <version>0.0.1-SNAPSHOT</version>

    <dependencies>
        <!-- 引入Quartz library -->
        <dependency>
            <groupId>org.quartz-scheduler</groupId>
            <artifactId>quartz</artifactId>
            <version>2.1.3</version>
        </dependency>
        <!-- Log4j library -->
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>1.2.16</version>
        </dependency>    
    </dependencies>
</project>
```

* 任务类定义

写一个业务类testjob类，需要实现quartz中的job接口(其中只有一个函数可用于封装业务逻辑，execute)

```Java
package com.otv.job;
import org.apache.log4j.Logger;
import org.quartz.Job;
import org.quartz.JobExecutionContext;
import org.quartz.JobExecutionException;

public class TestJob implements Job {
    private Logger log = Logger.getLogger(TestJob.class);    
    public void execute(JobExecutionContext jExeCtx) throws JobExecutionException {
        log.debug("[vigar]: TestJob run successfully...");
    }
}
```


* 调度 ,五秒执行一次

```Java
package com.otv;
import org.quartz.JobBuilder;
import org.quartz.JobDetail;
import org.quartz.Scheduler;
import org.quartz.SchedulerException;
import org.quartz.SchedulerFactory;
import org.quartz.SimpleScheduleBuilder;
import org.quartz.Trigger;
import org.quartz.TriggerBuilder;
import org.quartz.impl.StdSchedulerFactory;
import com.otv.job.TestJob;
public class JobScheduler {    
    public static void main(String[] args) {
        try {
            // job 中有testJob中的信息
            JobDetail job = JobBuilder.newJob(TestJob.class)
                .withIdentity("testjob").build();

            // 这是trigger 部分，与时间调度相关
            Trigger trigger = TriggerBuilder.newTrigger()
                  .withSchedule(SimpleScheduleBuilder.simpleSchedule()
                        .withIntervalInSeconds(5).repeatForever()).build();  

            //开始调度job
            SchedulerFactory schFactory = new StdSchedulerFactory();
            Scheduler sch = schFactory.getScheduler();
            sch.start();            
            sch.scheduleJob(job, trigger);        

        } catch (SchedulerException e) {
            e.printStackTrace();
        }
    }
}
```

### 4- ConcurrentHashMap 的使用

####  总
* segment 数组+许多个 hashEntry
* ConcurrentHashMap长度是16。
* 1.8之前concurrenthashmap仍然采用锁分段技术，1.8及之后就是采用的CAS算法了。
* 而Concurrenthashmap对于复合操作仍然是线程安全的
#### 1-分博主

[源码解析-元博主地址](http://www.importnew.com/22007.html)
[使用要点-博主地址](http://www.cnblogs.com/zhuawang/p/4779649.html)
[底层-博主地址](https://blog.csdn.net/u011328417/article/details/79284730)
* 部分总结：

```



```

* 使用要点

博主地址：http://www.cnblogs.com/zhuawang/p/4779649.html

```
ConcurrentHashMap的简要总结：

1、public V get(Object key)不涉及到锁，也就是说获得对象时没有使用锁；

2、put、remove方法要使用锁，但并不一定有锁争用，原因在于ConcurrentHashMap将缓存的变量分到多个Segment，每个Segment上有一个锁，只要多个线程访问的不是一个Segment就没有锁争用，就没有堵塞，各线程用各自的锁，ConcurrentHashMap缺省情况下生成16个Segment，也就是允许16个线程并发的更新而尽量没有锁争用；

3、Iterator对象的使用，不一定是和其它更新线程同步，获得的对象可能是更新前的对象，ConcurrentHashMap允许一边更新、一边遍历，也就是说在Iterator对象遍历的时候，ConcurrentHashMap也可以进行remove,put操作，且遍历的数据会随着remove,put操作产出变化，所以希望遍历到当前全部数据的话，要么以ConcurrentHashMap变量为锁进行同步(synchronized该变量)，要么使用CopiedIterator包装iterator，使其拷贝当前集合的全部数据，但是这样生成的iterator不可以进行remove操作。



Hashtable和ConcurrentHashMap的不同点：

1、Hashtable对get,put,remove都使用了同步操作，它的同步级别是正对Hashtable来进行同步的，也就是说如果有线程正在遍历集合，其他的线程就暂时不能使用该集合了，这样无疑就很容易对性能和吞吐量造成影响，从而形成单点。而ConcurrentHashMap则不同，它只对put,remove操作使用了同步操作，get操作并不影响，详情请看以上第1,2点，当前ConcurrentHashMap这样的做法对一些线程要求很严格的程序来说，还是有所欠缺的，对应这样的程序来说，如果不考虑性能和吞吐量问题的话，个人觉得使用Hashtable还是比较合适的；

2、Hashtable在使用iterator遍历的时候，如果其他线程，包括本线程对Hashtable进行了put，remove等更新操作的话，就会抛出ConcurrentModificationException异常，但如果使用ConcurrentHashMap的话，就不用考虑这方面的问题了，详情请看以上第3点；


1.HashMap或者ArrayList边遍历边删除数据会报java.util.ConcurrentModificationException异常
复制代码

Map<Long, String> mReqPacket = new HashMap<Long, String>();
        for (long i = 0; i < 15; i++) {
            mReqPacket.put(i, i + "");
        }

        for (Entry<Long, String> entry : mReqPacket.entrySet()) {
            long key = entry.getKey();
            String value = entry.getValue();
            if (key < 10) {
                mReqPacket.remove(key);
            }
        }

        for (Entry<Long, String> entry : mReqPacket.entrySet()) {
            System.out.println(entry.getKey() + " " + entry.getValue());
        }


        所以要用迭代器删除元素：
        复制代码

                Map<Long, String> mReqPacket = new HashMap<Long, String>();
                for (long i = 0; i < 15; i++) {
                    mReqPacket.put(i, i + "");
                }

                for (Iterator<Entry<Long, String>> iterator = mReqPacket.entrySet().iterator(); iterator.hasNext();) {
                    Entry<Long, String> entry = iterator.next();
                    long key = entry.getKey();
                    if (key < 10) {
                        iterator.remove();
                    }
                }

                for (Entry<Long, String> entry : mReqPacket.entrySet()) {
                    System.out.println(entry.getKey() + " " + entry.getValue());
                }

        复制代码
        2.对ConcurrentHashMap边遍历边删除或者增加操作不会产生异常(可以不用迭代方式删除元素)，因为其内部已经做了维护，遍历的时候都能获得最新的值。即便是多个线程一起删除、添加元素也没问题。
        复制代码

                Map<Long, String> conMap = new ConcurrentHashMap<Long, String>();
                for (long i = 0; i < 15; i++) {
                    conMap.put(i, i + "");
                }

                for (Entry<Long, String> entry : conMap.entrySet()) {
                    long key = entry.getKey();
                    if (key < 10) {
                        conMap.remove(key);
                    }
                }

                for (Entry<Long, String> entry : conMap.entrySet()) {
                    System.out.println(entry.getKey() + " " + entry.getValue());
                }

        复制代码



        3.一个线程对ConcurrentHashMap增加数据，另外一个线程在遍历时就能获得。
        复制代码

        static Map<Long, String> conMap = new ConcurrentHashMap<Long, String>();

            public static void main(String[] args) throws InterruptedException {
                for (long i = 0; i < 5; i++) {
                    conMap.put(i, i + "");
                }

                Thread thread = new Thread(new Runnable() {
                    public void run() {
                        conMap.put(100l, "100");
                        System.out.println("ADD:" + 100);
                        try {
                            Thread.sleep(100);
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }
                    }

                });
                Thread thread2 = new Thread(new Runnable() {
                    public void run() {
                        for (Iterator<Entry<Long, String>> iterator = conMap.entrySet().iterator(); iterator.hasNext();) {
                            Entry<Long, String> entry = iterator.next();
                            System.out.println(entry.getKey() + " - " + entry.getValue());
                            try {
                                Thread.sleep(100);
                            } catch (InterruptedException e) {
                                e.printStackTrace();
                            }
                        }
                    }
                });
                thread.start();
                thread2.start();

                Thread.sleep(3000);
                System.out.println("--------");
                for (Entry<Long, String> entry : conMap.entrySet()) {
                    System.out.println(entry.getKey() + " " + entry.getValue());
                }

            }

        输出：

        ADD:100
        0 - 0
        100 - 100
        2 - 2
        1 - 1
        3 - 3
        4 - 4
        --------
        0 0
        100 100
        2 2
        1 1
        3 3
        4 4
```



### 5- jdk 1.7 与1.8 的特性

[源博主地址](https://zhidao.baidu.com/question/2141065416381554228.html?qbl=relate_question_7&word=kdk%201.7%20%D6%A7%B3%D6lambda%B1%ED%B4%EF%CA%BD%C2%F0)

### 6-maven setting.xml 的配置

#### [博主地址](https://www.cnblogs.com/DreamDrive/p/5571916.html)

### 7- 连接集群问题

```
1. 网段设置成自动获取
使用1.185
网关 1.1
添加20.137


```
