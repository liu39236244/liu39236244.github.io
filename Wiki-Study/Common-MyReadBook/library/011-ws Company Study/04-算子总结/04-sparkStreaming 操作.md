# sparkStreaming 操作


# 设置sparktreaming

## 创建sparkStreaming

String path = Config.inputUserInfoPath+"/";  //"hdfs://" + Config.clusterID +
//        JavaSparkContext jsc = SparkInitMgr.jsc;
//        JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(120));
      JavaStreamingContext jssc = SparkInitMgr.jssc;

## 读取文件流

```Java
JavaDStream<String> lines = jssc.textFileStream(path);
```

## JavaDStream<String> 操作

### transform

```Java

JavaDStream<String> lines = jssc.textFileStream(path);

JavaDStream<String> reslut = lines.transform(new Function<JavaRDD<String>, JavaRDD<String>>() {

          private static final long serialVersionUID = 1L;
    @Override
          public JavaRDD<String> call(JavaRDD<String> v1) throws Exception {

            // Configuration & JavaSparkContext
            JavaSparkContext jsc = JavaSparkContext.fromSparkContext(v1.context());
            Configuration conf = new Configuration();

            // Create FileSystem & URI
      FileSystem fs;
            URI uri = URI.create(Config.URI);

            // Create RDD Content is null
            List<String> zzz = new ArrayList<String>();
            zzz.add("null");
            JavaRDD<String> result=jsc.parallelize(zzz);


Return a new DStream in which each RDD is generated by applying a function
on each RDD of 'this' DStream.

```

### foreachRDD

```
dStream.foreachRDD(new Function<JavaRDD<String>, Void>() {
```
