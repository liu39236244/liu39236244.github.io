# 常见算法介绍 无监督学习


内容简介：

1 、什么是无监督学习

2 、聚类算法

3、 K-Means

4 、实现步骤图示

5 、实现伪码

6、 K -Means 优缺点

## 无监督学习 上

1 、什么是无监督学习
无监督学习( Unsupervised learning ) : 从无标记的训练数据中推断结论。其特点为输入数据(训练数据)不存在明确的标识或结果(标签)。

常见的无监督学习为聚类 , 即发现隐藏的模式或者对数据进行分组。

即计算机根据我们提供的材料“自动”学习, 给定数据,寻找隐藏的结构或模式。

 

![](assets/002/01/01/03-1684122619686.png)


### 聚类算法


聚类:就是将相似的事物聚集在一起 ,而将不相似的事物划分到不同的类别的过程。它是一种探索性的分析，不必事先给出一个分类的标准,聚类分析能够从样本数据出发,自动进行分类。聚类分析所使用方法的不同，常常会得到不同的结论。

有未知分类的鸢尾花的测量数据,包括萼长、萼宽、瓣长、瓣宽，根据这些测量数据,将类似的测量记录归类(同种花的测量数据具有类似的特征)。需使用聚类实现"自动”学习。

 常见算法:层次聚类、划分聚类、基于密度的聚类


 ![](assets/002/01/01/03-1684130165305.png)


 ![](assets/002/01/01/03-1684130253187.png)



### K-Means


K-Means 即 K 均值聚类,属于划分聚类。其工作原理为根据初始化的聚类中心信息,计算每个样本到这些中心的距离,可以判断每个样本均归属于某个类簇,更新聚簇中心信息,重新计算每个样本到新的聚类中心的距离,重新划分样本到新的聚类中心对应的类中,重复进行,直到满足终止条件。

有N个样本点,使用K-Means将其聚类的步骤:

Step1:确定聚类的个数k,并指定k个聚类的中心 C1，C2..Ck

Step 2 :计算每个样本S;点到k个中心的距离,并将该点归入最近的C类中其中，ie(1,N)，j∈(1,k)

Step 3 :重新计算k个类簇的中心点，更新原有中心点的位置 C1，C2..Ck

Step4 :重复步骤 Step 2、Step 3 ,直到中心点位置不再变化或者变化幅度小

约定阈值,或者达到预定义的最大循环次数,结束。得到最终聚类结果


![](assets/002/01/01/03-1684130416709.png)


### 实现步骤图示

#### 第一步,确定聚类个数,确定聚类中心,确定距离计算公式

![](assets/002/01/01/03-1684130575309.png)


观察法  枚举法  其他技术手段

#### 第二步，计算每个点和聚类中心的距离,归类

![](assets/002/01/01/03-1684130658048.png)


第三步，计算当前类簇中心，更新聚类中心Ck的位置

重复第二步,将各样本S;点根据新聚类中心Ck进行重新划分

重复第三步，根据最新聚类计算聚类中心,更新中心C取值

重复第二步,重复第三步,直到聚类中心位置不再发生变化,

或者循环次数达到预先设定的阈值,结束,得到最终聚类结果


![](assets/002/01/01/03-1684130759441.png)

 
 ### 实现伪码
    
选择 k 个点作为初始类簇中心

repeat

将每个样本点指派到最近的类簇中心,形成k个类簇

重新计算每个类簇的中心

until 类簇不发生变化 or 达到最大迭代次数

 

 ![](assets/002/01/01/03-1684130799745.png)




![](assets/002/01/01/03-1684130834336.png)


### K -Means 优缺点

优点:

原理简单,容易理解,容易实现

聚类结果容易解释

聚类结果相对较好

缺点:

分类个数k需要事先指定,且指定的k值不同,聚类结果相差较大

初始的k个类簇中心对最终结果有影响,选择不同,结果可能会不同

能识别的类簇仅为球状,非球状的聚类效果很差

样本点较多时，计算量较大

对异常值敏感 ,对离散值需要特殊处理



## 无监督学习 下

内容简介：

1 关联规则: Association Rule

2 常见算法



### 1 关联规则: Association Rule

关联规则是反映事物与事物间相互的依存关系和关联性。如果两个或多个事物间存在一定的关联关系,则其中一个事物能够通过其他事物预测到。最常见的场景就是购物篮分析( Market Basket)。通过分

析顾客购物篮中的不同商品之间的关系,来分析顾客的购买习惯。经典案例就是啤酒与尿布。

根据某超市的购物篮信息，分析顾客的购物习惯，制定货物摆放或者捆绑销售策略。(Apriori)

首先确定最小支持度: 50% ,最小置信度: 50%

确定1 -频繁项集    {A}:50%,{B}:75%,{C} 75%,{E}:75%

![](assets/002/01/01/03-1684131175857.png)


确定2-频繁项集     {A,C}:50%,{B,C}:50%,{B,E} 75%,{C,E}:50%

![](assets/002/01/01/03-1684131234357.png)



确定3-频繁项集    {B,C, E}:50%

![](assets/002/01/01/03-1684131314853.png)


确定关联规则:    非空子集: {B},{C}{E},{B,C},{B,E},{C,E}

![](assets/002/01/01/03-1684131382951.png)

![](assets/002/01/01/03-1684131716775.png)

### 常见算法


聚类算法:

■ K 均值( K-Means )

■DBScan

■最大期望(EM:Expectation Maximization )

■降维: PCA(主成分分析)、PLS (偏最小二乘回归)、MDS (多维尺度分析)......

......
深度学习
关联规则:

■Apriori

■Eclat

■......


![](assets/002/01/01/03-1684131802319.png)


## 其他监督学习算法


内容简介：

1 半监督学习

2 集成学习

3 Bagging

4 Boosting

5 AdaBoost

6 随机森林: RandomForest

7 深度学习

8 增强学习: Reinforcement Learning

9 迁移学习: Transfer Learning


### 半监督学习

![](assets/002/01/01/03-1684131947592.png)


### 集成学习

![](assets/002/01/01/03-1684132199713.png)


![](assets/002/01/01/03-1684132207412.png)

### Bagging

![](assets/002/01/01/03-1684132255727.png)



![](assets/002/01/01/03-1684132723914.png)

***但是要注意多个模型 理论上 单个弱分类器 越多 准确率 将会理论提升，但是显示中却并不是这样的，因为他们都是单独的关联性不大***


### 4 Boosting


![](assets/002/01/01/03-1684132870364.png)

就是对于做错的进行反复训练，更专注于错的然后继续练习


###  AdaBoost


![](assets/002/01/01/03-1684133036061.png)


###  随机森林: RandomForest


比较重要，使用的也比较多

随机森林( RandomForest )

■由许多决策树组成,树生成时采用了随机的方

法

. Smart Bagging

■生成步骤

V随机采样,生成多个样本集

V对每个样本集构建决策树

■优点:

V可以处理多分类

V不会过拟合（很大程度上 会减少过拟合）

V容易实现并行

V对数据集容错能力强

![](assets/002/01/01/03-1684133078740.png)

![](assets/002/01/01/03-1684133203039.png)


### 7 深度学习 （比较火热）


图像识别 ，语音识别 ， 及时视觉翻译 ， 自动驾驶



深度学习:深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征,以发现数据的分布式特征表示。

属于机器学习研究中的一个新的领域,其动机在于建立、模拟人脑进行分析学习的神经网络,它模仿人脑的机制来解释数据,例如图像,声音和文本。

常见深度学习算法:

■受限波尔兹曼机 Restricted Boltzmann Machine ( RBM )

■深度信念网络  Deep Belief Networks ( DBN )

■卷积网络 Convolutional Network

■栈式自编码 Stacked Auto- encoders

![](assets/002/01/01/03-1684133332963.png)



### 增强学习: Reinforcement Learning


###  迁移学习: Transfer Learning

迁移学习是要把已学训练好的模型参数迁移到新的模型来帮助新模型训练数据集。初衷是节省人工标注

样本的时间,让模型可以通过已有的标记数据向未标记数据迁移。换言之,就是运用已有的知识来学习

新的知识,核心是找到已有知识和新知识之间的相似性。

![](assets/002/01/01/03-1684133395666.png)





## 总结与回顾

1 学习内容
（1）介绍算法的常见分类

（2）介绍有监督学习算法,重点介绍 KNN(懒惰算法，并没有生成真的模型，来数据了现场计算的，每次还需要进行大量计算)、ID3(决策树算法的一个原型吧，实现起来比较容易，正因为有确定所以衍生出来了 c45 等)

（3）介绍无监督学习算法,重点介绍 Apriori、K-Means

（4）介绍其他常见的算法


 学习目标

（1）了解无监督学习与有监督学习

（2）掌握 KNN、ID3分类算法 （最好用自己熟知的语言自己实现一个 这种算法）

（3）掌握 Apriori、K-Means 算法

（4）了解集成学习等其他算法


#### 3 思考与练习

01了解有监督学习和无监督学习的区别,分别描述几个相关的场景。

02用自己掌握的编程语言，实现分类算法 K-NN 和聚类算法 K-Means

03思考一下 KNN、K-Means  等算法的优缺点,制定一-些优化缺点的方案

04找几个小的周知数据集,按照课程中介绍的算法逻辑,手工计算一下，评估一下结果。包括关联规则、ID3 决策树等。

05 ID3 算法无法处理连续特征,想一些解决办法,自己动手试一下。了解一 下 C4.5 算法的工作原理。