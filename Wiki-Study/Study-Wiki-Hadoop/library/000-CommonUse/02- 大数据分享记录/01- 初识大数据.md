
# 大数据背景以及概念

## 什么是大数据


### 概念以及背景

> 1 究竟什么是大数据呢

百度百科：
大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。


你肯定会问：概念反正都是说的这么书面化，什么叫常规工具？什么是新处理模式？先别着急，咱们 来想想为什么会出现大数据呢？


> 2 背景

半个世纪以来，随着计算机技术全面融入社会生活在加快。<span style="color:red;font-size:20px">PC端、智能手机端网络冲浪使得信息爆炸已经积累到了一个开始引发变革的程度。</span>

21世纪是数据信息大发展的时代，移动互联、社交网络、电子商务等极大拓展了互联网的边界和应用范围，各种数据正在迅速膨胀并变大。

互联网（社交、搜索、电商）、移动互联网（微博）、物联网（传感器，智慧地球）、车联网、GPS、医学影像、安全监控、金融（银行、股市、保险）、电信（通话、短信）都在疯狂产生着数据。


![](assets/000/01/02/01-1596943621048.png)


<span style="color:red;font-size:50px">数据!数据！数据！ 重要的事情说三遍</span>


> 3 大数据的5个v




![](assets/000/01/02/01-1596944047837.png)

* 3.1 那么回归正题


 那么问题来了，前面也提到了无法用现有的处理工具解决，那现有处理工具有哪些呢，处理模式又是怎样的呢？


```
处理模式：单机处理
处理工具：做报表，程序分析，Excel 

```

* 3.2 或许你会觉得，这不挺正常吗，写个程序、额管他java  python scala go js ts 。。。 ；如果时间继续追溯的话是个什么样子的嘞？



![](assets/000/01/02/01-1596944408396.png)


* 3.3 大数据量的发展史

1 、统计学的出现1663年

在伦敦,约翰▪葛兰特(John Graunt)进行了第一次有记录的统计数据分析实验.他用记录下的当时肆虐欧洲的黑死病死亡人数信息,建立起了早期预警系统的理论.。


2 、1880年 处理人口普查数据


美国人口普查局有个问题，预计将花费8年的时间去捣弄1880年收集到的人口普查数据，而1890年的人口普查数据的处理将消耗超过10年的时间，这意味着到1900年人口普查的时候，人口普查局也无法看到那些过时的人口数据的结果。


在1881年人口普查局聘用了一位年轻的工程师    <span style="color:red;font-size:20px">  赫尔曼·霍尔瑞斯(Herman Hollerith)  </span>
 ,发明了著名的     <span style="color:red;font-size:20px">  霍勒瑞斯制表机</span>.穿孔卡片的使用让他将需要耗费10年的工作缩短为3个月,这使他在历史上取得了现代自动化计算之父的历史地位.他成立的公司就是日后著名的IBM.。



![](assets/000/01/02/01-1596944865278.png)



![](assets/000/01/02/01-1596944876486.png)


海量数据中心的开端1965年

美国政府计划在世界首个数据中心的磁盘上存储7.42亿的纳税申报单和1.75亿的指纹信息。


3 、1989年 

也许是第一次像今天使用大数据那样使用”大数据”这个术语.国际畅销书作者埃里克·拉森(Erik Larson)为哈珀斯杂志写了一篇他推测是首个收到
垃圾邮件的文章.他写到:”大数据的守护者说他们这么做是为了消费者的利益.但是有一种使用数据为了其他目的和意图的方式.”
另外,随着为分析商业及运算性能开发的新兴软件和系统的普及,上世纪50年代已经很流行的概念  <span style="color:red;font-size:20px"> ”商业智能”</span>看到了未来的大潮.


4 、 互联网的推动1991年

计算机科学家   <span style="color:red;font-size:20px">  蒂姆伯纳斯李 </span> 宣告了我们今天所熟知的万维网的诞生.在一个网站上,他制定了世界网络的协议书,使互联网的数据联通起来,让任何人可以在任何地方进行通信.



5 、 1997年 数据增长速度

迈克来斯克(Michael Lesk)在他的论文世界上有多少信息?理论地给出12,000PB这一可能不是很合理的猜想。他也在早期的发展中指出,网络数据正以每年10倍的速度增长。他说任何人从未看过其中的大部分数据,所以这个变化很难被察觉。
这一年,谷歌搜索也首次亮相,至少在未来的20年,它的名字将成为网络数据搜索的代名词

存储单位的有小到大

```
KB，MB，GB，TB,在往上是baiPB、EB、duZB、YB、BB、NB、DB、CB。

1、1DB=1024NB

2、1NB=1024BB

3、1BB=1024YB

4、1YB=1024ZB

5、1ZB=1024EB

6、1EB=1024PB

7、1PB=1024TB

8、1TB=1024GB

9、1GB=1024MB

10、1MB=1024KB

11、1KB=1024B
```

![](assets/000/02/01-1596945883466.png)


6 、 大数据初探1999年

两年后大数据这一术语出现在美国计算机协会发表的可视化实时探索G比特数据集中.倾向于存储海量数据而无法充分对他们进行分析再一次成为遗憾


7 、 网络2.0时代助涨了数据大爆发2005年

评论员说我们正在见证网络2.0的诞生，大多数的网络内容将由用户产生而不是网络服务提供商给出的。通过整合HTML样式的网页和基于SQL的后台数据库技术实现了这一目标。一年前出现的Facebook已经有550万人通过它来上传自己的数据并与朋友分享。
<span style="color:blue;font-size:20px"> 
Hadoop也在这一年诞生，它是专门为存储及分析大数据的开源框架。它的灵活性使得管理我们不断产生和采集的非结构化数据(语音，视频，文档等)的时候特别有用。
</span>






### Google 三大论文


#### 1 google 三大论文


```
1、GFS：Google File System
2、MapReduce
2、BigTable
```
 
* 1 大数据的开拓者Google

 毫不夸张的说，google就是大数据时代的开拓者，google的大数据技术架构一直都是全球互联网企业争相学习和研究的重点，也为行业大数据技术的架构树立起了标杆。

#### 2 GFS

* 1 GFS：Google File System     
         
》对数据进行分布式文件系统的存储，解决的大数据的存储问题 ；
2003年，Google发布Google File System论文，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。<span style="color:red;font-size:20px"> 它运行于廉价的普通硬件上，提供容错功能。</span>从根本上说：文件被分割成很多块，使用冗余的方式储存于商用机器集群上。   

* 2 问：那么分布式文件存储系统出来之前数据怎么更好存储？

<span style="color:red;font-size:20px"> 独立磁盘冗余阵列（ Redundant Array of Independent Disks) </span>

要知道，存储大量数据有三个最重要的指标，那就是速度，容量，容错性。速度和容量的重要性毋庸置疑，如果容量不够大，或者读取的速度不够快，那么海量数据存储也就无从谈起了。而磁盘又是计算机中很容易损坏的零件，当磁盘损坏的时候怎么办？放任数据的丢失吗，那可不行，这就有了容错性的需求。在没有分布式存储的时代，单个磁盘容量不够怎么办？加磁盘呗。磁盘容易坏怎么办，同时用其他磁盘备份呗。就这样，独立磁盘冗余阵列（ Redundant Array of Independent Disks ），简称 RAID，诞生了。


在 2000 年以前，磁盘还是很珍贵的资源，不像现在，磁盘是 PC 机中最廉价的部件。一开始 RAID 技术是为了将多个廉价的容量较小的磁盘组合起来充当一个大磁盘以节约成本，但后来人们发现这项技术也可以用在单台机器磁盘扩容上了，于是 RAID 技术开始被广泛使用。RAID 技术将多个磁盘组合成一个逻辑扇区，对计算机而言，它会将 RAID 当作一个磁盘来处理。


使用 RAID 的好处有：增强数据集成度，增强容错功能，增加处理量或容量。
另外 RAID 也有分为多个档次，标准的分法，分别是 RAID0 ， RAID1 ， RAID2 ，RAID3 ，RAID4 ，RAID5 ， RAID6 ，每个档次都有对应的优缺点。
这些 RAID 等级的不同主要是对 上述说到的三个数据存储要素（速度，容量，容错性）的不同取舍，各有各的应用场景。我们从上述的三个指标来看 RAID 技术。

```

速度： RAID通过在多个磁盘上同时存储和读取数据来大幅提高存储系统的数据吞吐量。在RAID中，可以让很多磁盘驱动器同时传输数据，而这些磁盘驱动器在逻辑上又是一个磁盘驱动器，所以使用RAID可以达到单个磁盘驱动器几倍、几十倍甚至上百倍的速率。

容量： 可以将多个磁盘连接起来，对比以前的单个磁盘存储，RAID 将存储的量级拔高了一个台阶。但依旧有其局限性，因为 RAID 始终是放在单台机器上，机器的磁盘卡槽不可能无限增加，磁盘也不可能一直增大。

容错性： 不同等级的 RAID 使用不同的数据冗余策略，保证数据的容错性。比如最简单的 RAID1 就是数据在写入磁盘时，将一份数据同时写入两块磁盘，这样任何一块磁盘损坏都不会导致数据丢失，而插入一块新磁盘就可以通过复制数据的方式自动修复，具有极高的可靠性。

```


* 3 所以 咱们的 HDFS 也是同样的思路

思维：Hdfs 和 RAID 在思想上是有一些相似之处的。都是通过水平拓展，比如 RAID 水平拓展磁盘，Hdfs 则是水平拓展机器。

分布式存储框架GFS-HDFS

Google在GFS上如何快速分析和处理数据方面开创了MapReduce并行计算框架，让以往的高端服务器计算变为廉价的x86集群计算，也让许多互联网公司能够从IOE(IBM小型机、Oracle数据库以及EMC（易安信）存储)中解脱出来，例如：淘宝早就开始了去IOE化的道路。

然而，Google之所以伟大就在于独享技术不如共享技术，在2002-2004年间以三大论文的发布向世界推送了其云计算的核心组成部分GFS、MapReduce以及BigTable。

Google虽然没有将其核心技术开源，但是这三篇论文已经向开源社区的大牛们指明了方向，一位大牛：<span style="color:red;font-size:20px"> Doug Cutting </span> 使用Java语言对Google的云计算核心技术(主要是GFS和MapReduce)做了开源的实现。
 就是他女儿一个大象玩具的名字，刚好就用Hadoop 这个“大象”

```
Doug Cutting根据GFS和MapReduce的思想创建了开源Hadoop框架。
2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发。
Doug Cutting任职于Cloudera公司。
2009年7月，Doug Cutting当选为Apache软件基金会董事，2010年9月，当选为 chairman。
Hadoop 
```
![](assets/000/02/01-1596947897976.png)


百度百科上的：EMC

![](assets/000/02/01-1596948281776.png)


去IOE

![](assets/000/02/01-1596948307286.png)



* 4 、 hadoop 重大发展


<span>

</br> 2004年— 最初的版本(现在称为HDFS和MapReduce)由Doug Cutting和Mike Cafarella开始实施。
</br> 2005年12月— Nutch移植到新的框架，Hadoop在20个节点上稳定运行。
</br> <span style="color:red;font-size:20px">
</br> 2006年1月— Doug Cutting加入雅虎。
</br> </span>
</br> 2006年2月— Apache Hadoop项目正式启动以支持MapReduce和HDFS的独立发展。
</br> 2006年2月— 雅虎的网格计算团队采用Hadoop。
</br> 2006年4月— 标准排序(10 GB每个节点)在188个节点上运行47.9个小时。
</br> 2006年5月— 雅虎建立了一个300个节点的Hadoop研究集群。
</br> 2006年5月— 标准排序在500个节点上运行42个小时(硬件配置比4月的更好)。
</br> 2006年11月— 研究集群增加到600个节点。
</br> 2006年12月— 标准排序在20个节点上运行1.8个小时，100个节点3.3小时，500个节点5.2小时，900个节点7.8个小时。
</br> 2007年1月— 研究集群到达900个节点。
</br> <span style="color:red;font-size:20px">
</br> 2007年4月— 研究集群达到两个1000个节点的集群。
</br> </span>
</br> 2008年4月— 赢得世界最快1TB数据排序在900个节点上用时209秒。
</br> 2008年7月— 雅虎测试节点增加到4000个
</br> 2008年9月— Hive成为Hadoop的子项目
</br> <span style="color:red;font-size:20px">
</br> 2008年11月— Google宣布其MapReduce用68秒对1TB的程序进行排序
</br> </span>
</br> 2008年10月— 研究集群每天装载10TB的数据。
</br> <span style="color:blue;font-size:20px">
</br> 2008年— 淘宝开始投入研究基于Hadoop的系统–云梯。云梯总容量约9.3PB，共有1100台机器，每天处理18000道作业，扫描500TB数据。
</br> </span>
                                                                                   </br>
2004年— 最初的版本(现在称为HDFS和MapReduce)由Doug Cutting和Mike Cafarella开始实施。                                                                                                                                             </br>
2005年12月— Nutch移植到新的框架，Hadoop在20个节点上稳定运行。                                                                                                                                                                 
<span style="color:red;font-size:20px">                                                                                                                                                                                       
2006年1月— Doug Cutting加入雅虎。                                                                                                                                                                                               </br>
</span>                                                                                                                                                                                                                         </br>
2006年2月— Apache Hadoop项目正式启动以支持MapReduce和HDFS的独立发展。                                                                                                                                                           </br>
2006年2月— 雅虎的网格计算团队采用Hadoop。                                                                                                                                                                                       </br>
2006年4月— 标准排序(10 GB每个节点)在188个节点上运行47.9个小时。                                                                                                                                                                 </br>
2006年5月— 雅虎建立了一个300个节点的Hadoop研究集群。                                                                                                                                                                            </br>
2006年5月— 标准排序在500个节点上运行42个小时(硬件配置比4月的更好)。                                                                                                                                                             </br>
2006年11月— 研究集群增加到600个节点。                                                                                                                                                                                           </br>
2006年12月— 标准排序在20个节点上运行1.8个小时，100个节点3.3小时，500个节点5.2小时，900个节点7.8个小时。                                                                                                                         </br>
2007年1月— 研究集群到达900个节点。                                                                                                                                                                                              </br>
<span style="color:red;font-size:20px">                                                                                                                                                                                   
2007年4月— 研究集群达到两个1000个节点的集群。                                                                                                                                                                                 
</span>                                                                                                                                                                                                                         </br>
2008年4月— 赢得世界最快1TB数据排序在900个节点上用时209秒。                                                                                                                                                                      </br>
2008年7月— 雅虎测试节点增加到4000个                                                                                                                                                                                             </br>
2008年9月— Hive成为Hadoop的子项目                                                                                                                                                                                               </br>
<span style="color:red;font-size:20px">                                                                                                                                                                                      
2008年11月— Google宣布其MapReduce用68秒对1TB的程序进行排序                                                                                                                                                                    
</span>                                                                                                                                                                                                                         </br>
2008年10月— 研究集群每天装载10TB的数据。                                                                                                                                                                                      
<span style="color:blue;font-size:20px">                                                                                                                                                                                    
2008年— 淘宝开始投入研究基于Hadoop的系统–云梯。云梯总容量约9.3PB，共有1100台机器，每天处理18000道作业，扫描500TB数据。                                                                                                          </br>
</span>                                                                                                                                                                                                                         </br>
2009年3月— 17个集群总共24 000台机器。                                                                                                                                                                                           </br>
2009年3月— Cloudera推出CDH（Cloudera’s Dsitribution Including Apache Hadoop）                                                                                                                                                   </br>
2009年4月— 赢得每分钟排序，雅虎59秒内排序500 GB(在1400个节点上)和173分钟内排序100 TB数据(在3400个节点上)。                                                                                                                      </br>
2009年5月— Yahoo的团队使用Hadoop对1 TB的数据进行排序只花了62秒时间。                                                                                                                                                            </br>
2009年7月— Hadoop Core项目更名为Hadoop Common;                                                                                                                                                                                  </br>
2009年7月— MapReduce 和 Hadoop Distributed File System (HDFS) 成为Hadoop项目的独立子项目。                                                                                                                                      </br>
2009年7月— Avro 和 Chukwa 成为Hadoop新的子项目。                                                                                                                                                                                </br>
2009年9月— 亚联BI团队开始跟踪研究Hadoop                                                                                                                                                                                         </br>
2009年12月—亚联提出橘云战略，开始研究Hadoop                                                                                                                                                                                     </br>
2010年5月— Avro脱离Hadoop项目，成为Apache顶级项目。                                                                                                                                                                             </br>
2010年5月— HBase脱离Hadoop项目，成为Apache顶级项目。                                                                                                                                                                            </br>
2010年5月— IBM提供了基于Hadoop 的大数据分析软件——InfoSphere BigInsights，包括基础版和企业版。                                                                                                                                   </br>
2010年9月— Hive( Facebook) 脱离Hadoop，成为Apache顶级项目。                                                                                                                                                                     </br>
2010年9月— Pig脱离Hadoop，成为Apache顶级项目。                                                                                                                                                                                  </br>
2011年1月— ZooKeeper 脱离Hadoop，成为Apache顶级项目。                                                                                                                                                                           </br>
2011年3月— Apache Hadoop获得Media Guardian Innovation Awards 。                                                                                                                                                                 </br>
2011年3月— Platform Computing 宣布在它的Symphony软件中支持Hadoop MapReduce API。                                                                                                                                                </br>
2011年5月— Mapr Technologies公司推出分布式文件系统和MapReduce引擎——MapR Distribution for Apache Hadoop。                                                                                                                        </br>
2011年5月— HCatalog 1.0发布。该项目由Hortonworks 在2010年3月份提出，HCatalog主要用于解决数据存储、元数据的问题，主要解决HDFS的瓶颈，它提供了一个地方来存储数据的状态信息，这使得 数据清理和归档工具可以很容易的进行处理。       </br>
2011年4月— SGI( Silicon Graphics International )基于SGI Rackable和CloudRack服务器产品线提供Hadoop优化的解决方案。                                                                                                               </br>
2011年5月— EMC为客户推出一种新的基于开源Hadoop解决方案的数据中心设备——GreenPlum HD，以助其满足客户日益增长的数据分析需求并加快利用开源数据分析软件。Greenplum是EMC在2010年7月收购的一家开源数据仓库公司。                       </br>
2011年5月— 在收购了Engenio之后， NetApp推出与Hadoop应用结合的产品E5400存储系统。                                                                                                                                                </br>
2011年6月— Calxeda公司(之前公司的名字是Smooth-Stone)发起了“开拓者行动”，一个由10家软件公司组成的团队将为基于Calxeda即将推出的ARM系统上芯片设计的服务器提供支持。并为Hadoop提供低功耗服务器技术。                                </br>
2011年6月— 数据集成供应商Informatica发布了其旗舰产品，产品设计初衷是处理当今事务和社会媒体所产生的海量数据，同时支持Hadoop。                                                                                                    </br>
2011年7月— Yahoo!和硅谷风险投资公司 Benchmark Capital创建了Hortonworks 公司，旨在让Hadoop更加鲁棒(可靠)，并让企业用户更容易安装、管理和使用Hadoop。                                                                             </br>
2011年8月— Cloudera公布了一项有益于合作伙伴生态系统的计划——创建一个生态系统，以便硬件供应商、软件供应商以及系统集成商可以一起探索如何使用Hadoop更好的洞察数据。                                                                 </br>
2011年8月— Dell与Cloudera联合推出Hadoop解决方案——Cloudera Enterprise。Cloudera Enterprise基于Dell PowerEdge C2100机架服务器以及Dell PowerConnect 6248以太网交换机                                                               </br>


</span>




* 5 、 hdfs 的优势与劣势


1、HDFS的优势

HDFS的英文全称是 Hadoop Distributed File System，即Hadoop分布式文件系统，它是Hadoop的核心子项目。实际上，Hadoop中有一个综合性的文件系统抽象，它提供了文件系统实现的各类接口.

而HDFS只是这个抽象文件系统的一种实现，但HDFS是各种抽象接口中应用最为广泛和最广为人知的一个。
　　　　
HDFS被设计成适合运行在通用和廉价硬件上的分布式文件系统。它和现有的分布式文件系统有很多共同点，但他和其它分布式文件系统的区别也是明显的。HDFS是基于流式数据模式访问和处理超大文件的需求而开发的，

```
　　　　其主要特点如下：
　　　　1、处理超大文件
　　　　　　这里的超大文件通常指的是GB、TB甚至PB大小的文件。通过将超大文件拆分为小的HDFS和MapReduce，并分配给数以百计、千计甚至万计的的节点，Hadoop可以很容易地扩展并处理这些超大文件。

　　　　2、运行于廉价的商用机器集群上
　　　　　　HDFS设计对硬件需求比较低，只需运行在低廉的的商用机器集群上，而无须使用昂贵的高可用机器。在设计HDFS时要充分考虑数据的可靠性、安全性和高可用性。

　　　　3、高容错性和高可靠性
　　　　　　HDFS设计中就考虑到低廉硬件的不可靠性，一份数据会自动保存多个副本（具体可用设置，通常三个副本），通过增加副本的数量来保证它的容错性。如果某一个副本丢失，HDFS会自动复制其它机器上的副本。
　　　　　　当然，有可能多个副本都会出现问题，但是HDFS保存的时候会自动跨节点和跨机架，因此这种概率非常低，HDFS同时也提供了各种副本放置策略来满足不同级别的容错需求。

　　　　4、流式的访问数据
　　　　　　HDFS的设计建立在更多低相应“一次写入，多次读写”任务的基础上，这意味着一个数据集一旦有数据源生成，就会被复制分发到不同的存储节点中，然后响应各种各种的数据分析任务需求。在多数情况下，分析任务都
　　　　　　会涉及数据集的大部分数据，也就是说，对HDFS来说，请求读取整个数据集比请求读取单条记录会更加高效。

```


2、 劣势局限性能


```
　　　　HDFS的上述种种特点非常适合于大数据量的批处理，但是对于一些特点问题不但没有优势，而且有一定的局限性，主要表现以下几个方面：
　　　　1、不适合低延迟数据访问
　　　　　　如果要处理一些用户要求时间比较短的低延迟应用请求（比如毫秒级、秒级的响应时间），则HDFS不适合。HDFS是为了处理大型数据集而设计的，主要是为了达到高的数据吞吐量而设计的，延迟时间通常是在分钟乃至小时级别。
　　　　　　对于那些有低延迟要求的应用程序，HBase是一个更好的选择，尤其是对于海量数据集进行访问要求毫秒级响应的情况，单HBase的设计是对单行或少量数据集的访问，对HBase的访问必须提供主键或主键范围。
　　　　2、无法高效存储大量小文件
　　　　3、不支持多用户写入和随机文件修改
　　　　　　在HDFS的一个文件中只有一个写入者，而且写操作只能在文件末尾完成，即只能执行追加操作。

```







#### 第二篇论文：MapReduce

> 1 MapReduce 

》紧随其后的就是2004年公布的 MapReduce论文，论文描述了大数据的分布式计算方式，主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，然后将结果合并从而完成大数据处理。

初心：
最初是因为Google要去爬取全球的网页然后对其进行排名(PageRank)而衍生出的问题，一种用于大数据计算的模型，核心思想是大事化小处理，最后在合并结果，先拆分再合并。

传说中，Google使用它计算他们的搜索索引。而Mikio L. Braun认为其工作模式应该是：Google把所有抓取的页面都放置于他们的集群上，然后每天使用MapReduce来重算。



![](assets/000/02/01-1596950198104.png)


![](assets/000/02/01-1596950204474.png)


>2 Map


Map端：

1．每个输入分片会让一个map任务来处理，默认情况下，以HDFS的一个块的大小（默认为64M）为一个分片，当然我们也可以设置块的大小。map输出的结果会暂且放在一个环形内存缓冲区中（该缓冲区的大小默认为100M，由io.sort.mb属性控制），当该缓冲区快要溢出时（默认为缓冲区大小的80%，由io.sort.spill.percent属性控制），会在本地文件系统中创建一个溢出文件，将该缓冲区中的数据写入这个文件。

2．在写入磁盘之前，线程首先根据reduce任务的数目将数据划分为相同数目的分区，也就是一个reduce任务对应一个分区的数据。这样做是为了避免有些reduce任务分配到大量数据，而有些reduce任务却分到很少数据，甚至没有分到数据的尴尬局面。其实分区就是对数据进行hash的过程。然后对每个分区中的数据进行排序，如果此时设置了Combiner，将排序后的结果进行Combia操作，这样做的目的是让尽可能少的数据写入到磁盘。

3．当map任务输出最后一个记录时，可能会有很多的溢出文件，这时需要将这些文件合并。合并的过程中会不断地进行排序和combia操作，目的有两个：1.尽量减少每次写入磁盘的数据量；2.尽量减少下一复制阶段网络传输的数据量。最后合并成了一个已分区且已排序的文件。为了减少网络传输的数据量，这里可以将数据压缩，只要将mapred.compress.map.out设置为true就可以了。

4．将分区中的数据拷贝给相对应的reduce任务。有人可能会问：分区中的数据怎么知道它对应的reduce是哪个呢？其实map任务一直和其父TaskTracker保持联系，而TaskTracker又一直和JobTracker保持心跳。所以JobTracker中保存了整个集群中的宏观信息。只要reduce任务向JobTracker获取对应的map输出位置就ok了哦。

到这里，map端就分析完了。那到底什么是Shuffle呢？Shuffle的中文意思是“洗牌”，如果我们这样看：一个map产生的数据，结果通过hash过程分区却分配给了不同的reduce任务，是不是一个对数据洗牌的过程呢？


> 3 Reduce端：


1．Reduce会接收到不同map任务传来的数据，并且每个map传来的数据都是有序的。如果reduce端接受的数据量相当小，则直接存储在内存中（缓冲区大小由mapred.job.shuffle.input.buffer.percent属性控制，表示用作此用途的堆空间的百分比），如果数据量超过了该缓冲区大小的一定比例（由mapred.job.shuffle.merge.percent决定），则对数据合并后溢写到磁盘中。

2．随着溢写文件的增多，后台线程会将它们合并成一个更大的有序的文件，这样做是为了给后面的合并节省时间。其实不管在map端还是reduce端，MapReduce都是反复地执行排序，合并操作，现在终于明白了有些人为什么会说：排序是hadoop的灵魂。

3．合并的过程中会产生许多的中间文件（写入磁盘了），但MapReduce会让写入磁盘的数据尽可能地少，并且最后一次合并的结果并没有写入磁盘，而是直接输入到reduce函数。



![](assets/000/02/01-1596950548336.png)


> MapReduce 优势与劣势


* 1 优势：

```
MapReduce 的优势

	MapReduce是Google公司的核心计算模型，它将运行于大规模集群上复杂并行计算过程高度抽象为两个函数：Map和Reduce。MapReduce目前非常流行，因为它有如下特点：

　　　	1、MapReduce易于理解：简单地实现一些接口，就可以完成一个分布式程序，而且这个分布式程序还可以分布到大量廉价的PC机器运行。也就是说，写一个分布式程序，跟写一个简单的串行程序是一模一样的。

        MapReduce
　　　　	易于编程的背后是MapReduce通过抽象模型和计算框架把需要做什么（What need to do）与具体怎么做（How to do）分开了，为程序员提供了一个抽象和高层的编程接口和框架，程序员仅需关心其应用层的具体计算问题，
仅需要编写少量的应用本身计算问题的程序代码；如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来，交给计算框架去处理-----从分布代码的执行到大到数千、小到输几个节点集群的自动调度使用。

　　　　2、良好的扩展性
　　　　当计算机资源不能得到满足的时候，可以通过简单的增加机器来扩展它的计算能力。多项研究发现，基于MapReduce的计算性可以随节点数目增长保持近似于线性的增长，这个特点是MapReduce处理海量数据的关键，通过将计算节点增至几百或者几千可以很容易地处理数百TB甚至PB级别的离线数据。

　　　　3、高容错性
　　　　MapReduce设计的初衷就是使程序能部署在廉价的PC机器上，这就要求它具有很高的容错性。比如，其中一台机器宕机了，它可以把上面的计算任务转移到另一个节点上运行，不至于这个任务运行失败，
　　　　　　怎样才能失败任务分发呢，策略如何实现呢？ Hadoop去管

```


* 2 局限性


```
MapReduce  的局限
	MapReduce虽然有很多的优势，但是也有它不擅长的。这里的“不擅长”，不代表不能做，而是在有些场景下实现的效果差，并不适合用MapReduce来处理，主要表现在以下结果方面：

　　　　1、实时计算：MapReduce无法像Oracle或MySQL那样在毫米或秒级内返回结果，如果需要大数据量的毫秒级响应，可以考虑使用HBase.

　　　　2、流计算：流计算的输入数据是动态的，而MapReduce的输入数据是静态的，不能动态变化，这是因为MapReduce自身的设计特点决定了数据源必须是静态的。如果需要处理流式数据可以用Storm,Spark Steaming、Flink等流计算框架。
　　　　3、DGA（有向图）计算：多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入磁盘，会造成大量的词频IO

　　　　导致性能非常低下，此时可以考虑用Spark等迭代计算框架。

```



#### 第三篇论文：BigTable 


BigTable    
    
》采用NoSQL[什么时Nosql](https://blog.csdn.net/a909301740/article/details/80149552)数据库将数据存在一张大表之中，通过牺牲存储的空间来换取性能。Bigtable发布于2006年，启发了无数的NoSQL数据库，比如：Cassandra、HBase等等。Cassandra架构中有一半是模仿Bigtable，包括了数据模型、SSTables以及提前写日志（另一半是模仿Amazon的Dynamo数据库，使用点对点集群模式）。

BigTable 是建立在 GFS 和 MapReduce 之上的。每个Table都是一个多维的稀疏图

为了管理巨大的Table，把Table根据行分割，这些分割后的数据统称为：Tablets。每个Tablets大概有 100-200 MB，每个机器存储100个左右的 Tablets。底层的架构是：GFS。

由于GFS是一种分布式的文件系统，采用Tablets的机制后，可以获得很好的负载均衡。比如：可以把经常响应的表移动到其他空闲机器上，然后快速重建。



> 1 概念

Hbase 概念

![](assets/000/02/01-1596951805023.png)

HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。

HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。

Hbase: 官网中文文档http://abloz.com/hbase/book.html#d613e75 


[列式存储特点](https://blog.csdn.net/nieson2012/article/details/79551337)

> 2 特点


Hbase 有哪些特点：

数据规模大—单表可容纳数十亿行，上百万列。
无模式—不像关系型数据库有严格的Scheme，每行可以有任意多的列，列可以动态增加，不同行可以有不同的列—列的类型没有限制。
稀疏—值为空的列不占存储空间，表可以非常稀疏，但实际存储时，能进行压缩。
面向列族—面向列族的存储和权限控制，支持列族独立查询。数据多版本，利用时间戳来标识版本数据无类型，所有数据以字节数据形式存储

>3 Hbase 架构



HBase中的存储包括HMaster、HRegionSever、HRegion、HLog、Store、MemStore、StoreFile、HFile等，以下是HBase存储架构图：


![](assets/000/02/01-1596951655714.png)


```
HBase中的每张表都通过键按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过256M就要被分割成两个，这个过程由HRegionServer管理，而HRegion的分配由HMaster管理。
 
    HMaster的作用：
为HRegionServer分配HRegion
负责HRegionServer的负载均衡
发现失效的HRegionServer并重新分配
HDFS上的垃圾文件回收
处理Schema更新请求
 
    HRegionServer的作用：
维护HMaster分配给它的HRegion，处理对这些HRegion的IO请求
负责切分正在运行过程中变得过大的HRegion

```

名词

![](assets/000/02/01-1596951792335.png)

```
Store
      每一个HRegion由一个或多个Store组成，至少是一个Store，HBase会把一起访问的数据放在一个Store里面，即为每个ColumnFamily建一个Store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个MemStore和0或者多个StoreFile组成。 HBase以Store的大小来判断是否需要切分HRegion。
     MemStore
     MemStore 是放在内存里的，保存修改的数据即keyValues。当MemStore的大小达到一个阀值（默认64MB）时，MemStore会被Flush到文件，即生成一个快照。目前HBase会有一个线程来负责MemStore的Flush操作。
 　 StoreFile
　 MemStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。
 
     HFile
　　HBase中KeyValue数据的存储格式，是Hadoop的二进制格式文件。 首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。
     HLog 
　  HLog(WAL log)：WAL意为write ahead log，用来做灾难恢复使用，HLog记录数据的所有变更，一旦region server 宕机，就可以从log中进行恢复。 
     LogFlusher 
　 定期的将缓存中信息写入到日志文件中 
     LogRoller　 
　 对日志文件进行管理维护

```

> 3 hbase 优缺点


     Hbase的优点及应用场景：
```

半结构化或非结构化数据:  对于数据结构字段不够确定或杂乱无章非常难按一个概念去进行抽取的数据适合用HBase，因为HBase支持动态添加列。

记录很稀疏： RDBMS的行有多少列是固定的。为null的列浪费了存储空间。HBase为null的Column不会被存储，这样既节省了空间又提高了读性能。

多版本号数据： 依据Row key和Column key定位到的Value能够有随意数量的版本号值，因此对于须要存储变动历史记录的数据，用HBase是很方便的。比方某个用户的Address变更，用户的Address变更记录也许也是具有研究意义的。

仅要求最终一致性： 对于数据存储事务的要求不像金融行业和财务系统这么高，只要保证最终一致性就行。（比如HBase+elasticsearch时，可能出现数据不一致）

高可用和海量数据以及很大的瞬间写入量： WAL解决高可用，支持PB级数据，put性能高适用于插入比查询操作更频繁的情况。比如，对于历史记录表和日志文件。

（HBase的写操作更加高效）业务场景简单： 不需要太多的关系型数据库特性，列入交叉列，交叉表，事务，连接等。

```

```
 Hbase的缺点：

半结构化或非结构化数据: 单一RowKey固有的局限性决定了它不可能有效地支持多条件查询

不适合于大范围扫描查询

不直接支持 SQL 的语句查询

```



到此呢三大论文就暂且告一段落了

---





# Hadoop Other  博主



[hadoop的介绍以及发展历史](https://blog.csdn.net/u012926411/article/details/82756100)