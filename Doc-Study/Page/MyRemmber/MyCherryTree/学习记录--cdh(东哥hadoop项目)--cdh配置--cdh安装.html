<!doctype html><html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>cdh安装 </title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="styles.css" type="text/css" />
</head>
<body><div class="main"><div class="tree">
<p><strong>Index</strong></p>
<p><a href="学习记录.html">学习记录</a></p>

<ol>
<li><a href="学习记录--Needto_Study.html">Needto Study</a></li>
<ol>
<li><a href="学习记录--Needto_Study--NeedStudy_01.html">NeedStudy_01</a></li>
</ol>
<li><a href="学习记录--Sqoop.html">Sqoop</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01.html">Sqoop_01</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop介紹.html">Sqoop介紹</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令.html">Sqoop命令</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop常用命令.html">Sqoop常用命令</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop测试命令.html">Sqoop测试命令</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--Hadoop.html">Hadoop</a></li>
<ol>
<li><a href="学习记录--Hadoop--Hadoop_介绍.html">Hadoop 介绍</a></li>
<li><a href="学习记录--Hadoop--Hadoop环境搭建.html">Hadoop环境搭建</a></li>
</ol>
<li><a href="学习记录--Linux.html">Linux</a></li>
<ol>
<li><a href="学习记录--Linux--linux_零散命令.html">linux 零散命令</a></li>
</ol>
<li><a href="学习记录--Spark.html">Spark</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_介绍.html">Spark 介绍</a></li>
<li><a href="学习记录--Spark--Spark_搭建.html">Spark 搭建</a></li>
<li><a href="学习记录--Spark--Spark_RDD.html">Spark RDD </a></li>
<li><a href="学习记录--Spark--SparkSQl.html">SparkSQl</a></li>
<li><a href="学习记录--Spark--Spark_other.html">Spark other</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_other--Spark_累加器.html">Spark 累加器</a></li>
</ol>
<li><a href="学习记录--Spark--Spark_每日一记.html">Spark 每日一记</a></li>
</ol>
<li><a href="学习记录--Flume.html">Flume</a></li>
<ol>
<li><a href="学习记录--Flume--Flume_介绍、架构.html">Flume 介绍、架构</a></li>
<li><a href="学习记录--Flume--Flume使用.html">Flume使用</a></li>
<ol>
<li><a href="学习记录--Flume--Flume使用--Flume_1701笔记.html">Flume 1701笔记</a></li>
</ol>
</ol>
<li><a href="学习记录--Kafka.html">Kafka</a></li>
<ol>
<li><a href="学习记录--Kafka--Kafka介绍、运行机制.html">Kafka介绍、运行机制</a></li>
</ol>
<li><a href="学习记录--Hive.html">Hive</a></li>
<ol>
<li><a href="学习记录--Hive--Hive_介绍架构.html">Hive 介绍架构</a></li>
<li><a href="学习记录--Hive--Hive_QL_语句.html">Hive QL 语句</a></li>
<li><a href="学习记录--Hive--Hive_UDF.html">Hive UDF</a></li>
<li><a href="学习记录--Hive--Hive东哥笔记.html">Hive东哥笔记</a></li>
<li><a href="学习记录--Hive--hive_装在数据的几种方式.html">hive 装在数据的几种方式</a></li>
<ol>
<li><a href="学习记录--Hive--hive_装在数据的几种方式--01.html">01</a></li>
</ol>
<li><a href="学习记录--Hive--hive的几种join.html">hive的几种join</a></li>
<ol>
<li><a href="学习记录--Hive--hive的几种join--01.html">01</a></li>
</ol>
</ol>
<li><a href="学习记录--Hbase.html">Hbase </a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记.html">东哥笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记--hbase笔记总结.html">hbase笔记总结</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记--Hbase命令笔记_01.html">Hbase命令笔记_01</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--ELK.html">ELK</a></li>
<ol>
<li><a href="学习记录--ELK--ELK总结.html">ELK总结</a></li>
<li><a href="学习记录--ELK--ELK_博客.html">ELK 博客</a></li>
</ol>
<li><a href="学习记录--ETL.html">ETL</a></li>
<ol>
<li><a href="学习记录--ETL--ETL总结.html">ETL总结</a></li>
<li><a href="学习记录--ETL--ETL_博客.html">ETL 博客</a></li>
</ol>
<li><a href="学习记录--服务器云计算.html">服务器云计算</a></li>
<ol>
<li><a href="学习记录--服务器云计算--简单总结.html">简单总结</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置.html">linux 添加ss 的配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--我的.html">我的</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令.html">服务器阶段的命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令.html">下载安装命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令--wget.html">wget</a></li>
</ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置.html">其他配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置--配置命令.html">配置命令</a></li>
</ol>
</ol>
</ol>
</ol>
<li><a href="学习记录--面试方面.html">面试方面</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备.html">代码准备</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01.html">代码准备_01</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01--worldCount准备.html">worldCount准备</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--每天记录.html">每天记录</a></li>
<ol>
<li><a href="学习记录--每天记录--今日整理待整理.html">今日整理待整理</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目).html">cdh(东哥hadoop项目)</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置.html">cdh配置</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装.html">cdh安装 </a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装集群模式.html">cdh安装集群模式</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html">Hadoop项目笔记</a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--项目公共日志.html">项目公共日志</a></li>
</ol>
<li><a href="学习记录--Hadoop、spark等进程.html">Hadoop、spark等进程</a></li>
<ol>
<li><a href="学习记录--Hadoop、spark等进程--Hadoop-spakr-zookeeper-等.html">Hadoop/spakr/zookeeper/等</a></li>
</ol>
<li><a href="学习记录--其他技术.html">其他技术</a></li>
<ol>
<li><a href="学习记录--其他技术--内存与磁盘问题.html">内存与磁盘问题</a></li>
<li><a href="学习记录--其他技术--flink.html">flink</a></li>
<li><a href="学习记录--其他技术--VPN搭建.html">VPN搭建</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas.html">服务器地址pas</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas--free_IP.html">free IP</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--人工智能.html">人工智能</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单.html">人工智能书单</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单--人工智能书单_01.html">人工智能书单_01</a></li>
</ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法.html">人工智能算法</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法--人工智能算法——01.html">人工智能算法——01</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--下载地址.html">下载地址</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--创作社区.html">创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--it创作社区.html">it创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--源码解读.html">源码解读</a></li>
<li><a href="学习记录--其他技术--下载地址--复习语句总结.html">复习语句总结</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--复习语句总结--复习语句总结.html">复习语句总结</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--负载均衡.html">负载均衡</a></li>
<ol>
<li><a href="学习记录--其他技术--负载均衡--负载均衡_01.html">负载均衡_01</a></li>
</ol>
</ol>
<li><a href="学习记录--Phthon.html">Phthon</a></li>
<ol>
<li><a href="学习记录--Phthon--python_使用spark.html">python 使用spark</a></li>
</ol>
<li><a href="学习记录--数据结构.html">数据结构</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1.html">数据结构1</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--Hash_问题.html">Hash 问题</a></li>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树.html">数据结构中的树</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树--数据结构树是怎么遍历的.html">数据结构树是怎么遍历的</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--通信协议.html">通信协议</a></li>
<li><a href="学习记录--开发工具记录.html">开发工具记录</a></li>
<ol>
<li><a href="学习记录--开发工具记录--idea_的一些记录.html">idea 的一些记录</a></li>
<li><a href="学习记录--开发工具记录--博客地址.html">博客地址</a></li>
</ol>
<li><a href="学习记录--数据库.html">数据库</a></li>
<ol>
<li><a href="学习记录--数据库--mysql.html">mysql</a></li>
</ol></ol></div>
<div class="page"><h1><b><u>cdh安装 </u></b></h1><br /><a name="h1-1"></a><br /><a name="h1-1"></a><h1>1.cdh版本hadoop</h1><br /><br /><a name="h2-1"></a><h2>1.1 cdh版本的单机版安装配置：</h2><br /><br />安装之前吧之前全局变量 /etc/profile 里面的配置的环境变量注释，注：jdk如果注释了那么你的hadoop用户没有办法jps，所以可以吧jps打开：<br /><span style="color:#a5e3a5;">    JAVA_HOME=/usr/local/jdk1.7.0 79/<br />    export PATH=$PATH:$JAVA_HOME/bin:</span><br />自己创建hadoop<br /><br />配置 自身的环境变量：<br />vi ~/.bash_profile <br /> 	把/etc/profile 配置文件注释了<br /><br /> 修改权限能够修改配置文件<br /> 	sudo chown -R hadoop:hadoop /home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/<br /> <br />第一步：配置hadoop-env.sh<br />export JAVA_HOME=/usr/local/jdk1.7.0_79/<br />export HADOOP_PID_DIR=/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/hdfs/tmp<br />    这个地方需要设置hadoop_classpath 的路径如果不设置的话以后往logs/12/12 日志写入hdfs会可能会失败！<br />export HADOOP_CLASSPATH=$HBASE_HOME/lib/*:classpath<br />第二步：配置mapred-env.sh<br />export HADOOP_MAPRED_PID_DIR=/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/hdfs/tmp<br />第三步：配置yarn-env.sh<br />export YARN_PID_DIR=/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/hdfs/tmp<br />第四步：配置core-site.xml文件<br />&lt;property&gt;<br />&lt;name&gt;fs.defaultFS&lt;/name&gt;<br />&lt;value&gt;hdfs://hadoop01:9000&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br />&lt;value&gt;/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/hdfs/tmp&lt;/value&gt;<br />&lt;/property&gt;<br />第五步：配置hdfs-site.xml文件<br />&lt;property&gt;<br />&lt;name&gt;dfs.replication&lt;/name&gt;<br />&lt;value&gt;1&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br />&lt;value&gt;/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/hdfs/name&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;dfs.namenode.data.dir&lt;/name&gt;<br />&lt;value&gt;/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/hdfs/data&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;dfs.permissions.enabled&lt;/name&gt;<br />&lt;value&gt;false&lt;/value&gt;<br />&lt;/property&gt;<br />第六步：创建mapred-site.xml文件，直接执行命令cp mapred-site.xml.templete mapred-site.xml<br />第七步：配置mapred-site.xml文件<br />&lt;property&gt;<br />&lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br />&lt;value&gt;yarn&lt;/value&gt;<br />&lt;/property&gt;<br />第八步：配置yarn-site.xml文件<br />&lt;property&gt;<br />&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br />&lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br />&lt;/property&gt;<br />第九步：配置slaves指定datanode节点，将localhost改成主机名<br />第十步：修改环境变量文件".base_profile",并使其生效<br /><br /><br /><br />###### hadoop 2.5.0<br />export HADOOP_HOME=/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6<br />export HADOOP_PREFIX=$HADOOP_HOME<br />export HADOOP_COMMON_HOME=$HADOOP_PREFIX<br />export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop<br />export HADOOP_HDFS_HOME=$HADOOP_PREFIX<br />export HADOOP_MAPRED_HOME=$HADOOP_PREFIX<br />export HADOOP_YARN_HOME=$HADOOP_PREFIX<br />export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<br /><br />对自己免密登陆：<br />	ssh-keygen -t rsa <br />	ssh-copy-id hadoop01<br />	输入密码<br /><br />格式化：<br />	格式化之前自己创建文件夹  mkdir -p ./hdfs/tmp<br />	hdfs namenode -format<br /><br />	然后创建 hdfs/tmp <br />			格式化之后 hdfs/tmp  里面有dfs文件夹，dfs文件夹里面有data、namesecondary文件夹<br /><br />   <span style="color:#000000;"> </span><span style="color:#000000;background-color:#da2929;">切记在hadoop用户开启之后一定要source ~/.bash_profile 再启动，否则 目录读不出来</span><br /><br />	然后开启 start-all.sh <br />		有五个进程：<br />			NodeManager、SecondaryNameNode、NameNode、ResourceManager、DataNode<br /><br />	然后测试：<br />	hdfs dfsadmin -safemode leave(如果有安全模式的话先离开安全模式)<br />	hdfs dfs -put ./test /<br />	hdfs dfs -cat / <br /><br />	yarn测试：<br />		yarn ./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar wordcount /test /out/00 <br /><br />		安全模式离开：<br />			hdfs dfsadmin -safemode leave<br />		<br /><br /><a name="h2-2"></a><h2>1.2  配置聚合日志：</h2><br />开启之后需要重启一下hadoop  <br /><br />在   yarn-site.xml<br /><br /><br /><br />&lt;!--是否开启聚合日志--&gt;<br />&lt;property&gt; <br />  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;<br />    &lt;value&gt;true&lt;/value&gt;<br />	<br />  &lt;property&gt;<br />    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;<br />    &lt;value&gt;86400&lt;/value&gt;<br />  &lt;/property&gt;<br />&lt;!--聚合日志检测时间段,--&gt;<br />&lt;property&gt; <br />    &lt;value&gt;3600&lt;/value&gt;<br />  &lt;/property&gt;<br />&lt;!--聚合日志保存在临时目录下的时间长度--&gt;<br />  &lt;property&gt;<br />    log aggregation is disabled<br />    &lt;/description&gt;<br />    &lt;name&gt;yarn.nodemanager.log.retain-seconds&lt;/name&gt;<br />    &lt;value&gt;10800&lt;/value&gt;<br />  &lt;/property&gt;<br />&lt;!--聚合日志所保存的目录--&gt;<br />  &lt;property&gt;<br />    &lt;description&gt;Where to aggregate logs to.&lt;/description&gt;<br />    &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;<br />    &lt;value&gt;/yarn/logs&lt;/value&gt;<br />  &lt;/property&gt;<br />  <br />  <br /> <br /> &lt;!--uber 模式允许的map的task的最大数--&gt;<br />&lt;property&gt;<br />  &lt;name&gt;mapreduce.job.ubertask.maxmaps&lt;/name&gt;<br />  &lt;value&gt;9&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;!--uber模式允许的最大输入字节数--&gt;<br />&lt;property&gt;<br />  &lt;name&gt;mapreduce.job.ubertask.maxbytes&lt;/name&gt;<br />  &lt;value&gt;67108864&lt;/value&gt;<br />&lt;/property&gt;<br /><br /><br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><a name="h2-3"></a><h2>1.2hive cdh的配置</h2><br /><br />一、 安装mysql<br />	1. 使用root用户: su root<br />	2. 安装<br />		yum install mysql<br />		yum install mysql-server<br />		yum install mysql-devel（可选）<br />	3. 修改配置信息，添加: vim /etc/my.cnf<br />	 service mysql status<br />		[mysql]<br />		default-character-set=utf8<br />		[mysqld]<br />		character-set-server=utf8<br />		lower_case_table_names=1<br />	4. 启动mysql：service mysqld restart/stop/start<br />	5. 配置超级用户名和密码， mysqladmin -u root password 123456<br />	6. root用户登录mysql -u root -p 123456<br />	7. 创建mysql用户：create user 'hive' identified by 'hive';<br />	8. 授权：grant all privileges on *.* to 'hive'@'%' with grant option<br />	9. 重启一下这个mysql服务。<br />	10. 使用hive用户创建数据库并修改连接信息。<br />	   create database cdhhive;<br />	   alter database cdhhive character set latin1;<br />	   <br />	   <br />二、 安装hive  注意hive的元数据必须是latin的，自己什么编码无所谓<br /><br />	1. 下载hive：<a href="http://archive.cloudera.com/cdh5/cdh/5/hive-0.13.1-cdh5.3.6.tar.gz">http://archive.cloudera.com/cdh5/cdh/5/hive-0.13.1-cdh5.3.6.tar.gz</a><br />	2. hive的帮助文档位置：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual，">https://cwiki.apache.org/confluence/display/Hive/LanguageManual，</a><br />		hive官网:<a href="http://hive.apache.org">http://hive.apache.org</a><br />	3. 参考hive-site.xml文件<br />	4. 将bin目录添加到path目录中，<br />	    vim ~/.bash_profile<br />		    export HIVE_HOME=/home/hadoop/bigdater/hive-0.13.1-cdh5.3.6/<br />			export PATH=$PATH:$HIVE_HOME/bin<br />		source ~/.bash_profile<br />	5. 移动mysql驱动jar到hive的lib文件夹下。<br />	6. 启动metastore服务(启动hdfs+yarn服务)<br />	   <span style="background-color:#b97777;"> hive --service metastore &amp; ， 启动之后进程就是 Runjar</span><br />	<br />	    可以通过命令 lsof -i:端口号进行查看<br />	7. 进入hive的客户端<br />		hive<br />    <br />   *  hive-site.xml<br />    <br />&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;<br />&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br /><br />&lt;configuration&gt;<br />  &lt;property&gt;<br />  	&lt;name&gt;hive.metastore.uris&lt;/name&gt;<br />  	&lt;value&gt;thrift://hadoop01:9083&lt;/value&gt;<br />  &lt;/property&gt;<br />  &lt;property&gt;<br />  	&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;<br />  	&lt;value&gt;/hive&lt;/value&gt;<br />  &lt;/property&gt;<br />  &lt;property&gt;<br />  	&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br />  	&lt;value&gt;jdbc:mysql://hadoop01:3306/cdhhive?createDatabaseIfNotExist=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8&lt;/value&gt;<br />  &lt;/property&gt;<br />  &lt;property&gt;<br />  	&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br />  	&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;<br />  &lt;/property&gt;<br />  &lt;property&gt;<br />  	&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br />  	&lt;value&gt;root&lt;/value&gt;<br />  &lt;/property&gt;<br />  &lt;property&gt;<br />  	&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br />  	&lt;value&gt;root&lt;/value&gt;<br />  &lt;/property&gt;<br />&lt;/configuration&gt;<br /><br />	<br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><a name="h2-4"></a><h2>1.3 hbase 配置 </h2><br />1 下载<br />	<a href="http://archive.cloudera.com/cdh5/cdh/5/hbase-0.98.6-cdh5.3.6.tar.gz">http://archive.cloudera.com/cdh5/cdh/5/hbase-0.98.6-cdh5.3.6.tar.gz</a><br />2 解压<br />	（sudo ）tar -zxvf xxx -C 目标目录<br />	同样配置hbase 对应的用户环境变量,(~/.bash_profile)<br />3 修改conf/hbase-env.sh<br />	JAVA_HOME=/usr/local/jdk1.7.0_79/<br />    Hbase的类环境变量：到时候类找不到再看，一班里面是jar包	<br />	HBASE_CLASSPATH=/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/etc/hadoop    #指定habse所需要的hadoop的配置<br />	export HBASE_PID_DIR=/home/hadoop/installed/hbase-0.98.6-cdh5.3.6/hbase/pids<br />	# 下面这个是默认的<br />	export HBASE_MANAGES_ZK=true<br />4 修改hbase-site.xml<br />需要配置的三个地方<br />hbase.rootdir=hdfs://hadoop01:9000/hbase<br />hbase.cluster.distributed=true<br />hbase.tmp.dir=/home/hadoop/installed/hbase-0.98.6-cdh5.3.6/hbase/tmp<br />配置真正数据如下<br />____________________________<br />&lt;property&gt;<br /><br />&lt;name&gt;hbase.rootdir&lt;/name&gt;<br />&lt;value&gt;hdfs://hadoop01:9000/hbase&lt;/value&gt;<br />&lt;/property&gt;<br /><br />&lt;property&gt;<br />&lt;!-- 伪分布式--&gt;<br />&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;<br />&lt;value&gt;true&lt;/value&gt;<br />&lt;/property&gt;<br /><br />&lt;property&gt;<br />&lt;name&gt;hbase.tmp.dir&lt;/name&gt;<br />&lt;value&gt;/home/hadoop/installed/hbase-0.98.6-cdh5.3.6/tmp&lt;/value&gt;<br />&lt;/property&gt;<br />_________<br />	<br />5 修改regionservers<br />	将主机名添加进去，每行一个，单机版的hadoop01<br />6 修改环境变量<br />	添加HBASE_HOME和PATH,在  ~/.bash_profile 里面添加<br />7 启动验证<br />    1.启动前创建 tmp.dir 配置文件配置的目录：<br />        mkdir -p /home/hadoop/installed/hbase-0.98.6-cdh5.3.6/tmp<br />           如果创建文件加没有权限那么对hbase文件夹进行给权限<br />           sudo chown -R hadoop:/hadoop /home/hadoop/installed/hbase-0.98.6-cdh5.3.6<br />	2.start-hbase.sh 开启hbase 服务  ，开启之后有HMaster 、HRegionServer<br />	hbase shell  进入<br />	<br />	3 .创建表进行测试：<br />	    create 'test','cf1'<br />	    list 查看 <br />	    scan ‘test'<br />	    put ‘test’,'rk1','cf1:','zs'    <br />	<br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><a name="h2-5"></a><h2>1.4 Flume </h2>可以不用配置<br /><br />1  可以修改一下安装目录的名字（apache-** ,flume ），Flume 的话就配置一下环境变量（安装目录，bin环境），然后进入conf文件夹<br /><img src="images\147-1.png" alt="images\147-1.png" /><br />修改成flume-env.sh   <br />  <br />2 配置flume-env.sh 的jdk的路径<br /> export JAVA_HOME=/usr/local/jdk1.7.0_79/ <br /><br />使用的话在进行创建文件<br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><a name="h2-6"></a><h2>1.5 Sqoop的配置</h2><br /><br />1. 解压之后配置一下环境变量，当前用户的 hadoop  （~/.bash_profile),配置<span style="background-color:#b97777;">安装目录</span>与<span style="background-color:#b97777;">bin</span>,<br />    吧sqoop-env.sh.template.sh  改成sqoop-env.sh<br /><br /><img src="images\147-2.png" alt="images\147-2.png" /><br /><br />2.配置sqoop-env.sh <br /><br />export HADOOP_COMMON_HOME=/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/<br /><br />export HADOOP_MAPRED_HOME=/home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/<br /><br />export HBASE_HOME=/home/hadoop/installed/hbase-0.98.6-cdh5.3.6/<br /><br />export HIVE_HOME=/home/hadoop/installed/hive-0.13.1-cdh5.3.6/<br /><br />zookeeper没有安装，先不配置<br /><br /><br /><br />3 . 导入mysql的驱动包到sqoop/lib目录下面，（mysql-connector-java-5.1.6-bin.jar <br />    <br />sqoop-help  差看命令<br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br /><a name="h1-2"></a><h1>2 NGINX的配置</h1><br />hadoop项目中采用js埋点以及java应用程序，数据清洗的话hive+mr<br />nginx 不能部署java程序，tomcat <br /><br />默认页面配置在这里面，里面的日志目录如果注释的话就去父目录中去查看<br />ll /etc/nginx/conf.d/default.conf  <br /># 里面有access.log <br />vi /etc/nginx/nginx.conf<br />——access_log  /var/log/nginx/access.log  日志文件<br />修改配置文件 并创建配置的目录<br /> /www/data/source<br /> 并且重启服务器 <br /> <br /> 配置的格式<br />     log_format  log_format   '$remote_addr^A$msec^A$http_host^A$request_uri'; <br />创建资源文件夹，并且创建index.html<br />     mkdir -p  /www/data/source<br />    service nginx restart <br /><br /> 写入内容<br /><img src="images\147-3.png" alt="images\147-3.png" /><br />/home/hadoop/access.log 是日志监控文件<br />/www/data/source 配置的是资源文件夹<br />~~~~~~~~~~~~~~~~~~~~~~<br /><br />user  nginx;<br />worker_processes  1;<br />error_log  /var/log/nginx/error.log warn;<br />pid        /var/run/nginx.pid;<br /><br />events {<br />    worker_connections  1024;<br />}<br /><br />http {<br />    include       /etc/nginx/mime.types;<br />    default_type  application/octet-stream;<br /><br />    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '<br />                      '$status $body_bytes_sent "$http_referer" '<br />                      '"$http_user_agent" "$http_x_forwarded_for"';<br /><br />   <br />    log_format  log_format   '$remote_addr^A$msec^A$http_host^A$request_uri';<br /><br />    sendfile        on;<br />    keepalive_timeout  65;<br />    #include /etc/nginx/conf.d/*.conf;<br /><br />server {<br />	listen       80;<br />    server_name  192.168.111.123   hadoop01  0.0.0.0<br /><br />	   # location ~ .*(BCImg)\.(gif)$ {<br />       # location ~  {<br />	  #default_type image/gif;<br />      access_log /home/hadoop/access.log log_format;<br />      root /www/data/source;  <br />   }<br />}<br />}<br /><br />——————<br />nginx 默认配置文件：<br /><br />user  nginx;<br />worker_processes  1;<br /><br />error_log  /var/log/nginx/error.log warn;<br />pid        /var/run/nginx.pid;<br /><br /><br />events {<br />    worker_connections  1024;<br />}<br /><br /><br />http {<br />    include       /etc/nginx/mime.types;<br />    default_type  application/octet-stream;<br /><br />    include       /etc/nginx/mime.types;<br />    default_type  application/octet-stream;<br /><br />    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '<br />                      '$status $body_bytes_sent "$http_referer" '<br />                      '"$http_user_agent" "$http_x_forwarded_for"';<br /><br />    access_log  /var/log/nginx/access.log  main;<br /><br />    sendfile        on;<br />    #tcp_nopush     on;<br /><br /><br />http {<br />    include       /etc/nginx/mime.types;<br />    default_type  application/octet-stream;<br /><br />    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '<br />                      '$status $body_bytes_sent "$http_referer" '<br />                      '"$http_user_agent" "$http_x_forwarded_for"';<br /><br />    access_log  /var/log/nginx/access.log  main;<br /><br />    sendfile        on;<br />    #tcp_nopush     on;<br /><br />    keepalive_timeout  65;<br /><br />    #gzip  on;<br /><br />    #include /etc/nginx/conf.d/*.conf;<br />    server {<br />        listen 80;<br />        server_name <a href="http://www.lyd.com;">www.lyd.com;</a><br />        #server_name 192.168.111.123<br />        index index.html;<br />        root /data/www/web;<br />        location ~ / {<br />        }<br />     }<br />}<br /><br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /></div></div>
</body></html>