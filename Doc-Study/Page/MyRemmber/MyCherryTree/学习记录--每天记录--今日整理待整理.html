<!doctype html><html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>今日整理待整理</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="styles.css" type="text/css" />
</head>
<body><div class="main"><div class="tree">
<p><strong>Index</strong></p>
<p><a href="学习记录.html">学习记录</a></p>

<ol>
<li><a href="学习记录--Needto_Study.html">Needto Study</a></li>
<ol>
<li><a href="学习记录--Needto_Study--NeedStudy_01.html">NeedStudy_01</a></li>
</ol>
<li><a href="学习记录--Sqoop.html">Sqoop</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01.html">Sqoop_01</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop介紹.html">Sqoop介紹</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令.html">Sqoop命令</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop常用命令.html">Sqoop常用命令</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop测试命令.html">Sqoop测试命令</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--Hadoop.html">Hadoop</a></li>
<ol>
<li><a href="学习记录--Hadoop--Hadoop_介绍.html">Hadoop 介绍</a></li>
<li><a href="学习记录--Hadoop--Hadoop环境搭建.html">Hadoop环境搭建</a></li>
</ol>
<li><a href="学习记录--Linux.html">Linux</a></li>
<ol>
<li><a href="学习记录--Linux--linux_零散命令.html">linux 零散命令</a></li>
</ol>
<li><a href="学习记录--Spark.html">Spark</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_介绍.html">Spark 介绍</a></li>
<li><a href="学习记录--Spark--Spark_搭建.html">Spark 搭建</a></li>
<li><a href="学习记录--Spark--Spark_RDD.html">Spark RDD </a></li>
<li><a href="学习记录--Spark--SparkSQl.html">SparkSQl</a></li>
<li><a href="学习记录--Spark--Spark_other.html">Spark other</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_other--Spark_累加器.html">Spark 累加器</a></li>
</ol>
<li><a href="学习记录--Spark--Spark_每日一记.html">Spark 每日一记</a></li>
</ol>
<li><a href="学习记录--Flume.html">Flume</a></li>
<ol>
<li><a href="学习记录--Flume--Flume_介绍、架构.html">Flume 介绍、架构</a></li>
<li><a href="学习记录--Flume--Flume使用.html">Flume使用</a></li>
<ol>
<li><a href="学习记录--Flume--Flume使用--Flume_1701笔记.html">Flume 1701笔记</a></li>
</ol>
</ol>
<li><a href="学习记录--Kafka.html">Kafka</a></li>
<ol>
<li><a href="学习记录--Kafka--Kafka介绍、运行机制.html">Kafka介绍、运行机制</a></li>
</ol>
<li><a href="学习记录--Hive.html">Hive</a></li>
<ol>
<li><a href="学习记录--Hive--Hive_介绍架构.html">Hive 介绍架构</a></li>
<li><a href="学习记录--Hive--Hive_QL_语句.html">Hive QL 语句</a></li>
<li><a href="学习记录--Hive--Hive_UDF.html">Hive UDF</a></li>
<li><a href="学习记录--Hive--Hive东哥笔记.html">Hive东哥笔记</a></li>
<li><a href="学习记录--Hive--hive_装在数据的几种方式.html">hive 装在数据的几种方式</a></li>
<ol>
<li><a href="学习记录--Hive--hive_装在数据的几种方式--01.html">01</a></li>
</ol>
<li><a href="学习记录--Hive--hive的几种join.html">hive的几种join</a></li>
<ol>
<li><a href="学习记录--Hive--hive的几种join--01.html">01</a></li>
</ol>
</ol>
<li><a href="学习记录--Hbase.html">Hbase </a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记.html">东哥笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记--hbase笔记总结.html">hbase笔记总结</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记--Hbase命令笔记_01.html">Hbase命令笔记_01</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--ELK.html">ELK</a></li>
<ol>
<li><a href="学习记录--ELK--ELK总结.html">ELK总结</a></li>
<li><a href="学习记录--ELK--ELK_博客.html">ELK 博客</a></li>
</ol>
<li><a href="学习记录--ETL.html">ETL</a></li>
<ol>
<li><a href="学习记录--ETL--ETL总结.html">ETL总结</a></li>
<li><a href="学习记录--ETL--ETL_博客.html">ETL 博客</a></li>
</ol>
<li><a href="学习记录--服务器云计算.html">服务器云计算</a></li>
<ol>
<li><a href="学习记录--服务器云计算--简单总结.html">简单总结</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置.html">linux 添加ss 的配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--我的.html">我的</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令.html">服务器阶段的命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令.html">下载安装命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令--wget.html">wget</a></li>
</ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置.html">其他配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置--配置命令.html">配置命令</a></li>
</ol>
</ol>
</ol>
</ol>
<li><a href="学习记录--面试方面.html">面试方面</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备.html">代码准备</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01.html">代码准备_01</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01--worldCount准备.html">worldCount准备</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--每天记录.html">每天记录</a></li>
<ol>
<li><a href="学习记录--每天记录--今日整理待整理.html">今日整理待整理</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目).html">cdh(东哥hadoop项目)</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置.html">cdh配置</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装.html">cdh安装 </a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装集群模式.html">cdh安装集群模式</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html">Hadoop项目笔记</a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--项目公共日志.html">项目公共日志</a></li>
</ol>
<li><a href="学习记录--Hadoop、spark等进程.html">Hadoop、spark等进程</a></li>
<ol>
<li><a href="学习记录--Hadoop、spark等进程--Hadoop-spakr-zookeeper-等.html">Hadoop/spakr/zookeeper/等</a></li>
</ol>
<li><a href="学习记录--其他技术.html">其他技术</a></li>
<ol>
<li><a href="学习记录--其他技术--内存与磁盘问题.html">内存与磁盘问题</a></li>
<li><a href="学习记录--其他技术--flink.html">flink</a></li>
<li><a href="学习记录--其他技术--VPN搭建.html">VPN搭建</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas.html">服务器地址pas</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas--free_IP.html">free IP</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--人工智能.html">人工智能</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单.html">人工智能书单</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单--人工智能书单_01.html">人工智能书单_01</a></li>
</ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法.html">人工智能算法</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法--人工智能算法——01.html">人工智能算法——01</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--下载地址.html">下载地址</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--创作社区.html">创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--it创作社区.html">it创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--源码解读.html">源码解读</a></li>
<li><a href="学习记录--其他技术--下载地址--复习语句总结.html">复习语句总结</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--复习语句总结--复习语句总结.html">复习语句总结</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--负载均衡.html">负载均衡</a></li>
<ol>
<li><a href="学习记录--其他技术--负载均衡--负载均衡_01.html">负载均衡_01</a></li>
</ol>
</ol>
<li><a href="学习记录--Phthon.html">Phthon</a></li>
<ol>
<li><a href="学习记录--Phthon--python_使用spark.html">python 使用spark</a></li>
</ol>
<li><a href="学习记录--数据结构.html">数据结构</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1.html">数据结构1</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--Hash_问题.html">Hash 问题</a></li>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树.html">数据结构中的树</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树--数据结构树是怎么遍历的.html">数据结构树是怎么遍历的</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--通信协议.html">通信协议</a></li>
<li><a href="学习记录--开发工具记录.html">开发工具记录</a></li>
<ol>
<li><a href="学习记录--开发工具记录--idea_的一些记录.html">idea 的一些记录</a></li>
<li><a href="学习记录--开发工具记录--博客地址.html">博客地址</a></li>
</ol>
<li><a href="学习记录--数据库.html">数据库</a></li>
<ol>
<li><a href="学习记录--数据库--mysql.html">mysql</a></li>
</ol></ol></div>
<div class="page"><h1><b><u>今日整理待整理</u></b></h1><br />  ◇ <a href="学习记录--每天记录--今日整理待整理.html#h2-1">1 every day tostudy hadoop大数据相关</a><br />  ◇ <a href="学习记录--每天记录--今日整理待整理.html#h2-2">2 .other need to study soft</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-1">2017/12/09 - 11:19</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-2">2017/12/09 - 11:02</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-3">2017/12/11 - 09:18</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-4">2017/12/12 - 09:16</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-5">2017/12/13 - 21:06</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-6">2017/12/14 - 09:07</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-7">2017/12/15 - 11:46</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-8">2017/12/16 - 10:11</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-9">2017/12/20 - 09:36</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-10">2017/12/21 - 10:53</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-11">2017/12/24 - 10:50</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-12">2017/12/25 - 21:43</a><br />• <a href="学习记录--每天记录--今日整理待整理.html#h1-13">2017/12/26 - 13:09</a><br /><br /><br /><br /><a name="h2-1"></a><strong><h2>1 every day tostudy hadoop大数据相关</h2></strong><br /><div class="codebox"><div class="codebox">1、加载数据<br />2、框架选择框架<br />3、优化<br />4、测试数据<br />闭环<br />两点：存储，分析<br />输入：hive+&nbsp;hdfs&nbsp;+hbase+redis&nbsp;<br />可视化：放到mysql中<br />数据采集+数据清洗（处理，清洗-》预处理）&nbsp;+&nbsp;数据建模+数据可视化（数据展示-要用到web接口）<br />用户画项&nbsp;：就是改变数据当中的字段，然后客户访问了进行了某个动作之后跟字段刚好进行匹配，钟下种子<br />&nbsp;<br />大数据本质：结构化&nbsp;,体系化<br />hdfs&nbsp;hbase&nbsp;hive&nbsp;spark&nbsp;-&gt;&nbsp;&nbsp;yarn&nbsp;zookeeper&nbsp;&nbsp;:解决数据依赖性<br />体系化：<br />离线、实时、在线&nbsp;&nbsp;hadoop&nbsp;spark&nbsp;离线处理&nbsp;&nbsp;<br />1&nbsp;hadoop<br />&nbsp;<br />2&nbsp;&nbsp;spark:&nbsp;实时<br />前提：&nbsp;kafka&nbsp;elasticsearch&nbsp;<br />&nbsp;<br />&nbsp;<br />3&nbsp;storme&nbsp;&nbsp;流式计算</div></div><br /> <br /> <br /> <br /> <br />2017.11.09<br /> <br />海鹏： 区块链<br /><a href="https://www.zhihu.com/question/37290469">https://www.zhihu.com/question/37290469</a><br /> <br /> <br />Kafka direct 的机制 还有一种 机制，<br />Optional&lt;Boolean&gt;&gt;：这个是google的，项目中注意别到错了 <br />2017.11.10 <br />李天祥：<br />*访问者设计模式：<br />*通过开窗函数：<br />*   Broadcast ：广播存到内存，减少io<br />*   s4,storm,puma 流处理框架<br />**  问优化，调优（包括spark，hadoop,数据倾斜等）<br />* 拓扑<br /><strong>5. 学技术要学主流框架</strong>，学 Node 要学 Express，学 Java 就不要忘了 SSH，学 Ruby 首学 Rails。<br />11.11 ,周六<br />Docker flink <br />11.12<br />企业设计模式<br />https://www.cnblogs.com/wanliwang01/p/6207842.html<br /> <br />Cdh<br /> <br />11.13  周一： <br />Storm 项目开始：<br /> <br /> <br />Spark  有误mapped reduce 端，<br /> <br /> <br /> <br />11.15 陈高峰 代做项目<br /> <br /><span style="background-color:#ff0000;">Spark比较重要。</span>hadoop hdfs 份额不高，storm 大公司会用。<br />往架构师方向发展，偏向业务，业务有钱数据为辅，数据部门缺钱<br /> <br />--  微信号 ：架构师之路<br />反向代理，dns轮询 ，<br /> <br /> <br /> <br />-- 关键词<br />UpdateStateByKey   <br />数据算法<br /><img src="images\30-1.png" alt="images\30-1.png" /> <br />-- 软件<br />wireshark<br /> <br />推荐系统，session分析，黑名单，安利<br />NFS网络文件系统<br />负载均衡<br />普通负载均衡<br /><a href="http://developer.51cto.com/art/201609/517313.htm">http://developer.51cto.com/art/201609/517313.htm</a><br />DNS；<br /><a href="http://369369.blog.51cto.com/319630/812889/">http://369369.blog.51cto.com/319630/812889/</a><br /> <br />软件级别负载均衡对比<br /><a href="http://network.51cto.com/art/201101/241997.htm">http://network.51cto.com/art/201101/241997.htm</a><br /> <br /> <br /> <br /> <br />11.16 号 <br /><br /> <br />曼哈顿算法，皮尔逊算法 docker<br />11.17 <br />zookeeper底层选举机制，kafka均衡算法<br />zab,paxos <br />zookeeper的选举表现形式，原理<br />spark，kafka，sparkstreaming如何容错<br />涉及技术<br />Hadoop2.x、Zookeeper、Flume、Hive、Hbase、Kafka、Spark2.2、SparkStreaming、SparkSQL、StructuredStreaming、MySQL、Hue、J2EE、websoket、Echarts <br /> <br />11.21 周二<br />公司集群用非root用户；<br />大招老师讲：<br />特征：lable	a	b	c<br />-1		a:3	b:3	c:3<br />1		a:2	b:2	c:2<br />		a:3	b:4	c:4<br />索引：  字典，index<br />		a3<br />		a2<br />		b3<br />		b2<br />		b4<br />		c3<br />		c2<br />		c4<br /> <br />       转boolean类型<br />		1	0	1	0	0	1	0	0<br />		0	1	0	1	0	0	1	0<br />		1	0	0	0	1	0	0	1<br /> <br /> <br />Hbase:的<br />count ‘表名’<br />Scan ‘表名’ , {LIMIT =&gt;10 }<br />Zookeeper的选举机制<br />Kafka 的选举机制：<br />Hbase是如何查询数据的，<br /> <br />11.22  <br />分享：<br />朱高鵬： 协同过滤算法，（基于用户、商品、离线、在线推荐）<br />深度学习，机器学习<br />大数据-大数据架构师；算法设计师；产品经理（需求分析师）<br /><img src="images\30-2.png" alt="images\30-2.png" /> <br />Python2:运维<br />Python3:做spark 脚本上传，<br /> <br />msr基于windows  <br /> <br /> <br /> <br />Today need tostudy:<br />大数据产品：百度地图（流量报警系统），三大运营商（精准服务，报警系统）<br />总结：<br />1、精准营销：OA(综合系统，电商的)<br />2、传统行业：（三大运营商，百度地图，）的报警系统：  某时刻输入一个接口数据超过某个值，报警大屏幕。| 过年-人口迁徙，| 智慧城市- 人口，交通 | <br />3、无人机，汽车：  （AI,大数据，人工系统，神经网络）<br /> <br /> <br />大数据商业顾问<br /> <br />2017.11.23<br />Id 技术部，产品部，产品经理；<br />北青的只论技术<br />埋点也叫锚点就是加密了<br />RDD:<span style="background-color:#ff0000;">复用</span>redis <br />cookie 一般都加密，怎么<span style="background-color:#ff0000;">加密</span>怎么解密<br /><span style="color:#0000ff;">RDD算子调优：</span><br />团队合作中最重要的是<span style="color:#0000ff;">业务逻辑</span><br /><span style="color:#0000ff;"> </span><br />想问题看逻辑看问题代码<br />大数据小组：标配4个<br />两个算法，<br /> <br />*  数据清洗：MR hive <br />小规模的hive(hql) ，几百G的用MR.<br /> <br />Spark-core ：算子（看调优）<br />Spark-sql  : sql   <br />Spark-Streaming : 也可以做数据清洗<br />Boolean类型转换<br /> <br />*  一定会有hive  hbase <br /> <br />*  科研、工业做产出<br /> <br />* spark基于 <br /> <br />* 运维：网络协议比如rpc 什么的<br /> <br />* impala  比hive快很多<br /> <br /> <br />*<br />ELK<br />Elasticsearch + Logstash + Kibana<br /> <br /><strong><span style="color:#ff0000;">ETL（数据仓库技术）</span></strong><br />用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程<br /> <br /> <br /> <br />*<br /> <br />Shuffle相关<br /><a href="http://spark-config.readthedocs.io/en/latest/shuffle.html">http://spark-config.readthedocs.io/en/latest/shuffle.html</a><br />Storage相关配置参数<br /><a href="http://spark-config.readthedocs.io/en/latest/storage.html">http://spark-config.readthedocs.io/en/latest/storage.html</a><br />压缩和序列化相关<br /><a href="http://spark-config.readthedocs.io/en/latest/compress.html">http://spark-config.readthedocs.io/en/latest/compress.html</a><br />Schedule调度相关<br /><a href="http://spark-config.readthedocs.io/en/latest/scheduler.html">http://spark-config.readthedocs.io/en/latest/scheduler.html</a><br /> <br /> <br /> <br />11.24  ：<br />负责线下，其他就不管了，（看下项目）<br /> <br />四大数据，两算法，一老大，<br />1. 20-30  70-80  几百 来号人<br />站内搜索，locy sory 什么的 ，<br />* mappreduce spark 运行时间，处理的数据量<br /> <br />Shuffle 之前，transform 类型的时候持久化到内存中，遇到action算子之前不释放内存，遇到job 的时候，stage 切分<br /> <br />* 有思路，磕巴，可以不对<br /> <br />*  join hbase elticsearch <br /> <br />* 1 。hadoop 体系<br /> <br />Bigtable (hbase) hdfs mappreduce（mr最早的计算模型） yarn <br />2 . MR 的过程<br />Jobtricker<br />3  Hive 自定义函数udf <br />* talkingData <br />* Hive  分区分桶<br /> <br />Es  IR :  elasticSearch  <br /> <br /> <br />Openstack  docker :<br /> <br />* 分析业务，分析数据量，分析技术实现，数据库表设计，功能实现，功能调试性能调优，数据倾斜<br /> <br /> <br />一 、项目介绍：<br />自我介绍<br /> <br />日志分析<br /> <br />项目组有多少人  3<br /> <br />开发周期 9个月<br /> <br />数据量==40G<br /> <br />拿到的数据量 ==<br /> <br />集群规模<br />Hadoop 计算了多少时间<br />spark  计算了多少时间<br /> <br /> <br />总框架描述<br /> flume：采集数据，离线的用spooldir（type）收集数据，在线的用exec（type），10台服务器（flume），集群（3）<br /> hdfs：指定hdfs：//ip和端口/data/日期<br /> kafka：在线工具，实时收集flume数据，flume的sink接kafka，配上sink到的消息队列名称（topic）<br /> hive：清洗工作 hql语句过滤掉无效数据（空数据、不完整数据等）<br /> spark：core streaming sql mllib （分析、统计数据、计算模型）<br /> MySQL：存储计算结果<br /> <br />你负责的框架是什么<br /> <br />二、流程<br /> <br />数据获取<br />1 log4j日志传到flume里<br /> <br />数据处理<br />1 hive过滤无效数据，格式转换（udf函数）：map（k，ip） （k，city）<br /> <br /> <br />数据储存（核心）<br />1 存储：2张表，第一张load源数据表，第二张add当天日期的parttion并插入inset清洗过的数据<br /> <br />数据分析（核心）<br />1 广告分析模块：streaming{（reduceBykeyAndwindo）：设置每隔10s计算前一个小时收集到的数据、updatestateBykey（累加每个用户每个广告点击次数，获取生成黑名单list）、transform（-----））}<br />core（-------） <br />sql（-----）<br />2 游戏统计：3*sql<br />3 推荐模型：算法（协同过滤(本质：找相似度)和逻辑回归、场景：跟线性回归区别）<br />（1）加载逻辑回归算法的输入文件data，标签label和feature按\t分开<br />（2）构建特征字典dict：获取所有特征并去重，data.map(_.2).flatMap(_.split()).distinct.zipWithIndex().collectAsMap()，转化成map[特征，索引]<br />（3）构建训练数据：<br />		 val trainData = data.map(sample=&gt;{<br />				调用map算子将data中的label转为double类型,直接sample(0) match{case "1" =&gt; 1.0 case "-1" =&gt; -1.0}<br />				得到每条样本的特征索引：得到每条样本的特征(抽取每条数据的特征)feature:Array[]=data.map(_.2).split().map(_.1)<br />												  indexs=feature.map(x=&gt;dict.get(x) match{case Some(x) =&gt; x<br />																				case None =&gt; -1})<br />				new LabeledPoint(lable,new SparseVector(dict.size,indexs,Array.fill(indexs.length)(1.0))//数据集 lablepoint格式<br />		  })<br />																					<br />（4）随机梯度下降的逻辑回归算法训练:val model: LogisticRegressionModel =  LogisticRegressionWithSGD.train(trainData,10,0.1)<br />（5）输出模型文件：       //模型特征值  val pw = new PrintWriter("F://test.txt")<br />						  val weights: Array[Double] = model.weights.toArray //所有的特征系数返回到Array里<br />						  // 将字典反转为了找到对应下标的特征名称<br />						  val map: Map[Long, String] = dict.map(x=&gt;(x._2,x._1))<br />						  for(i &lt;- 0 until(weights.length)){<br />						  //取得每个特征权重的特征名称<br />						  val feature: String = map.getOrElse(i,"")<br />						  pw.write(feature+"\t"+weights(i))<br />（6）模型评估率高低：针对AUC公式进行参数调优<br />数据展现<br /> <br />数据应用   <br /> <br /> <br />三 具体项目<br /> <br />广告分析：用到什么技术点？什么算子？介绍算子?能处理什么业务？<br />热点：以此类推<br />推荐：什么算法？应用场景？特征抽取？评估好坏？<br /> <br />四 算法使用和场景<br />------------------<br /> <br />五业务逻辑训练<br />1.Hadoop体系<br />2.计算框架类<br />*****二者与业务整合<br /> <br />"""<br />行话，什么是。<br /><br />    0.用户画项：比如 0 是用户的id，还是什么，1 代表的是用户机器类型<br />    1.画原型：<br />        产品流程，业务流程<br />        （项目原型就是产品流程，与销售业务沟通，与客户等沟通）<br />    2.建模：<br />        写world文档，怎么营销，把工作流程写下来比较细<br />        产品，技术流程<br />        * 图图里面有图标每个图标是干嘛的，建一个逻辑的模型。<br />        * 比如把某一年的业绩数据弄成图标；<br />        * 比如滴滴怎么弄一个更赚钱的软件；<br />        <br />        本身就是本身把要做的工程给写出来<br />        <br />    3.闭环<br />        工程师定期给你指标，不同人多少会点其他人的技术，         <br />一、IT技术研发部：<br />1.大数据项目（传统项目（农业，病毒），互联网项目-金融项目有埋点，监控系统，运营商，cookie，session，分析url，分析ds广告投放）<br />2.业务分析 --行话！<br />3.数据量分析（集群、年、月、日定期跑数据），半夜凌晨脚本自动搜集数据，<br />4.技术实现（数据获取（sdk，或者传输工具），收集、存储，分析，展现，产品数据应用）<br />    -1-大数据小组团队<br />    -2- 大数据框架：SDK(log4J+JAVA+JS就是软件开发工具发送日志)+Nginx集群+Lvs、传感器 <br />    + Flume + Kafka  （搜集数据，每秒百万kafka）<br />    + HDFS 2.x + Hbase  （存储）<br />    + MR/Hive/Spark/storm （分析） <br />    + Mysql （可视化）<br />    + alays online （上线）<br />    互联网项目sdk ，非互联网项目传感器 <br />5.数据库表设计（关系型数据库mysql,oracle &amp;&amp; 非关系型数据库  hbase ,redis,往往解决物理存储，hbase永远是做存储用的）<br />    列簇是怎么设计的：<br />6.代码实现 ，用那些spark工具类，包<br />7.线下功能调试<br />    shuffler ，jvm调优，oom等 问题<br />8.性能调优，时间效率，空间换时间，site文件等，网络流量调优，mappreduce调优，combiner<br />9.数据倾斜 （热点）<br />    大量的task汇聚到一台机器上，前几台为机器都执行完了，唯独很少的几台拉低了整个集群业务的时间，广播变量：boradcast发到其他机器上数据量小的<br />10.TroubleShooting(机器故障)：物理环境，代码层，网络流量激增。<br />"""<br /> <br /> <br />2017.11.27 <br />Jupyter :可以编程的<br />Jupyter notebook安装与使用<br />同声传译<br />*<br />Shuffle  过程的切片，shuffle  具体详解<br /><a href="https://www.cnblogs.com/bonelee/p/6039202.html">https://www.cnblogs.com/bonelee/p/6039202.html</a><br /> <br />Map 与字典的区别与不同<br />Blockmanager <br />Spark submit 与spark shell的关系<br />序列化的目的，优劣<br />Hdfs 运行机制<span style="background-color:#ffffff;"><br /><br /><br /><br /></span><div class="codebox"><div class="codebox">(1)fs是文件系统，&nbsp;dfs是分布式文件系统<br />(2)fs&nbsp;&gt;&nbsp;dfs<br />(3)分布式环境情况下，fs与dfs无区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />(4)本地环境中，fs就是本地文件，dfs就不能用了<br />and<br />FS涉及到一个通用的文件系统，可以指向任何的文件系统如local，HDFS等。但是DFS仅是针对HDFS的。<br />那么什么时候用FS呢？可以在本地与hadoop分布式文件系统的交互操作中使用。特定的DFS指令与HDFS有关</div></div><br /><br /><br /><br />Kafka 内部解析：零拷贝机制：<br /><a href="https://www.baidu.com/baidu?tn=monline_3_dg=utf-8=kafka%E9%9B%B6%E6%8B%B7%E8%B4%9D">https://www.baidu.com/baidu?tn=monline_3_dg=utf-8=kafka%E9%9B%B6%E6%8B%B7%E8%B4%9D</a><br />* Hdfs 中block ，spark 中的block<br /><a href="http://blog.csdn.net/u010990043/article/details/77683823">http://blog.csdn.net/u010990043/article/details/77683823</a><br /> <br />* Kafka还有一个非常优秀的机制就是DelayQueue机制<br />* spark RDD中的核心方法<br />* RDD的storage level不是NONE的话，引申出来RDD storage level 的级别<br />* RDD 的数据管理策略 <br />* hadoop资源调度模式<br />hadoop yarn资源调度的博客<br /><br /> <br />11.28 号 文峰： phoenix <br /> <br />问题：你对spark了解多少<br /> <br />一天100G 7个集群，1-3分钟，代码三个月，bug一个月，调优一个月，性能调优，比如永久代，shuffle，文件找不到<br /> <br />简单介绍一下Spark：<br />1. 什么事Spark? <br />2. Spark可以做什么？<br />3. Spark组件？<br />4. Spark结构图<br />5. Spark与hadoop区别？<br />6. Spark流程图？<br /> <br />* 数据源<br />Nginx hbase 发过来的数据<br /> <br />* 大多数底层数据结构都是B树，<br /> <br />* <br />*B树，普通二叉树 的区别<br />lsm树，hbase底层的数据结构<br /><a href="https://www.cnblogs.com/yanghuahui/p/3483754.html">https://www.cnblogs.com/yanghuahui/p/3483754.html</a><br /><br /> <br /> <br /> <br />Hbase 的hfile 数据<br /> <br /> <br />* 新大数据框架 什么麒麟 阿里啊的一些框架<br /> <br />* 二项分布，正向分布，随机分布，高斯函数，透视表，<br /> <br />*<br /> <br />周四周五：数据可视化<br />11.29  周三<br /> <br />http://google.apachecn-ladder.xyz/ <br /> <br />Svm knn dnn 神经网络 |用数学的眼光看算法，用统计的眼光看世界<br /> <br />抽取（extract）、转换（transform）、加载（load）<br /> <br />技术顾问、分享，刚好赶上大数据，python等都能做，<br />40G-10G<br />100G-25G  <br /> <br />西二旗中关村，朝阳<br /> <br />* HA高可用搭建，Spark core ， Spark Streaming  hive  hbase （三个女生）<br /> <br />* Spark-Sql 的练习thrift <br /><a href="http://blog.csdn.net/book_mmicky/article/details/39152727">http://blog.csdn.net/book_mmicky/article/details/39152727</a><br /><br />Hive  thrift server metastore<br /><a href="http://blog.163.com/captain_zmc/blog/static/204012588201401734250298/">http://blog.163.com/captain_zmc/blog/static/204012588201401734250298/</a><br /><br /> <br />dfs 与fs 区别：<br /><br />11.30:<br />Issm<br />12.1 周五<br />站长之家<br />站长工具、<br />爱站网<br />SageMaker 平台<br /><a href="https://mp.weixin.qq.com/s/wjDsgzGma2Keb1zRgvokPw">https://mp.weixin.qq.com/s/wjDsgzGma2Keb1zRgvokPw</a><br />十年大数据，十年区块链，两技术如何共生演进?<br /> <br /><a href="https://www.cnblogs.com/meet/p/5435979.html">https://www.cnblogs.com/meet/p/5435979.html</a><br /> <br />12.1 <br />跟hr问的：<br /> 工作地点、项目组人、他们所用的技术<br />Flume 中的NG  OG  还有其他的面试题<br /> <br /> <br /> <br />12.2 ：<br />Hdfs 精准介绍：<br /><a href="http://www.aboutyun.com/thread-6966-1-1.html">http://www.aboutyun.com/thread-6966-1-1.html</a><br />Mapreduce ：<br />  执行流程<br /><a href="http://blog.jobbole.com/84089/">http://blog.jobbole.com/84089/</a><br /> <br />Hbase:的region<br /><a href="http://www.aboutyun.com/blog-3779-366.html">http://www.aboutyun.com/blog-3779-366.html</a><br />Spark 的介绍，包括cache checkpoint ，还有broadcast机制博客：<br /><a href="https://www.kancloud.cn/kancloud/spark-internals/45239">https://www.kancloud.cn/kancloud/spark-internals/45239</a><br />腾讯几个面试题：<br /> <br /><a href="http://blog.csdn.net/lisonglisonglisong/article/details/51327586">http://blog.csdn.net/lisonglisonglisong/article/details/51327586</a><br />Spark 今日头条解析<br /> <br /><a href="https://www.toutiao.com/a6494479282935431694/?tt_from=mobile_qq_campaign=client_share=news_article_source=mobile_qq=18096290818_medium=toutiao_android">https://www.toutiao.com/a6494479282935431694/?tt_from=mobile_qq_campaign=client_share=news_article_source=mobile_qq=18096290818_medium=toutiao_android</a><br /> <br /> <br />2017.12.3 <br />Hive 优化<br /><a href="http://blog.csdn.net/javastart/article/details/52387979">http://blog.csdn.net/javastart/article/details/52387979</a><br /><a href="http://blog.csdn.net/xiantian7/article/details/51596714">http://blog.csdn.net/xiantian7/article/details/51596714</a><br /> <br />2017.12.4 <br />懂金融、会股票、会算法、<br />中华文本库：<br /><a href="http://www.chinadmd.com/">http://www.chinadmd.com/</a><br />Kafka的一部分中华文本库<br /><a href="http://www.chinadmd.com/file/tsaropaprvouoizwsvzotz63_4.html">http://www.chinadmd.com/file/tsaropaprvouoizwsvzotz63_4.html</a><br /> <br /> <br />Hdfs 中的block 块问题大数据趟过的坑<br /><a href="https://mp.weixin.qq.com/s/-1VHNpIwW6vtiKnmrPNaiw">https://mp.weixin.qq.com/s/-1VHNpIwW6vtiKnmrPNaiw</a><br />Hive 与数据库的区别<br /><a href="https://mp.weixin.qq.com/s?__biz=MzU0ODI5NTM1MQ===2247484571=1=f56415634aa23c27cec06119acb579b4=fb400343cc378a55adf2dbccdd2f63b84cfde307c55bd10d63973581b75b25cb6d25e46138ab=1=23=1203xaVsuVOLvUcDwAc11STK#rd">https://mp.weixin.qq.com/s?__biz=MzU0ODI5NTM1MQ===2247484571=1=f56415634aa23c27cec06119acb579b4=fb400343cc378a55adf2dbccdd2f63b84cfde307c55bd10d63973581b75b25cb6d25e46138ab=1=23=1203xaVsuVOLvUcDwAc11STK#rd</a><br />收益：<br />第一次设计有缺陷，改进方案 你懂得<br />  1.竞标第一次项目<br />  2.根据大数据项目营销投放，100 app 收益改变 4%-9% <br />Other：<br />* 西门子<br />*SK 算发包<br />* python机器学习库scikit-learn<br />* 讯飞科技 （做语音识别）<br />* hadoop大数据能存储的内容<br />* 1258网站目录<br />* 事务的隔离性的必要性，线程安全<br />*反转二叉树<br /> <br />2017.1205 <br />浅谈mysql优化<br /><a href="http://www.jb51.net/article/69984.htm">http://www.jb51.net/article/69984.htm</a><br />Mongodb命令<br /><a href="http://www.jb51.net/article/48217.htm">http://www.jb51.net/article/48217.htm</a><br /><a href="http://www.jb51.net/article/87978.htm">http://www.jb51.net/article/87978.htm</a><br />Shell 面试<br /><a href="http://www.jb51.net/article/51574.htm">http://www.jb51.net/article/51574.htm</a><br />* 调“超参”，学习参，超参。Hadoop是一个系统，二Spark是一个计算框架，想要完全替代不可能<br />2017.12.6<br /><a href="http://www.cnblogs.com/zlslch/">http://www.cnblogs.com/zlslch/</a><br />你那个技术掌握的最牛逼、优化的一些、oom内存溢出数据倾斜<br />Flume<br />* Mapreduce、spakr 中的mapr、reduce 的区别<br /><a href="https://www.cnblogs.com/yangsy0915/p/5528774.html">https://www.cnblogs.com/yangsy0915/p/5528774.html</a><br />* 血统。Lineage <br />* Spark-Sql 比hive块 <br />*Flume 的底层原理flume与其他logstash 、kafka 相比较来说搜集数据的优势在哪里<br />*Mysql能够承载多大<br />*马辉面试：<br />  Scala框架、Spark 、ELK、数据清洗是用什么，流程是什么。、hive分词提取器、hive的情感关系<br /> <br />2017.12.8 <br />twitter的storm和yahoo的S4<br />展示的数据 就存到olap，mysql数据库中，<br />慢查询的话存到hive中也行<br /> <br /><br /><a name="h2-2"></a><strong><h2>2 .other need to study soft</h2></strong><br /><div class="codebox"><div class="codebox">1&nbsp;、版本管理工具<br />Svn、git、(cvs)&nbsp;<br />2.&nbsp;管理网站<br />禅道<br />1.&nbsp;论坛<br />About、过往记忆、archive（列出apache的所有的项目）、</div></div><br /><br /><br /><a name="h1-1"></a><h1>2017/12/09 - 11:19</h1><br />spark  运行时间<br /><a href="http://www.aboutyun.com/thread-17085-1-1.html">http://www.aboutyun.com/thread-17085-1-1.html</a><br /><br /><a href="http://forum.huawei.com/enterprise//thread-326169.html">http://forum.huawei.com/enterprise//thread-326169.html</a> <br /><br />ll /etc/nginx/conf.d/default.conf<br />vi /etc/nginx/nginx.conf<br /><br />• hbase 列簇设计原则<br />• 数据量<br /><br /><a name="h1-2"></a><h1>2017/12/09 - 11:02</h1><br />脚本之家的spark处理时间、清洗算法等<br /><a href="http://so.jb51.net/cse/search?q=spark%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E5%88%86%E9%92%9F&click=1&s=10520733385329581432&nsid=">http://so.jb51.net/cse/search?q=spark%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E5%88%86%E9%92%9F&amp;click=1&amp;s=10520733385329581432&amp;nsid=</a><br /><br />• CC攻击、刺探 struts框架重大bug、doker 、accesslog代理日志、<br />• 设计模式、优化、java源码等、高并发、Spark<br />• flume kafka 、设计模式，设计原则<br />• netty(TCP)、RPC<br /><br /><br /><a name="h1-3"></a><h1>2017/12/11 - 09:18</h1><br /><br />• 10G Spark运行时间<br />• 俊杰讲解radio 5 6。。<br /><br /><a name="h1-4"></a><h1>2017/12/12 - 09:16</h1><br /><br />服务器：<br />    docker <br />    最大40G 宽带<br />    理论185左右  实际120左右MB<br /><br />    ——————————————————————————————————————————————————————————<br /> flume:<br />    flume 直接配置日志直接存储到hive中<br />   <br /><a name="h1-5"></a><h1>2017/12/13 - 21:06</h1><br /><br /><br />• spark  shuffle 调优、资源调度 、<br />*<br />    combiner 不适用在哪；jvm的重用；hadoop中的缓存机制 mapreduce；hdfs和hbase 的分区与应用 ；hive实现wordcount；java与scala的区别<br />    kafka是否是顺序读写<br /><br /><br /><a name="h1-6"></a><h1>2017/12/14 - 09:07</h1> <br /><br />俊杰分享：<br />    比特币；<br />    cdn:静态内容分发<br />    BT EMULE : 中心化？<br />    区块链：完全去中心化 ，创世块在客户端，广播（相同创世块就比算数，hash random）<br /><br /><a name="h1-7"></a><h1>2017/12/15 - 11:46</h1><br /><br />项目 自己定义udf 在hive中操作一下<br /><br />• 拿代码 hdfs 下面的三个<br /><br /><br /><a name="h1-8"></a><h1>2017/12/16 - 10:11</h1><br /><br />netty 替换akka  、<br />（Spark中的）bloker 、<br />kabina、<br />Azkaban 开元任务资源调度程序<br /><br />数据结构够树是怎么遍历的，hash方面<br /><br /><br /><a name="h1-9"></a><h1>2017/12/20 - 09:36</h1><br /><br />接替容器<br /><br /><br />平西府、<br />中关村、望京、<br /><br /><br />cnn knn ，数据量,逻辑回归、svm<br /><br /><img src="images\30-3.png" alt="images\30-3.png" /><br /><br />kettle  datax (阿里的)<br />数据量，公司集群，部门人数<br />sparkRDD : transform DataFrame   SparkStreaming:DStream<br /><br />——————<br /><br />500 万级别的 数据对比mysql hbase <br /><a href="https://www.zhihu.com/question/26518864">https://www.zhihu.com/question/26518864</a><br /><br /><br />你自己公司每天的pv uv ，接口性能<br />统计流量<br /><a href="http://www.alexa.cn/">http://www.alexa.cn/</a><br /><br />kafka保证数据不丢失 ，ack校验机制<br /><br /><br /><br />12580.tv<br />查询pv uv 网<br /><a href="http://www.alexa.cn/">http://www.alexa.cn/</a><br /><br /><br /><strong><span style="color:#020e00;">优梦优</span></strong> (上海飨都信息技术有限公司) <br /><a href="http://www.youmyou.com/">http://www.youmyou.com/</a><br /><br />1688 货源<br /><a href="http://www.168w.cc/">http://www.168w.cc/</a><br /><br /><br />曾用名：上海飨都实业有限公司<br />优梦优百度介绍：<br /> <a href="https://baike.baidu.com/item/%E4%BC%98%E6%A2%A6%E4%BC%98/20313607?fr=aladdin">https://baike.baidu.com/item/%E4%BC%98%E6%A2%A6%E4%BC%98/20313607?fr=aladdin</a>   <br /><br /><br /> 欣欣<br /><a href="http://www.gaoqingyy.net/">http://www.gaoqingyy.net/</a><br /><br /><strong>补天志网游指标分析</strong><br /><br /><br /><br /><br /><br /><br /><a name="h1-10"></a><h1>2017/12/21 - 10:53</h1><br />朴素贝叶斯、线性回归、knn<br /><br />Spark<br /> 其他架构技术如Spark Mllib、Spark Graphx<br /><br /><br />windows 安装request<br /><br /><a name="h1-11"></a><h1>2017/12/24 - 10:50</h1> 周日<br /><br />今日名次整理：<br /><br />hive:<br />    Snappy压缩,hive数据倾斜解决方案，hive使用Python脚本进行预处理，Zeus资源调度框架<br />Hbase:性能优化，配置Snappy压缩<br />Echarts：<br />    Echarts图标高级：混搭折现与柱状图，多图标联动，一部数据加载，时间与行为，组件交互行为<br />Flume:<br />    ETL:流程分析，ETLmap类的实现，ETL的driver累得实现，<br />    hourly分析-hbase与hive继承<br />    <img src="images\30-4.png" alt="images\30-4.png" /><br /><br /><br /><a name="h1-12"></a><h1>2017/12/25 - 21:43</h1><br /><br />kafka中的topic在项目中是怎么设置的、<br />Spark有哪些组件、Spark 中的RDD/DataFrame/Dstream/有什么关系  <br />Spark中的RDD指的时 Spark的重要的一块，是弹性分布式数据集，DataFrame是SparkSQl中映射的表，Dstream是SparkStreaming中的连续的RDD<br /><br />sqoop如何实现并发导入功能<br />实现一个a+b的udaf<br /><br /><br /><a name="h1-13"></a><h1>2017/12/26 - 13:09</h1><br /><br />家试天下<br />spark 中的算子，都是用来干什么的 action算子、transfrom算子<br /><br />coalesce :指定分区<br />collect: 以数组的方式返回数据<br />glom:算子<br /><br />flume+kakfa与elk有什么区别；<br /><br /><br />二分查找与三分查找；<br />如何在不知道数组排序的情况下，如何查找一个数；<br /><br />TIDB代替mysql；<br />新技术处理数据；<br /><br />jvm：调优<br /><a href="http://pengjiaheng.iteye.com/blog/518623">http://pengjiaheng.iteye.com/blog/518623</a><br /><br /><br /><a name="h1-14"></a><h1>2017/12/27 - 10:54 周三 </h1><br /><br /><br />召回率：<br /><br /><a href="https://www.cnblogs.com/sddai/p/5696870.html">https://www.cnblogs.com/sddai/p/5696870.html</a><br /><a name="h1-15"></a><h1>2017/12/30 - 15:10 周六</h1><br /><br />今日<br />ha中namenode切换的原理：<br />ha的原理：<br /><a href="https://www.cnblogs.com/sy270321/p/4398815.html">https://www.cnblogs.com/sy270321/p/4398815.html</a><br /><br /><br /><a name="h1-16"></a><h1>有哪些基于ELK的亿级实时日志分析平台实践的案例？</h1><br /><br /><a href="https://www.zhihu.com/question/59957272/answer/170694929">https://www.zhihu.com/question/59957272/answer/170694929</a><br /><br />namenode 脑裂<br /><br /><a href="http://blog.csdn.net/u014774781/article/details/51940301">http://blog.csdn.net/u014774781/article/details/51940301</a><br /><br />namenode zkfc机制<br /><br /><a href="https://www.cnblogs.com/cssdongl/p/6046397.html">https://www.cnblogs.com/cssdongl/p/6046397.html</a><br /><br />reducebykey与groupbykey 有什么区别<br /><br /><br />扒一扒ReentrantLock以及AQS实现原理<br /><a href="http://www.importnew.com/24006.html">http://www.importnew.com/24006.html</a><br /><br /><br />• bitmap /布隆过滤器 / simhash <br />• sparksql读取的数据怎存到hive中，hivecontext<br />• <br /><br /><a name="h1-17"></a><h1>2017/12/31 - 14:47</h1><br /><br />hbase 二级索引 、solr 、二级索引<br /><a href="https://www.baidu.com/baidu?wd=hbase+solr+%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95&tn=monline_4_dg&ie=utf-8">https://www.baidu.com/baidu?wd=hbase+solr+%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95&amp;tn=monline_4_dg&amp;ie=utf-8</a><br /><a href="http://blog.csdn.net/u011462328/article/details/53008434">http://blog.csdn.net/u011462328/article/details/53008434</a><br />storm与spark的区别对比<br /><a href="https://www.baidu.com/s?ie=utf-8&f=3&rsv_bp=1&tn=monline_4_dg&wd=storm%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D&oq=storm&rsv_pq=b91b649f00037b24&rsv_t=76adhRvBVohaBBVhOv4NxFatTvqtg7HDPAvru%2BAoeVSmUM%2B4MOuhNIxfmW4oVxDacsq2&rqlang=cn&rsv_enter=1&inputT=38&rsv_sug3=17&rsv_sug2=0&prefixsug=storm%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D&rsp=0&rsv_sug4=11666">https://www.baidu.com/s?ie=utf-8&amp;f=3&amp;rsv_bp=1&amp;tn=monline_4_dg&amp;wd=storm%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D&amp;oq=storm&amp;rsv_pq=b91b649f00037b24&amp;rsv_t=76adhRvBVohaBBVhOv4NxFatTvqtg7HDPAvru%2BAoeVSmUM%2B4MOuhNIxfmW4oVxDacsq2&amp;rqlang=cn&amp;rsv_enter=1&amp;inputT=38&amp;rsv_sug3=17&amp;rsv_sug2=0&amp;prefixsug=storm%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D&amp;rsp=0&amp;rsv_sug4=11666</a><br /><a href="https://www.cnblogs.com/swanspouse/p/5135679.html">https://www.cnblogs.com/swanspouse/p/5135679.html</a><br /><br /><br /><a name="h1-18"></a><h1>2018/01/01 - 18:26</h1><br />1 怎么把sparksql中的数据导入hive ？<br />• 传到hdfs然后load data (local) inpath '路径' (overwrite) into table ***<br />• 是创建临时表sparksql，schema 然后利用hivecontext.sql（“insert 语句”）<br /><br />2 上传 hdfs 文件。就是put命令 hdfs dfs -put /home/*  /root ,  hdfs dfs -get /root/**<br /><br />3 怎么把hive 数据导入本地磁盘。其实就是hdfs上的数据 hive中外表直接get下来。内表的话其实是在hive/下的warehouse目录下面。但是可以直接查询：<br />    insert overwrite directory 'hdfs_path' select * from dept;  //注意这里的hdfs_path 必须存在<br />    <br />4 广播大的变量    <br />使用可用的广播功能SparkContext可以大大减少们每个序列化任务的大小，自己在集群上启动作业的成本，如果您的任务使用其中的驱动程序中的任何大对象（如，静态查找表，）请考虑将其变为广播变量，Spark点击上每个任务的序列化大小，因此您可以查看该任务是否过大，一般任务大于20KB大概值得优化<br /><br /><a name="h1-19"></a><h1>2018/01/02 - 17:22</h1><br /><br />• flume数据采集有瓶颈<br />SQL comtext 与hive context<br /><br /><h1>2018/01/06 - 22:48</h1><br /><br /><br /><br /><br />    </div></div>
</body></html>