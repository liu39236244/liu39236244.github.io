<!doctype html><html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Flume 1701笔记</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="styles.css" type="text/css" />
</head>
<body><div class="main"><div class="tree">
<p><strong>Index</strong></p>
<p><a href="学习记录.html">学习记录</a></p>

<ol>
<li><a href="学习记录--Needto_Study.html">Needto Study</a></li>
<ol>
<li><a href="学习记录--Needto_Study--NeedStudy_01.html">NeedStudy_01</a></li>
</ol>
<li><a href="学习记录--Sqoop.html">Sqoop</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01.html">Sqoop_01</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop介紹.html">Sqoop介紹</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令.html">Sqoop命令</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop常用命令.html">Sqoop常用命令</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop测试命令.html">Sqoop测试命令</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--Hadoop.html">Hadoop</a></li>
<ol>
<li><a href="学习记录--Hadoop--Hadoop_介绍.html">Hadoop 介绍</a></li>
<li><a href="学习记录--Hadoop--Hadoop环境搭建.html">Hadoop环境搭建</a></li>
</ol>
<li><a href="学习记录--Linux.html">Linux</a></li>
<ol>
<li><a href="学习记录--Linux--linux_零散命令.html">linux 零散命令</a></li>
</ol>
<li><a href="学习记录--Spark.html">Spark</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_介绍.html">Spark 介绍</a></li>
<li><a href="学习记录--Spark--Spark_搭建.html">Spark 搭建</a></li>
<li><a href="学习记录--Spark--Spark_RDD.html">Spark RDD </a></li>
<li><a href="学习记录--Spark--SparkSQl.html">SparkSQl</a></li>
<li><a href="学习记录--Spark--Spark_other.html">Spark other</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_other--Spark_累加器.html">Spark 累加器</a></li>
</ol>
<li><a href="学习记录--Spark--Spark_每日一记.html">Spark 每日一记</a></li>
</ol>
<li><a href="学习记录--Flume.html">Flume</a></li>
<ol>
<li><a href="学习记录--Flume--Flume_介绍、架构.html">Flume 介绍、架构</a></li>
<li><a href="学习记录--Flume--Flume使用.html">Flume使用</a></li>
<ol>
<li><a href="学习记录--Flume--Flume使用--Flume_1701笔记.html">Flume 1701笔记</a></li>
</ol>
</ol>
<li><a href="学习记录--Kafka.html">Kafka</a></li>
<ol>
<li><a href="学习记录--Kafka--Kafka介绍、运行机制.html">Kafka介绍、运行机制</a></li>
</ol>
<li><a href="学习记录--Hive.html">Hive</a></li>
<ol>
<li><a href="学习记录--Hive--Hive_介绍架构.html">Hive 介绍架构</a></li>
<li><a href="学习记录--Hive--Hive_QL_语句.html">Hive QL 语句</a></li>
<li><a href="学习记录--Hive--Hive_UDF.html">Hive UDF</a></li>
<li><a href="学习记录--Hive--Hive东哥笔记.html">Hive东哥笔记</a></li>
<li><a href="学习记录--Hive--hive_装在数据的几种方式.html">hive 装在数据的几种方式</a></li>
<ol>
<li><a href="学习记录--Hive--hive_装在数据的几种方式--01.html">01</a></li>
</ol>
<li><a href="学习记录--Hive--hive的几种join.html">hive的几种join</a></li>
<ol>
<li><a href="学习记录--Hive--hive的几种join--01.html">01</a></li>
</ol>
</ol>
<li><a href="学习记录--Hbase.html">Hbase </a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记.html">东哥笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记--hbase笔记总结.html">hbase笔记总结</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记--Hbase命令笔记_01.html">Hbase命令笔记_01</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--ELK.html">ELK</a></li>
<ol>
<li><a href="学习记录--ELK--ELK总结.html">ELK总结</a></li>
<li><a href="学习记录--ELK--ELK_博客.html">ELK 博客</a></li>
</ol>
<li><a href="学习记录--ETL.html">ETL</a></li>
<ol>
<li><a href="学习记录--ETL--ETL总结.html">ETL总结</a></li>
<li><a href="学习记录--ETL--ETL_博客.html">ETL 博客</a></li>
</ol>
<li><a href="学习记录--服务器云计算.html">服务器云计算</a></li>
<ol>
<li><a href="学习记录--服务器云计算--简单总结.html">简单总结</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置.html">linux 添加ss 的配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--我的.html">我的</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令.html">服务器阶段的命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令.html">下载安装命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令--wget.html">wget</a></li>
</ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置.html">其他配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置--配置命令.html">配置命令</a></li>
</ol>
</ol>
</ol>
</ol>
<li><a href="学习记录--面试方面.html">面试方面</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备.html">代码准备</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01.html">代码准备_01</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01--worldCount准备.html">worldCount准备</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--每天记录.html">每天记录</a></li>
<ol>
<li><a href="学习记录--每天记录--今日整理待整理.html">今日整理待整理</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目).html">cdh(东哥hadoop项目)</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置.html">cdh配置</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装.html">cdh安装 </a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装集群模式.html">cdh安装集群模式</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html">Hadoop项目笔记</a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--项目公共日志.html">项目公共日志</a></li>
</ol>
<li><a href="学习记录--Hadoop、spark等进程.html">Hadoop、spark等进程</a></li>
<ol>
<li><a href="学习记录--Hadoop、spark等进程--Hadoop-spakr-zookeeper-等.html">Hadoop/spakr/zookeeper/等</a></li>
</ol>
<li><a href="学习记录--其他技术.html">其他技术</a></li>
<ol>
<li><a href="学习记录--其他技术--内存与磁盘问题.html">内存与磁盘问题</a></li>
<li><a href="学习记录--其他技术--flink.html">flink</a></li>
<li><a href="学习记录--其他技术--VPN搭建.html">VPN搭建</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas.html">服务器地址pas</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas--free_IP.html">free IP</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--人工智能.html">人工智能</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单.html">人工智能书单</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单--人工智能书单_01.html">人工智能书单_01</a></li>
</ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法.html">人工智能算法</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法--人工智能算法——01.html">人工智能算法——01</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--下载地址.html">下载地址</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--创作社区.html">创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--it创作社区.html">it创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--源码解读.html">源码解读</a></li>
<li><a href="学习记录--其他技术--下载地址--复习语句总结.html">复习语句总结</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--复习语句总结--复习语句总结.html">复习语句总结</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--负载均衡.html">负载均衡</a></li>
<ol>
<li><a href="学习记录--其他技术--负载均衡--负载均衡_01.html">负载均衡_01</a></li>
</ol>
</ol>
<li><a href="学习记录--Phthon.html">Phthon</a></li>
<ol>
<li><a href="学习记录--Phthon--python_使用spark.html">python 使用spark</a></li>
</ol>
<li><a href="学习记录--数据结构.html">数据结构</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1.html">数据结构1</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--Hash_问题.html">Hash 问题</a></li>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树.html">数据结构中的树</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树--数据结构树是怎么遍历的.html">数据结构树是怎么遍历的</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--通信协议.html">通信协议</a></li>
<li><a href="学习记录--开发工具记录.html">开发工具记录</a></li>
<ol>
<li><a href="学习记录--开发工具记录--idea_的一些记录.html">idea 的一些记录</a></li>
<li><a href="学习记录--开发工具记录--博客地址.html">博客地址</a></li>
</ol>
<li><a href="学习记录--数据库.html">数据库</a></li>
<ol>
<li><a href="学习记录--数据库--mysql.html">mysql</a></li>
</ol></ol></div>
<div class="page"><h1><b><u>Flume 1701笔记</u></b></h1><br /><a name="h1-1"></a><h1>1 1701 笔记</h1><br /><br />文本数据：软件、硬件打印信息。<br />流媒体：音视频、图片<br /><br />flume是什么？？<br />flume是一个高效的可靠、可用的、分布式的海量日志数据收集、聚合、传输工具。<br />Flume is a distributed, reliable, and available service for efficiently <br />collecting, aggregating, and moving large amounts of log data<br /><br />flume中组件及作用？？<br />client：客户端(运行agent的地方)<br />source:数据源，负责接收数据<br />channel:管道，负责接收source的数据，然后并将数据推送到sink<br />sink:负责拉去channel中的数据，将其推送到持久化系统。<br />interceptor：拦截器，flume允许使用拦截器拦截数据，它作用于source阶段，flume还允许拦截器链。<br />selector:选择器，作用于source阶段，然后决定数据发送的方式。<br />event：flume的事件，相当于一条数据。<br />agent：flume的客户端，一个agent运行在一个jvm里面。它是flume的最小运行单元。<br /><br /><br />source的种类：<br />avro 、 exec 、 spooling dir 、 syslogtcp 、 httpsource 、 avro sink() 、kafka等。<br />channel的种类：<br />file:<br />memory:<br />jdbc：<br />kafka：<br /><br />sinks的种类：<br />logger 、 avro 、 hdfs、kafka 等。<br /><br /><br />数据模型：<br />单一数据流模型<br />多数据流模型：<br /><br /><br />flume的安装：<br /><br />flume 0.9 和 1.x的版本的区别？<br />1、0.9以前的叫flume-og，而1.x的flume-ng<br />2、0.9区分逻辑和物理的节点，而1.x不在区分逻辑的和物理的node节点，每一个agent就是一个服务。<br />3、0.9需要master和zookeeper的支持，而1.x不在需要其支持。<br />4、0.9开发并不是很灵活，而1.x较为灵活，可以支持很多功能模块的自定义(source、sink、channel、interceptor、selector等)。<br /><br /><br />flume案例？？<br />案例1、 avro + memory + logger<br /><br /><br />vi ./conf/avro<br />#定义agent必须三个组件<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />#配置sources<br />a1.sources.r1.type=avro<br />a1.sources.r1.bind=hadoop02<br />a1.sources.r1.port=6666<br /><br />#配置channels<br />a1.channels.c1.type=memory<br /><br />#配置sinks<br />a1.sinks.s1.type=logger<br /><br />#将source和sink通过channel连接上<br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/avro1.conf -n a1 -Dflume.root.logger=INFO,console<br />测试：<br />flume-ng avro-client -c /usr/local/flume-1.6.0/conf/ -H hadoop02 -p 6666 -F /home/flumedata/avrodata<br /><br /><br />案例2、 exec + memory + logger<br />vi ./conf/exec<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=exec<br />a1.sources.r1.command= tail -f /home/flumedata/exedata<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br /><br />a1.sinks.s1.type = hdfs<br />a1.sinks.s1.hdfs.path = /flume/events/exec<br />a1.sinks.s1.hdfs.filePrefix = events-<br />a1.sinks.s1.hdfs.round = true<br />a1.sinks.s1.hdfs.roundValue = 10<br />a1.sinks.s1.hdfs.roundUnit = minute<br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/exec -n a1 -Dflume.root.logger=INFO,console<br />测试：<br /><br /><br /><br />案例3、 spooldir + memory + logger<br />vi ./conf/spooldir<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=spoolDir<br />a1.sources.r1.spoolDir=/home/flumedata/spool1<br />a1.sources.r1.fileHeader=true<br />a1.sources.r1.fileHeaderKey=file<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br /><br />a1.sinks.s1.type = logger<br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/exec -n a1 -Dflume.root.logger=INFO,console<br />测试：<br /><br /><br /><br /><br />案例4、 syslogtcp + memory + logger<br />vi ./conf/syslogtcp<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=syslogtcp<br />a1.sources.r1.port=6666<br />a1.sources.r1.host=hadoop01<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.sinks.s1.type = logger<br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/syslogtcp -n a1 -Dflume.root.logger=INFO,console<br />测试：<br />echo "hello qianfeng" | nc hadoop01 6666<br /><br /><br /><br />案例5、 http + memory + logger<br />vi ./conf/http<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=org.apache.flume.source.http.HTTPSource<br />a1.sources.r1.port=6666<br />a1.sources.r1.bind=hadoop01<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.sinks.s1.type = logger<br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/http -n a1 -Dflume.root.logger=INFO,console<br />测试：<br />curl -X POST -d '[{"headers":{"time":"2017-06-13"},"body":"this is http"}]' <a href="http://hadoop01:6666">http://hadoop01:6666</a><br /><br /><br />案例6、 exec + memory + hdfs<br />vi ./conf/hdfs<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=exec<br />a1.sources.r1.command= tail -f /home/flumedata/exedata<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.sinks.s1.type = hdfs<br />a1.sinks.s1.hdfs.path = hdfs://qianfeng/flume/events/%y-%m-%d/%H%M/%S<br />a1.sinks.s1.hdfs.filePrefix = qianfeng-<br />a1.sinks.s1.hdfs.fileSuffix=.log<br />a1.sinks.s1.hdfs.inUseSuffix=.tmp<br />a1.sinks.s1.hdfs.rollInterval=2<br />a1.sinks.s1.hdfs.rollSize=1024<br />a1.sinks.s1.hdfs.fileType=DataStream<br />a1.sinks.s1.hdfs.writeFormat=Text<br />a1.sinks.s1.hdfs.round = true<br />a1.sinks.s1.hdfs.roundValue = 1<br />a1.sinks.s1.hdfs.roundUnit = second<br />a1.sinks.s1.hdfs.useLocalTimeStamp=false<br /><br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/hdfs -n a1 -Dflume.root.logger=INFO,console<br />测试：<br /><br /><br /><br />案例7、 exec + file + hdfs<br />vi ./conf/file<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=exec<br />a1.sources.r1.command= tail -f /home/flumedata/exedata<br /><br />a1.channels.c1.type=file<br />a1.channels.c1.checkpointDir=/home/flumedata/checkpoint<br />a1.channels.c1.dataDirs=/home/flumedata/data<br /><br />a1.sinks.s1.type = hdfs<br />a1.sinks.s1.hdfs.path = hdfs://qianfeng/flume/events/%y-%m-%d/%H%M/%S<br />a1.sinks.s1.hdfs.filePrefix = qianfeng-<br />a1.sinks.s1.hdfs.fileSuffix=.log<br />a1.sinks.s1.hdfs.inUseSuffix=.tmp<br />a1.sinks.s1.hdfs.rollInterval=2<br />a1.sinks.s1.hdfs.rollSize=1024<br />a1.sinks.s1.hdfs.fileType=DataStream<br />a1.sinks.s1.hdfs.writeFormat=Text<br />a1.sinks.s1.hdfs.round = true<br />a1.sinks.s1.hdfs.roundValue = 1<br />a1.sinks.s1.hdfs.roundUnit = second<br />a1.sinks.s1.hdfs.useLocalTimeStamp=false<br /><br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/file -n a1 -Dflume.root.logger=INFO,console<br />测试：<br /><br /><br /><br /><br /><br />---------------------#############拦截器--------------------<br />案例1、<br />vi ./conf/ts1<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=exec<br />a1.sources.r1.command= tail -f /home/flumedata/exedata<br />a1.sources.r1.interceptors = i1 i2 i3<br />a1.sources.r1.interceptors.i1.type = timestamp<br />a1.sources.r1.interceptors.i1.preserveExisting=true<br />a1.sources.r1.interceptors.i2.type = host<br />a1.sources.r1.interceptors.i2.hostHeader = hostname<br />a1.sources.r1.interceptors.i2.preserveExisting=true<br />a1.sources.r1.interceptors.i3.type = static<br />a1.sources.r1.interceptors.i3.key = city<br />a1.sources.r1.interceptors.i3.value = NEW_YORK<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.sinks.s1.type = hdfs<br />a1.sinks.s1.hdfs.path = hdfs://qianfeng/flume/events/%y-%m-%d/%H%M/%S<br />a1.sinks.s1.hdfs.filePrefix = %{hostname}-<br />a1.sinks.s1.hdfs.fileSuffix=.log<br />a1.sinks.s1.hdfs.inUseSuffix=.tmp<br />a1.sinks.s1.hdfs.rollInterval=2<br />a1.sinks.s1.hdfs.rollSize=1024<br />a1.sinks.s1.hdfs.fileType=DataStream<br />a1.sinks.s1.hdfs.writeFormat=Text<br />a1.sinks.s1.hdfs.round = true<br />a1.sinks.s1.hdfs.roundValue = 1<br />a1.sinks.s1.hdfs.roundUnit = second<br />a1.sinks.s1.hdfs.useLocalTimeStamp=false<br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/ts -n a1 -Dflume.root.logger=INFO,console<br />测试：<br /><br /><br /><br /><br />案例2、<br />vi ./conf/ts3<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=exec<br />a1.sources.r1.command= tail -f /home/flumedata/exedata<br />a1.sources.r1.interceptors = i1 i2 i3<br />a1.sources.r1.interceptors.i1.type = timestamp<br />a1.sources.r1.interceptors.i1.preserveExisting=true<br />a1.sources.r1.interceptors.i2.type = host<br />a1.sources.r1.interceptors.i2.hostHeader = hostname<br />a1.sources.r1.interceptors.i2.preserveExisting=true<br />a1.sources.r1.interceptors.i3.type = static<br />a1.sources.r1.interceptors.i3.key = city<br />a1.sources.r1.interceptors.i3.value = NEW_YORK<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.sinks.s1.type = logger<br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/ts -n a1 -Dflume.root.logger=INFO,console<br />测试：<br /><br /><br />案例3、正则拦截器<br />vi ./conf/rex<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=exec<br />a1.sources.r1.command= tail -f /home/flumedata/exedata<br />a1.sources.r1.interceptors = i1<br />a1.sources.r1.interceptors.i1.type = regex_filter<br />a1.sources.r1.interceptors.i1.regex=^[0-9].*$<br />a1.sources.r1.interceptors.i1.excludeEvents=false<br /><br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.sinks.s1.type = logger<br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br />启动agent:<br />flume-ng agent -c ./conf/ -f ./conf/rex -n a1 -Dflume.root.logger=INFO,console<br />测试：<br /><br /><br /><br /><br /><br />#####案例3、复制选择器<br />vi ./conf/rep<br />a1.sources=r1<br />a1.channels=c1 c2<br />a1.sinks=s1 s2<br /><br />a1.sources.r1.type=exec<br />a1.sources.r1.command= tail -f /home/flumedata/exedata<br />a1.sources.r1.selector.type = replicating<br />a1.sources.r1.selector.optional = c2<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.channels.c2.type=memory<br />a1.channels.c2.capacity=1000<br />a1.channels.c2.transactionCapacity=100<br />a1.channels.c2.keep-alive=3<br />a1.channels.c2.byteCapacityBufferPercentage = 20<br />a1.channels.c2.byteCapacity = 800000<br /><br />a1.sinks.s1.type = logger<br /><br />a1.sinks.s2.type = hdfs<br />a1.sinks.s2.hdfs.path = hdfs://qianfeng/flume/events/%y-%m-%d/%H%M/%S<br />a1.sinks.s2.hdfs.filePrefix = event-<br />a1.sinks.s2.hdfs.fileSuffix=.log<br />a1.sinks.s2.hdfs.inUseSuffix=.tmp<br />a1.sinks.s2.hdfs.rollInterval=2<br />a1.sinks.s2.hdfs.rollSize=1024<br />a1.sinks.s2.hdfs.fileType=DataStream<br />a1.sinks.s2.hdfs.writeFormat=Text<br />a1.sinks.s2.hdfs.round = true<br />a1.sinks.s2.hdfs.roundValue = 1<br />a1.sinks.s2.hdfs.roundUnit = second<br />a1.sinks.s2.hdfs.useLocalTimeStamp=true<br /><br />a1.sources.r1.channels=c1 c2<br />a1.sinks.s1.channel=c1<br />a1.sinks.s2.channel=c2<br /><br /><br />#####案例4、复分选择器<br />vi ./conf/mul<br />a1.sources=r1<br />a1.channels=c1 c2<br />a1.sinks=s1 s2<br /><br />a1.sources.r1.type=org.apache.flume.source.http.HTTPSource<br />a1.sources.r1.port=6666<br />a1.sources.r1.bind=hadoop01<br />a1.sources.r1.selector.type = multiplexing<br />a1.sources.r1.selector.header = status<br />a1.sources.r1.selector.mapping.CZ = c1<br />a1.sources.r1.selector.mapping.US = c2<br />a1.sources.r1.selector.default = c1<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.channels.c2.type=memory<br />a1.channels.c2.capacity=1000<br />a1.channels.c2.transactionCapacity=100<br />a1.channels.c2.keep-alive=3<br />a1.channels.c2.byteCapacityBufferPercentage = 20<br />a1.channels.c2.byteCapacity = 800000<br /><br />a1.sinks.s1.type = logger<br /><br />a1.sinks.s2.type = hdfs<br />a1.sinks.s2.hdfs.path = hdfs://qianfeng/flume/events/%y-%m-%d/%H%M/%S<br />a1.sinks.s2.hdfs.filePrefix = event-<br />a1.sinks.s2.hdfs.fileSuffix=.log<br />a1.sinks.s2.hdfs.inUseSuffix=.tmp<br />a1.sinks.s2.hdfs.rollInterval=2<br />a1.sinks.s2.hdfs.rollSize=1024<br />a1.sinks.s2.hdfs.fileType=DataStream<br />a1.sinks.s2.hdfs.writeFormat=Text<br />a1.sinks.s2.hdfs.round = true<br />a1.sinks.s2.hdfs.roundValue = 1<br />a1.sinks.s2.hdfs.roundUnit = second<br />a1.sinks.s2.hdfs.useLocalTimeStamp=true<br /><br />a1.sources.r1.channels=c1 c2<br />a1.sinks.s1.channel=c1<br />a1.sinks.s2.channel=c2<br /><br />测试数据：<br />curl -X POST -d '[{"headers":{"status":"2017-06-13"},"body":"this is default"}]' <a href="http://hadoop01:6666">http://hadoop01:6666</a><br />curl -X POST -d '[{"headers":{"status":"CZ"},"body":"this is CZ"}]' <a href="http://hadoop01:6666">http://hadoop01:6666</a><br />curl -X POST -d '[{"headers":{"status":"US"},"body":"this is US"}]' <a href="http://hadoop01:6666">http://hadoop01:6666</a><br /><br /><br />案例5、flume集群搭建：<br />hadoop01的配置：<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=syslogtcp<br />a1.sources.r1.port=6666<br />a1.sources.r1.host=hadoop01<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.sinks.s1.type =avro<br />a1.sinks.s1.hostname=hadoop03<br />a1.sinks.s1.port=6666<br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br /><br />hadoop02的配置：<br />a1.sources=r1<br />a1.channels=c1<br />a1.sinks=s1<br /><br />a1.sources.r1.type=syslogtcp<br />a1.sources.r1.port=6666<br />a1.sources.r1.host=hadoop02<br /><br />a1.channels.c1.type=memory<br />a1.channels.c1.capacity=1000<br />a1.channels.c1.transactionCapacity=100<br />a1.channels.c1.keep-alive=3<br />a1.channels.c1.byteCapacityBufferPercentage = 20<br />a1.channels.c1.byteCapacity = 800000<br /><br />a1.sinks.s1.type =avro<br />a1.sinks.s1.hostname=hadoop03<br />a1.sinks.s1.port=6666<br /><br />a1.sources.r1.channels=c1<br />a1.sinks.s1.channel=c1<br /><br /><br />hadoop03的配置：<br />agent.sources=r1<br />agent.channels=c1<br />agent.sinks=s1<br /><br />agent.sources.r1.type=avro<br />agent.sources.r1.port=6666<br />agent.sources.r1.bind=hadoop03<br /><br />agent.channels.c1.type=memory<br />agent.channels.c1.capacity=1000<br />agent.channels.c1.transactionCapacity=100<br />agent.channels.c1.keep-alive=3<br />agent.channels.c1.byteCapacityBufferPercentage = 20<br />agent.channels.c1.byteCapacity = 800000<br /><br />agent.sinks.s1.type =logger<br /><br />agent.sources.r1.channels=c1<br />agent.sinks.s1.channel=c1<br /> <br />####然后测试：<br />先启动master的agent:<br />flume-ng agent -c ./conf/ -f ./conf/master -n agent -Dflume.root.logger=INFO,console &amp;<br />然后再启动slave的agent：<br />flume-ng agent -c ./conf/ -f ./conf/slave1 -n a1 -Dflume.root.logger=INFO,console &amp;<br />flume-ng agent -c ./conf/ -f ./conf/slave2 -n a1 -Dflume.root.logger=INFO,console &amp;<br /><br />flume的缺点：？？<br />同步部署较难<br /><br />tail -f /vat/access.log <br />cat /vat/---access.log<br /><br /><br />#####？？？？<br />将合并的这台flume的sink写成hdfs sink？<br />将sink写成hive sink？<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></div></div>
</body></html>