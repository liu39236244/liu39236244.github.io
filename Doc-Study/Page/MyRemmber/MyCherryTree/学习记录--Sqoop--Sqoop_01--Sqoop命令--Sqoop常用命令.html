<!doctype html><html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Sqoop常用命令</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="styles.css" type="text/css" />
</head>
<body><div class="main"><div class="tree">
<p><strong>Index</strong></p>
<p><a href="学习记录.html">学习记录</a></p>

<ol>
<li><a href="学习记录--Needto_Study.html">Needto Study</a></li>
<ol>
<li><a href="学习记录--Needto_Study--NeedStudy_01.html">NeedStudy_01</a></li>
</ol>
<li><a href="学习记录--Sqoop.html">Sqoop</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01.html">Sqoop_01</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop介紹.html">Sqoop介紹</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令.html">Sqoop命令</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop常用命令.html">Sqoop常用命令</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop测试命令.html">Sqoop测试命令</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--Hadoop.html">Hadoop</a></li>
<ol>
<li><a href="学习记录--Hadoop--Hadoop_介绍.html">Hadoop 介绍</a></li>
<li><a href="学习记录--Hadoop--Hadoop环境搭建.html">Hadoop环境搭建</a></li>
</ol>
<li><a href="学习记录--Linux.html">Linux</a></li>
<ol>
<li><a href="学习记录--Linux--linux_零散命令.html">linux 零散命令</a></li>
</ol>
<li><a href="学习记录--Spark.html">Spark</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_介绍.html">Spark 介绍</a></li>
<li><a href="学习记录--Spark--Spark_搭建.html">Spark 搭建</a></li>
<li><a href="学习记录--Spark--Spark_RDD.html">Spark RDD </a></li>
<li><a href="学习记录--Spark--SparkSQl.html">SparkSQl</a></li>
<li><a href="学习记录--Spark--Spark_other.html">Spark other</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_other--Spark_累加器.html">Spark 累加器</a></li>
</ol>
<li><a href="学习记录--Spark--Spark_每日一记.html">Spark 每日一记</a></li>
</ol>
<li><a href="学习记录--Flume.html">Flume</a></li>
<ol>
<li><a href="学习记录--Flume--Flume_介绍、架构.html">Flume 介绍、架构</a></li>
<li><a href="学习记录--Flume--Flume使用.html">Flume使用</a></li>
<ol>
<li><a href="学习记录--Flume--Flume使用--Flume_1701笔记.html">Flume 1701笔记</a></li>
</ol>
</ol>
<li><a href="学习记录--Kafka.html">Kafka</a></li>
<ol>
<li><a href="学习记录--Kafka--Kafka介绍、运行机制.html">Kafka介绍、运行机制</a></li>
</ol>
<li><a href="学习记录--Hive.html">Hive</a></li>
<ol>
<li><a href="学习记录--Hive--Hive_介绍架构.html">Hive 介绍架构</a></li>
<li><a href="学习记录--Hive--Hive_QL_语句.html">Hive QL 语句</a></li>
<li><a href="学习记录--Hive--Hive_UDF.html">Hive UDF</a></li>
<li><a href="学习记录--Hive--Hive东哥笔记.html">Hive东哥笔记</a></li>
<li><a href="学习记录--Hive--hive_装在数据的几种方式.html">hive 装在数据的几种方式</a></li>
<ol>
<li><a href="学习记录--Hive--hive_装在数据的几种方式--01.html">01</a></li>
</ol>
<li><a href="学习记录--Hive--hive的几种join.html">hive的几种join</a></li>
<ol>
<li><a href="学习记录--Hive--hive的几种join--01.html">01</a></li>
</ol>
</ol>
<li><a href="学习记录--Hbase.html">Hbase </a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记.html">东哥笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记--hbase笔记总结.html">hbase笔记总结</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记--Hbase命令笔记_01.html">Hbase命令笔记_01</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--ELK.html">ELK</a></li>
<ol>
<li><a href="学习记录--ELK--ELK总结.html">ELK总结</a></li>
<li><a href="学习记录--ELK--ELK_博客.html">ELK 博客</a></li>
</ol>
<li><a href="学习记录--ETL.html">ETL</a></li>
<ol>
<li><a href="学习记录--ETL--ETL总结.html">ETL总结</a></li>
<li><a href="学习记录--ETL--ETL_博客.html">ETL 博客</a></li>
</ol>
<li><a href="学习记录--服务器云计算.html">服务器云计算</a></li>
<ol>
<li><a href="学习记录--服务器云计算--简单总结.html">简单总结</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置.html">linux 添加ss 的配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--我的.html">我的</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令.html">服务器阶段的命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令.html">下载安装命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令--wget.html">wget</a></li>
</ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置.html">其他配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置--配置命令.html">配置命令</a></li>
</ol>
</ol>
</ol>
</ol>
<li><a href="学习记录--面试方面.html">面试方面</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备.html">代码准备</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01.html">代码准备_01</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01--worldCount准备.html">worldCount准备</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--每天记录.html">每天记录</a></li>
<ol>
<li><a href="学习记录--每天记录--今日整理待整理.html">今日整理待整理</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目).html">cdh(东哥hadoop项目)</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置.html">cdh配置</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装.html">cdh安装 </a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装集群模式.html">cdh安装集群模式</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html">Hadoop项目笔记</a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--项目公共日志.html">项目公共日志</a></li>
</ol>
<li><a href="学习记录--Hadoop、spark等进程.html">Hadoop、spark等进程</a></li>
<ol>
<li><a href="学习记录--Hadoop、spark等进程--Hadoop-spakr-zookeeper-等.html">Hadoop/spakr/zookeeper/等</a></li>
</ol>
<li><a href="学习记录--其他技术.html">其他技术</a></li>
<ol>
<li><a href="学习记录--其他技术--内存与磁盘问题.html">内存与磁盘问题</a></li>
<li><a href="学习记录--其他技术--flink.html">flink</a></li>
<li><a href="学习记录--其他技术--VPN搭建.html">VPN搭建</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas.html">服务器地址pas</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas--free_IP.html">free IP</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--人工智能.html">人工智能</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单.html">人工智能书单</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单--人工智能书单_01.html">人工智能书单_01</a></li>
</ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法.html">人工智能算法</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法--人工智能算法——01.html">人工智能算法——01</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--下载地址.html">下载地址</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--创作社区.html">创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--it创作社区.html">it创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--源码解读.html">源码解读</a></li>
<li><a href="学习记录--其他技术--下载地址--复习语句总结.html">复习语句总结</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--复习语句总结--复习语句总结.html">复习语句总结</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--负载均衡.html">负载均衡</a></li>
<ol>
<li><a href="学习记录--其他技术--负载均衡--负载均衡_01.html">负载均衡_01</a></li>
</ol>
</ol>
<li><a href="学习记录--Phthon.html">Phthon</a></li>
<ol>
<li><a href="学习记录--Phthon--python_使用spark.html">python 使用spark</a></li>
</ol>
<li><a href="学习记录--数据结构.html">数据结构</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1.html">数据结构1</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--Hash_问题.html">Hash 问题</a></li>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树.html">数据结构中的树</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树--数据结构树是怎么遍历的.html">数据结构树是怎么遍历的</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--通信协议.html">通信协议</a></li>
<li><a href="学习记录--开发工具记录.html">开发工具记录</a></li>
<ol>
<li><a href="学习记录--开发工具记录--idea_的一些记录.html">idea 的一些记录</a></li>
<li><a href="学习记录--开发工具记录--博客地址.html">博客地址</a></li>
</ol>
<li><a href="学习记录--数据库.html">数据库</a></li>
<ol>
<li><a href="学习记录--数据库--mysql.html">mysql</a></li>
</ol></ol></div>
<div class="page"><h1><b><u>Sqoop常用命令</u></b></h1>sqoop 常用命令<br /><br /><a name="h1-1"></a><strong><h1>一、Sqoop与MySQL</h1></strong><br /><br /><a name="h2-1"></a><strong><h2>1.列出mysql数据库中的所有数据库</h2></strong><br />sqoop list-databases -connect jdbc:mysql://192.168.1.10:3306 -username root -password root<br /><strong>注意：</strong><br />以下URL写法，都可以<br />jdbc:mysql://192.168.1.10:3306/（推荐）<br />jdbc:mysql://192.168.1.10/<br />jdbc:mysql://192.168.1.10<br />jdbc:mysql://masters:3306/<br />jdbc:mysql://master/<br />jdbc:mysql://master<br />jdbc:mysql://localhost:3306/<br />jdbc:mysql://localhost/<br />jdbc:mysql:///<br />jdbc:mysql://<br /> <br /><br /><a name="h2-2"></a><strong><h2>2.列出数据库sqoop中的所有数据表</h2></strong><br />sqoop list-tables -connect jdbc:mysql:///sqoop -username root -password root<br /> <br /><br /><a name="h2-3"></a><strong><h2>3.通过Sqoop执行SQL语句</h2></strong><br />sqoop eval -connect jdbc:mysql:///sqoop -username root -password root -query "select * from employee where id=5"<br />可以快速地使用SQL语句对关系数据库进行操作，这可以使得在使用import这种工具进行数据导入的时候，可以预先了解相关的SQL语句是否正确，并能将结果显示在控制台。<br /> <br /><br /><a name="h1-2"></a><strong><h1>二、Sqoop与HDFS</h1></strong><br /><br /><a name="h2-4"></a><strong><h2>1.将sqoop.employee表中的数据导入HDFS的/sqfs目录下</h2></strong><br />sqoop import -connect jdbc:mysql://192.168.10.71:3306/t2 -username=root -password=root -table employee -m 1 -target-dir /output/1<br /><strong>叠加</strong><br />Ø 追加模式<br />sqoop import -connect jdbc:mysql://192.168.10.71:3306/t2 -username root -password root -table employee -m 1 -target-dir /output/3 -incremental append -check-column id -last-value "5"<br />Ø 最后修改模式<br /><span style="color:#ff0000;">sqoop import -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -table employee -m 1 -target-dir /sqfs -incremental lastmodified -check-column lastmodified  -last-value '2016/1/5 18:00:05'</span><br /> <br /><strong>SQL语句</strong><br />sqoop import -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -m 1 -query 'SELECT id,birthday from employee where $CONDITIONS' -target-dir /res<br />注：不能添加-table参数<br /> <br />sqoop import-all-tables -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -paseeword root -m 1<br />注：不能添加-target-dir参数<br />导出的默认路径是：/user/用户名/若干数据表名/数据文件和_SUCCESS<br />所有表都有主键时，可以设置-m的参数大于1，否则只能唯 1<br /> <br /><br /><a name="h2-5"></a><strong><h2>2.将HDFS上/sqfs目录下的数据导入的sqoop.employee表中</h2></strong><br />sqoop export -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -table employee -m 1 -export-dir /sqfs<br /> <br />采用export插入数据的时候，如果数据已经存在了，插入会失败，如果我们使用-update-key，它会认为每个数据都是更新，比如我们使用下面这条语句：<br />sqoop export -table foo -update-key id -export-dir /path/to/data -connect …<br />UPDATE foo SET msg='this is a test', bar=42 WHERE id=0;<br />UPDATE foo SET msg='some more data', bar=100 WHERE id=1;<br />...<br />这样即使找不到它也不会报错<br />-update-mode allowinsert	如果存在就更新，不存在就插入<br /> <br /><br /><a name="h1-3"></a><strong><h1>三、Sqoop与Hive</h1></strong><br /><br /><a name="h2-6"></a><strong><h2>1.将关系型数据的employee表结构复制到Hive中</h2></strong><br />sqoop create-hive-table -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -table employee -hive-table emp -fields-terminated-by "\0001" -lines-terminated-by "\n"<br />注：<br />-hive-table emp指定在Hive中创建的表名为emp（默认数据库default）<br />-hive-table sqoop.emp指定在Hive中的sqoop数据库下创建emp表<br />-fields-terminated-by "\0001"  是设置每列之间的分隔符，"\0001"是ASCII码中的1，是hive的默认行内分隔符，而sqoop的默认行内分隔符为"，" <br />-lines-terminated-by "\n"  设置的是每行之间的分隔符，此处为换行符，也是默认的分隔符；<br /><br /><a name="h2-7"></a><strong><h2>2.将关系数据库中的employee表的数据导入文件到Hive表中</h2></strong><br />sqoop import -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -table employee -hive-table sqoop.emp -m 1 -fields-terminated-by "\0001" -hive-import<br />注：<br />-fields-terminated-by "\0001"		需同创建Hive表时保持一致<br />-hive-import 					指定是Hive导入数据<br />-split-by id 						employee中没有主键时，用于指定Mapper时的Key<br /> <br /><strong>追加1</strong><br />sqoop import -append -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -target-dir /user/hive/warehouse/sqoop.db/emp/ -fields-terminated-by "\0001" -query "select * from employee where \$CONDITIONS" -m 1<br />注：<br />可以添加-columns，-where参数，同时使用时-where参数会失效<br /> <br /><strong>追加2</strong><br />sqoop import -append -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -table employee -columns "id,name,birthday" -where "id=2" -m 1 -target-dir /user/hive/warehouse/sqoop.db/emp/ -fields-terminated-by "\0001"<br />注：<br />-target-dir /user/hive/warehouse/sqoop.db/emp 可用-hive-table sqoop.emp -hive-import替换,但是要去掉 -append 参数。<br /> <br />在导入大对象，比如BLOB和CLOB列时需要特殊处理，小于16MB的大对象可以和别的数据一起存储，超过这个值就存储在_lobs的子目录当中，它们采用的是为大对象做过优化的存储格式，最大能存储2^63字节的数据，我们可以用-inline-lob-limit参数来指定每个lob文件最大的限制是多少，如果设置为0，则大对象使用外部存储。<br /> <br /><br /><a name="h2-8"></a><strong><h2>3. Hive导入参数</h2></strong><br />-hive-home &lt;dir&gt;			重写$HIVE_HOME<br />-hive-import				插入数据到hive当中，使用hive的默认分隔符<br />-hive-overwrite				重写插入<br />-create-hive-table			建表，如果表已经存在，该操作会报错！<br />-hive-table &lt;table-name&gt;		设置到hive当中的表名<br />-hive-drop-import-delims	导入到hive时删除 \n, \r, and \0001 <br />-hive-delims-replacement	导入到hive时用自定义的字符替换掉 \n, \r, and \0001 <br />-hive-partition-key			hive分区的key<br />-hive-partition-value &lt;v&gt;		hive分区的值<br />-map-column-hive &lt;map&gt;	类型匹配，sql类型对应到hive类型<br /><strong>hive空值处理</strong><br />sqoop会自动把NULL转换为null处理，但是hive中默认是把\N来表示null，因为预先处理不会生效的，我们需要使用 -null-string 和 -null-non-string来处理空值 把\N转为<a href="file://n\">\\N</a><br />例句：sqoop import  ... -null-string '\\N' 或-null-non-string '\\N'<br /><strong>sqoop导入hive数据到MySql碰到hive表中列的值为null的情况：</strong><br />在导入数据的过程中，如果碰到列值为null的情况，hive中为null的是以\N代替的，所以你在导入到MySql时，需要加上两个参数：--input-null-string '\\N' --input-null-non-string '\\N'，多加一个'\'，是为转义。如果你通过这个还不能解决字段为null的情况，还是报什么NumberFormalt异常的话，那就是比较另类的了，没有关系，我们还是要办法解决。<br />你应该注意到每次通过sqoop导入MySql的时，都会生成一个以MySql表命名的.java文件，然后打成JAR包，给sqoop提交给hadoop 的MR来解析Hive表中的数据。那我们可以根据报的错误，找到对应的行，改写该文件，编译，重新打包，sqoop可以通过 -jar-file ，--class-name 组合让我们指定运行自己的jar包中的某个class。来解析该hive表中的每行数据。脚本如下：一个完整的例子如下：<br />sqoop export --connect "jdbc:mysql://localhost/aaa?useUnicode=true=utf-8" <br />--username aaa --password bbb --table table <br />--export-dir /hive/warehouse/table --input-fields-terminated-by '\t' <br />--input-null-string '\\N' --input-null-non-string '\\N' <br />--class-name com.chamago.sqoop.codegen.bi_weekly_sales_item <br />--jar-file /tmp/sqoop-chamago/bi_weekly_sales_item.jar<br />上面--jar-file 参数指定jar包的路径。--class-name 指定jar包中的class。<br />这样就可以解决所有解析异常了。<br /> <br /><br /><a name="h2-9"></a><strong><h2>4.将Hive中的表数据导入到mysql数据库employee表中</h2></strong><br />sqoop export -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -table employee -export-dir /user/hive/warehouse/sqoop.db/emp/part-m-00000 -input-fields-terminated-by '\0001'<br />注：<br />在进行导入之前，mysql中sqoop数据库中employee表必须已经提起创建好了。<br />jdbc:mysql://192.168.1.10:3306/sqoop中的IP地址改成localhost会报异常<br />指定/user/hive/warehouse/sqoop.db/emp/part-m-00000，只加载该文件<br />指定/user/hive/warehouse/sqoop.db/emp/，加载该目录下的所有文件<br /> <br /><br /><a name="h1-4"></a><strong><h1>四、Sqoop与HBase</h1></strong><br /> <br /><br /><a name="h2-10"></a><strong><h2>1. MySQL 中的employee表中的数据导入数据到 HBase的emp表中</h2></strong><br />sqoop import -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -table employee -hbase-table emp -column-family 'per data' -hbase-row-key id -m 1<br />注：<br />-hbase-table emp			指定HBase的表emp<br />-column-family 'per data'		指定列族名per data<br />-hbase-create-table			该参数是用来创建HBase表的，但不太管用<br /> <br /><br /><a name="h2-11"></a><strong><h2>2. 将employee表不同列的数据添加到emp表中不同列族中</h2></strong><br />Ø 先将employee表的name列中的数据到per data列族中<br />sqoop import -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -table employee -hbase-table emp -column-family 'per data' -hbase-row-key id -m 1 -columns id,name<br />注：<br />-columns id,name		指定employee表中的id、name列<br />-hbase-row-key id		指定emp表中的行id<br />Ø 先将employee表的age、birthday列中的数据到pro data列族中<br />sqoop import -connect jdbc:mysql://192.168.1.10:3306/sqoop -username root -password root -table employee -hbase-table emp -column-family 'per data' -hbase-row-key id -m 1 -columns id,age,birthday<br /> <br /><br /><a name="h1-5"></a><strong><h1>五、重要参数</h1></strong><br /><br /><a name="h2-12"></a><strong><h2>1.转换为对象</h2></strong><br />-map-column-java &lt;mapping&gt;  将转换为java数据类型<br />-map-column-hive &lt;mapping&gt;  将转换为hive数据类型<br /><br /><a name="h2-13"></a><strong><h2>2.分隔符、转义字符</h2></strong><br />例句：<br />Some string, with a comma.<br />Another "string with quotes"<br />导入命令：<br />$ sqoop import -fields-terminated-by , -escaped-by \\ -enclosed-by '\"' ...<br />处理结果：<br />"Some string, with a comma.","1","2","3"...<br />"Another \"string with quotes\"","4","5","6"...<br />导入命令：<br />$ sqoop import -optionally-enclosed-by '\"' (the rest as above)...<br />处理结果：<br />"Some string, with a comma.",1,2,3...<br />"Another \"string with quotes\"",4,5,6...<br /> <br /><br /><a name="h1-6"></a><strong><h1>六、常用工具</h1></strong><br /><br /><a name="h2-14"></a><strong><h2>1. sqoop job</h2></strong><br />保存常用的作业，以便下次快速调用<br />-create &lt;job-id&gt;			创建一个新的job<br />　　-delete &lt;job-id&gt;			删除job<br />　　-exec &lt;job-id&gt; 			执行job<br />　　-show &lt;job-id&gt;			显示job的参数<br />　　-list						列出所有的job<br />　<br />Ø 创建job<br />sqoop job -create myjob - import -connect jdbc:mysql://example.com/db ... -table mytable<br />Ø 列出所有job<br />sqoop job -list<br />Ø 查看job<br />sqoop job -show myjob<br />Ø 执行job<br />sqoop job -exec myjob<br />Ø 重写参数<br />sqoop job -exec myjob -username someuser -P<br /> <br /><br /><a name="h2-15"></a><strong><h2>2. 聚合工具</h2></strong><br />sqoop-metastore、sqoop-merge<br />合并两个目录<br />sqoop merge -new-data newer -onto older -target-dir merged -jar-file datatypes.jar -class-name Foo -merge-key id<br /> <br /><br /><a name="h2-16"></a><strong><h2>3. 校验Validate</h2></strong><br />它用来比较源数据和目标数据的数量。<br />sqoop import --connect jdbc:mysql://db.foo.com/corp ... -table EMPLOYEES --validate<br />sqoop export --connect jdbc:mysql://db.example.com/foo --table bar -export-dir /results/bar_data --validate<br />注：<br />它有三个接口<br /><strong>Validator</strong><br />Property:         validator<br />Description:      Driver for validation,<br />                  must implement org.apache.sqoop.validation.Validator<br />Supported values: The value has to be a fully qualified class name.<br />Default value:    org.apache.sqoop.validation.RowCountValidator<br /><strong> </strong><br /><strong>Validation Threshold</strong><br />Property:			validation-threshold<br />Description:			Drives the decision based on the validation meeting the<br />					threshold or not. Must implement<br />					org.apache.sqoop.validation.ValidationThreshold<br />Supported values:	The value has to be a fully qualified class name.<br />Default value:		org.apache.sqoop.validation.AbsoluteValidationThreshold<br /> <br /><strong>Validation Failure Handler</strong><br />Property:			validation-failurehandler<br />Description:			Responsible for handling failures, must implement<br />					org.apache.sqoop.validation.ValidationFailureHandler<br />Supported values:	The value has to be a fully qualified class name.<br />Default value:		org.apache.sqoop.validation.LogOnFailureHandler<br /> <br /> <br />例句：<br />sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES <br />添加参数：<br />-validate -validator org.apache.sqoop.validation.RowCountValidator<br />或-validate -validation-threshold<br />或-validate -validation-failurehandler<br /> <br /> <br /><br /><a name="h1-7"></a><strong><h1>七、配置文件</h1></strong><br />通过配置文件conf/sqoop-site.xml来配置常用参数<br />例：<br />&lt;property&gt;<br />&lt;name&gt;property.name&lt;/name&gt;<br />&lt;value&gt;property.value&lt;/value&gt;<br />&lt;/property&gt;<br />如果不在这里面配置的话，就需要像这样写命令<br />sqoop import -D property.name=property.value ...<br />　<br />参数：sqoop.bigdecimal.format.string<br />作用：大decimal是否保存为string，如果保存为string就是 0.0000007,否则为1E7<br />　　<br />参数：sqoop.hbase.add.row.key<br />作用：是否把作为rowkey的列也加到行数据当中，默认是false的<br /> <br /> <br /><br /><br /><br /><a name="h1-8"></a><strong><h1>附件：</h1></strong><br /><br /><a name="h2-17"></a><strong><h2>1. 乱码问题：</h2></strong><br />MySQL与HDFS相互导入，导出的乱码问题。<br />修改MySQL的编码格式，由Latin1改为UTF-8<br /> vi /etc/my.cnf<br />添加如下内容：红色的这个 是适合老版mysql 入：mysql5.1.71<br />[mysqld]<br /><span style="color:#ff0000;">default-character-set=utf8</span><br />character_set_server=utf8<br />init_connect='SET NAMES utf8' <br /> <br />[mysql]<br />default-character-set=utf8<br /> <br />[client]<br />default-character-set=utf8<br /> <br />重启MySQL服务，重建库、表；<br /> <br /><br /><a name="h2-18"></a><strong><h2>2. MySQL建表语句</h2></strong><br />创建员工表1<br />create table employee(<br />id int primary key auto_increment,<br />name varchar(20),<br />birthday date<br />);<br />测试数据：<br />insert into employee values('','张三','2000-01-01');<br />insert into employee values('','李四','2001-01-01');<br />insert into employee values('','王五','2002-01-01');<br />insert into employee values('','张小三','2003-01-01');<br />insert into employee values('','李小四','2004-01-01');<br />insert into employee values('','王小五','2005-01-01');<br />insert into employee values('','张大三','2006-01-01');<br />insert into employee values('','李大四','2007-01-01');<br />insert into employee values('','王大五','2008-01-01');<br />insert into employee values('','王二麻子','2009-01-01');<br />insert into employee values('','wangermazi','2010-01-01');<br /> <br />创建员工表2<br />create table em(<br />id int,<br />name varchar(20),<br />birthday date<br />);<br />测试数据：<br />insert into employee values('0','张三','2000-01-01');<br />insert into employee values('1','李四','2001-01-01');<br />insert into employee values('2','王五','2002-01-01');<br />insert into employee values('3','张小三','2003-01-01');<br />insert into employee values('4','李小四','2004-01-01');<br />insert into employee values('5','王小五','2005-01-01');<br />insert into employee values('6','张大三','2006-01-01');<br />insert into employee values('7','李大四','2007-01-01');<br />insert into employee values('8','王大五','2008-01-01');<br />insert into employee values('9','王二麻子','2009-01-01');<br />insert into employee values('10','wangermazi','2009-01-01');<br /> <br /><br /><a name="h2-19"></a><strong><h2>3. HBase建表语句</h2></strong><br />MySQL建表语句<br />create table employee(<br />id int,<br />name varchar(20),<br />age int,<br />birthday date<br />);<br />测试数据<br />insert into employee values('0','张三','18','2003-01-01');<br />insert into employee values('1','李四','18','2003-01-01');<br />insert into employee values('2','王五','18','2003-01-01');<br />insert into employee values('3','wangermazi','18','2003-01-01');<br /> <br />创表语句<br />create 'emp','per data','pro data'<br />删表语句<br />disable 'emp'<br />drop 'emp'<br />查询语句<br />scan ‘emp’<br /> <br /></div></div>
</body></html>