<!doctype html><html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Hadoop项目笔记</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="styles.css" type="text/css" />
</head>
<body><div class="main"><div class="tree">
<p><strong>Index</strong></p>
<p><a href="学习记录.html">学习记录</a></p>

<ol>
<li><a href="学习记录--Needto_Study.html">Needto Study</a></li>
<ol>
<li><a href="学习记录--Needto_Study--NeedStudy_01.html">NeedStudy_01</a></li>
</ol>
<li><a href="学习记录--Sqoop.html">Sqoop</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01.html">Sqoop_01</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop介紹.html">Sqoop介紹</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令.html">Sqoop命令</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop常用命令.html">Sqoop常用命令</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop测试命令.html">Sqoop测试命令</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--Hadoop.html">Hadoop</a></li>
<ol>
<li><a href="学习记录--Hadoop--Hadoop_介绍.html">Hadoop 介绍</a></li>
<li><a href="学习记录--Hadoop--Hadoop环境搭建.html">Hadoop环境搭建</a></li>
</ol>
<li><a href="学习记录--Linux.html">Linux</a></li>
<ol>
<li><a href="学习记录--Linux--linux_零散命令.html">linux 零散命令</a></li>
</ol>
<li><a href="学习记录--Spark.html">Spark</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_介绍.html">Spark 介绍</a></li>
<li><a href="学习记录--Spark--Spark_搭建.html">Spark 搭建</a></li>
<li><a href="学习记录--Spark--Spark_RDD.html">Spark RDD </a></li>
<li><a href="学习记录--Spark--SparkSQl.html">SparkSQl</a></li>
<li><a href="学习记录--Spark--Spark_other.html">Spark other</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_other--Spark_累加器.html">Spark 累加器</a></li>
</ol>
<li><a href="学习记录--Spark--Spark_每日一记.html">Spark 每日一记</a></li>
</ol>
<li><a href="学习记录--Flume.html">Flume</a></li>
<ol>
<li><a href="学习记录--Flume--Flume_介绍、架构.html">Flume 介绍、架构</a></li>
<li><a href="学习记录--Flume--Flume使用.html">Flume使用</a></li>
<ol>
<li><a href="学习记录--Flume--Flume使用--Flume_1701笔记.html">Flume 1701笔记</a></li>
</ol>
</ol>
<li><a href="学习记录--Kafka.html">Kafka</a></li>
<ol>
<li><a href="学习记录--Kafka--Kafka介绍、运行机制.html">Kafka介绍、运行机制</a></li>
</ol>
<li><a href="学习记录--Hive.html">Hive</a></li>
<ol>
<li><a href="学习记录--Hive--Hive_介绍架构.html">Hive 介绍架构</a></li>
<li><a href="学习记录--Hive--Hive_QL_语句.html">Hive QL 语句</a></li>
<li><a href="学习记录--Hive--Hive_UDF.html">Hive UDF</a></li>
<li><a href="学习记录--Hive--Hive东哥笔记.html">Hive东哥笔记</a></li>
<li><a href="学习记录--Hive--hive_装在数据的几种方式.html">hive 装在数据的几种方式</a></li>
<ol>
<li><a href="学习记录--Hive--hive_装在数据的几种方式--01.html">01</a></li>
</ol>
<li><a href="学习记录--Hive--hive的几种join.html">hive的几种join</a></li>
<ol>
<li><a href="学习记录--Hive--hive的几种join--01.html">01</a></li>
</ol>
</ol>
<li><a href="学习记录--Hbase.html">Hbase </a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记.html">东哥笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记--hbase笔记总结.html">hbase笔记总结</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记--Hbase命令笔记_01.html">Hbase命令笔记_01</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--ELK.html">ELK</a></li>
<ol>
<li><a href="学习记录--ELK--ELK总结.html">ELK总结</a></li>
<li><a href="学习记录--ELK--ELK_博客.html">ELK 博客</a></li>
</ol>
<li><a href="学习记录--ETL.html">ETL</a></li>
<ol>
<li><a href="学习记录--ETL--ETL总结.html">ETL总结</a></li>
<li><a href="学习记录--ETL--ETL_博客.html">ETL 博客</a></li>
</ol>
<li><a href="学习记录--服务器云计算.html">服务器云计算</a></li>
<ol>
<li><a href="学习记录--服务器云计算--简单总结.html">简单总结</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置.html">linux 添加ss 的配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--我的.html">我的</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令.html">服务器阶段的命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令.html">下载安装命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令--wget.html">wget</a></li>
</ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置.html">其他配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置--配置命令.html">配置命令</a></li>
</ol>
</ol>
</ol>
</ol>
<li><a href="学习记录--面试方面.html">面试方面</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备.html">代码准备</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01.html">代码准备_01</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01--worldCount准备.html">worldCount准备</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--每天记录.html">每天记录</a></li>
<ol>
<li><a href="学习记录--每天记录--今日整理待整理.html">今日整理待整理</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目).html">cdh(东哥hadoop项目)</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置.html">cdh配置</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装.html">cdh安装 </a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装集群模式.html">cdh安装集群模式</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html">Hadoop项目笔记</a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--项目公共日志.html">项目公共日志</a></li>
</ol>
<li><a href="学习记录--Hadoop、spark等进程.html">Hadoop、spark等进程</a></li>
<ol>
<li><a href="学习记录--Hadoop、spark等进程--Hadoop-spakr-zookeeper-等.html">Hadoop/spakr/zookeeper/等</a></li>
</ol>
<li><a href="学习记录--其他技术.html">其他技术</a></li>
<ol>
<li><a href="学习记录--其他技术--内存与磁盘问题.html">内存与磁盘问题</a></li>
<li><a href="学习记录--其他技术--flink.html">flink</a></li>
<li><a href="学习记录--其他技术--VPN搭建.html">VPN搭建</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas.html">服务器地址pas</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas--free_IP.html">free IP</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--人工智能.html">人工智能</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单.html">人工智能书单</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单--人工智能书单_01.html">人工智能书单_01</a></li>
</ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法.html">人工智能算法</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法--人工智能算法——01.html">人工智能算法——01</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--下载地址.html">下载地址</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--创作社区.html">创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--it创作社区.html">it创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--源码解读.html">源码解读</a></li>
<li><a href="学习记录--其他技术--下载地址--复习语句总结.html">复习语句总结</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--复习语句总结--复习语句总结.html">复习语句总结</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--负载均衡.html">负载均衡</a></li>
<ol>
<li><a href="学习记录--其他技术--负载均衡--负载均衡_01.html">负载均衡_01</a></li>
</ol>
</ol>
<li><a href="学习记录--Phthon.html">Phthon</a></li>
<ol>
<li><a href="学习记录--Phthon--python_使用spark.html">python 使用spark</a></li>
</ol>
<li><a href="学习记录--数据结构.html">数据结构</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1.html">数据结构1</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--Hash_问题.html">Hash 问题</a></li>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树.html">数据结构中的树</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树--数据结构树是怎么遍历的.html">数据结构树是怎么遍历的</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--通信协议.html">通信协议</a></li>
<li><a href="学习记录--开发工具记录.html">开发工具记录</a></li>
<ol>
<li><a href="学习记录--开发工具记录--idea_的一些记录.html">idea 的一些记录</a></li>
<li><a href="学习记录--开发工具记录--博客地址.html">博客地址</a></li>
</ol>
<li><a href="学习记录--数据库.html">数据库</a></li>
<ol>
<li><a href="学习记录--数据库--mysql.html">mysql</a></li>
</ol></ol></div>
<div class="page"><h1><b><u>Hadoop项目笔记</u></b></h1><br />    ▸ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html">Hadoop项目笔记</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-1">Day02 项目编码埋点搜集日志，模拟处理订单，会员退款</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-2">2 第二个transform maven项目</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-3">3 解析useragent</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-4">Day03 2017/12/12 - 09:29</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-1">1 些时间的工具类</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-2">2 然后开始提交数据</a><br />          ▪ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h3-1">2.1 开始写mr 程序</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-5">运行测试 出错</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-3">3 导入hdfs  表映射</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-6">Day04 hive  </a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-4">1. AM</a><br />          ▪ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h3-2">1.1 创建hive表</a><br />          ▪ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h3-3">1.2 写创建hive 表语句</a><br />          ▪ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h3-4">1.3 导入数据</a><br />          ▪ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h3-5">1.4 对数据进行分数线四</a><br />          ▪ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h3-6">1.6 需求分析sql</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-5">PM</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-7">Day05  2017/12/14 - 09:23</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-6">1 总结：</a><br />          ▪ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h3-7">1.1 项目环境搭建：</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-7">1.2 数据收集</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-8">1.3 etl:</a><br />          ▪ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h3-8">1.4  新增用户 维度</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-8">Day06  Day</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-9">1  修改错误</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-10">2 查询语句</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-9"> 3 统计浏览器模块新增用户</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-11">     3.1 浏览器维度新增用户指标</a><br />        ◇ <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h2-12">        3.2 处理总共数据量：</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-10">    4 新增活跃用户指标</a><br />      • <a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html#h1-11">N  Day</a><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><a name="h1-1"></a><h1>Day02 项目编码埋点搜集日志，模拟处理订单，会员退款</h1><br /><br />• vi ./conf/agent<br />• 删除  /home/hadoop/access.log<br />• sudo vi ./conf/agent<br />•<br />1、启动hdfs<br /> 	sudo chown -R hadoop:hadoop /home/hadoop/installed/hadoop-2.5.0-cdh5.3.6/<br /><br /><br /><br />2、后台启动flume<br />执行命令：flume-ng agent --conf  /home/hadoop/installed/apache-flume-1.5.0-cdh5.3.6-bin/conf/ -f /home/hadoop/installed/apache-flume-1.5.0-cdh5.3.6-bin/conf/agent.conf -n agent  -Dflume.root.logger=INFO,console &amp;<br />汇创建一个文件，/home/hadoop/access.log 就是之前删除的文件<br />3、启动配置nginx：<br />4、在java中运行test测试类、<br /><br />application进程就是flume进程<br /><br /><br />• <img src="images\89-1.png" alt="images\89-1.png" /> 就算开起来<br /><br />Demo2 页面上<br />改变订单id，价格<br />点击requestsend,提交订单，这边先开启接受订单，然后再执行退款<br /><br />测试 支付成功，季刊如测试页面2 、触发chargeRequest事件<br /><br /><img src="images\89-2.png" alt="images\89-2.png" /><br />u_mid 就是会员id<br /><br />——————<br />tail -f /home/hadoop/<br /><br /><a name="h1-2"></a><h1>2 第二个transform maven项目</h1>解析ip<br /><img src="images\89-3.png" alt="images\89-3.png" /><br /><br />空间换时间<br /><br /><img src="images\89-4.png" alt="images\89-4.png" /><br /><br />在我们本次项目中ip解析采用的是纯真ip数据库，官网是<a href="http://www.cz88.net/">http://www.cz88.net/</a><br />另外：ip解析可以采用淘宝提供的ip接口来进行解析<br />	地址：<a href="http://ip.taobao.com/">http://ip.taobao.com/</a><br />	接口：<a href="http://ip.taobao.com/service/getIpInfo.php?ip=[ip地址字串]">http://ip.taobao.com/service/getIpInfo.php?ip=[ip地址字串]</a><br />	<br /><br /><br />43.246.229.178  江苏<br /><br /><br />在etl过程中，我们需要将我们收集得到的数据进行处理，包括：userAgent解析、ip地址解析、服务器时间解析<br />创建maven项目：bc_transform，创建类包，添加依赖等。<br /><br /><br /><br />包括ip地址解析:<br /><br /><br />userAgent解析:<br /><br /><br />服务器时间解析<br /><br /><br /><img src="images\89-5.png" alt="images\89-5.png" /><br />导入prom.xml 配置文件<br /><br /><br /><br />	————————————————————————————————————————————————————————————<br /><a name="h1-3"></a><h1>3 解析useragent</h1><br /><br /><br /><br /><br />——————————————————————————————————————————————————————————————<br /><br /><a name="h1-4"></a><h1>Day03 2017/12/12 - 09:29</h1><br /><br /><a name="h2-1"></a><h2>1 些时间的工具类</h2><br /><br /><a name="h2-2"></a><h2>2 然后开始提交数据</h2><br /><br /><img src="images\89-6.png" alt="images\89-6.png" /><br /><br />row-key  的设计：满足业务条件下越短越好，region能够提前创建，这里采用时间戳：<br />    <br />   原则： 加时间戳、加时间戳对region个数进行模余、时间戳前面加上随机数<br /><br /><span style="color:#808080;">192.168.123.1^A1512964431.432^A192.168.123.123^A/index.html?ver=11&amp;u_mid=%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE123456\xE6\xB5\x8B\xE8\xAF\x95\xE6\x95\xB0\xE6\x8D\xAE123456&amp;en=e_cre_cr&amp;oid=56419615641961&amp;sdk=javajava&amp;pl=java_serverjava_server</span><br /><br />create 'event_logs','info'<br /><br /><br /><a name="h3-1"></a><h3>2.1 开始写mr 程序</h3><br /><br /><br /><img src="images\89-7.png" alt="images\89-7.png" /><br />导包所需：<br />——————————————<br /><span style="color:#cc7832;">import </span>org.apache.hadoop.hbase.client.Put<span style="color:#cc7832;">;<br />import </span>org.apache.hadoop.io.LongWritable<span style="color:#cc7832;">;<br />import </span>org.apache.hadoop.io.NullWritable<span style="color:#cc7832;">;<br />import </span>org.apache.hadoop.io.Text<span style="color:#cc7832;">;<br />import </span>org.apache.hadoop.mapreduce.Mapper<span style="color:#cc7832;">;<br /></span><em><span style="color:#629755;">/**<br /> * 将hdfs中的数据清洗之后存储到hbase，以便后续分析<br /> * 为啥选择hbase:<br /> * hbase 可以存储非结构化的数据，而我们的参数url请求参数列表恰好字段不固定<br /> * hbase也可以方便和hive整合来分析数据，<br /> * 缺点：hbase可能不太适合海量的批量的实时的读写<br /> */</span></em><br />public class ParserLogDataMapper <span style="color:#cc7832;">extends </span>Mapper &lt;LongWritable<span style="color:#cc7832;">,</span>Text<span style="color:#cc7832;">,</span>NullWritable<span style="color:#cc7832;">,</span>Put&gt;{<br />}<br />——————————————<br />mr程序 到<br /><br />core-site.xml<br />hbase-site.xml <br /><br /><img src="images\89-8.png" alt="images\89-8.png" /><br /><br /><img src="images\89-9.png" alt="images\89-9.png" /><br /><br /><br />2.2 大jar包运行出错解决<br /><br /><br />运行程序之前需要起起来hbase<br /><br />start-hbase.sh <br />hbase shell<br /><br />执行 ParserLogDataRunner<br /><br /><br /><br /><a name="h1-5"></a><h1>运行测试 出错</h1><br /><br />设置为true，true为本地，所以设置为false <br /><img src="images\89-10.png" alt="images\89-10.png" /><br /><br />打成jar包，<br /><br /><img src="images\89-11.png" alt="images\89-11.png" /><br />如果包上面的错误那么就把hbase下面lib包都加到classpath环境变量当中<br /><img src="images\89-12.png" alt="images\89-12.png" /><br /><br /><img src="images\89-13.png" alt="images\89-13.png" /><br /><br /><br /><img src="images\89-14.png" alt="images\89-14.png" /><br /><br />找不到jar包 ，配置新的shadle<br /><br /><br /><img src="images\89-15.png" alt="images\89-15.png" /><br /><br /><br /><br /><a name="h2-3"></a><h2>3 导入hdfs  表映射</h2><br />注意在本地的时候，要设置为true  ：所为本地也就是设置输入输出目录在本地<br />Path inpath = <span style="color:#cc7832;">new </span>Path(<span style="color:#6a8759;">"E://hadoopdata//11"</span>)<span style="color:#cc7832;">;</span><br /><br />如果是在本地环境获取集群上的数据，并且把数据写入到集群上的化要设置为false，在集群上跑的jar的话，也得设置为false<br /><br /><br /><br /><a name="h1-6"></a><h1>Day04 hive  </h1><br /><br /><a name="h2-4"></a><h2>1. AM</h2><br /><a name="h3-2"></a><h3>1.1 创建hive表</h3><br /><br /><br />create table <br /><br /><img src="images\89-16.png" alt="images\89-16.png" /><br /><img src="images\89-17.png" alt="images\89-17.png" /><br />hdfs dfs -get /gpTransform/12/12/part-m--00000<br /><br /><img src="images\89-18.png" alt="images\89-18.png" /><br /><img src="images\89-19.png" alt="images\89-19.png" /><br /><br /><img src="images\89-20.png" alt="images\89-20.png" /><br /><br /><a name="h3-3"></a><h3>1.2 写创建hive 表语句</h3><br /><br />// 要建立分区表<br />create external table if not exists logs (<br />s_time string ,<br />en string ,<br />ver string ,<br />u_ud string ,<br />u_mid string ,<br />u_sid string ,<br />c_time string ,<br />language string ,<br />b_iev string ,<br />b_rst string ,<br />p_url string ,<br />p_ref string ,<br />tt string ,<br />pl string ,<br />o_id string ,<br />`on` string ,<br />cut string ,<br />cua string ,<br />pt string ,<br />ca string ,<br />ac string ,<br />kv_ string ,<br />du string ,<br />os string ,<br />os_v string ,<br />browser string ,<br />browser_v string ,<br />country string ,<br />province string ,<br />city string ,<br />)<br />partitioned by (month String ,day string)<br />row format delimited fields terminated by '\001';<br /><br /><br />——————————————————————<br /><a name="h3-4"></a><h3>1.3 导入数据</h3> 最终数据写入mysql中<br />加载数据<br />加载数据<br /><br />load data   (local)  inpath '/gpTransform/month=12/day=12/part-m-00000' overwrite into table logs partition(month=12,day=12)；<br />注意导入hive的过程 是mv的过程，因为是从hbase 中导入的<br /><br /><img src="images\89-21.png" alt="images\89-21.png" /><br /><br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><a name="h3-5"></a><h3>1.4 对数据进行分数线四</h3><br /><br /><img src="images\89-22.png" alt="images\89-22.png" /><br /><img src="images\89-23.png" alt="images\89-23.png" /><br /> 分析数据：<br /> <br />统计谷歌浏览器在北京访问的 数量为多少<br /><br />浏览器  北京 <br /><br /><img src="images\89-24.png" alt="images\89-24.png" /><br /><br />所以这里需要创建10个维度<br /><br /><img src="images\89-25.png" alt="images\89-25.png" /><br /><br /><br /><img src="images\89-26.png" alt="images\89-26.png" /><br /><br /><a name="h3-6"></a><h3>1.6 需求分析sql</h3><br />这个自增长需要自己设计row——number 设计<br /><br /><br /><br /><br /><br /><br />1.6.1 创建mysql 数据库<br /><br />执行sql文件<br />执行sql文件创建数据库，<br />1.先创建report数据库，（navicat中创建一个数据库。utf-8 ）<br />2. root进入mysql ，source 一下sql文件<br /><br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><br />1.N<br />维度存放位置<br /><img src="images\89-27.png" alt="images\89-27.png" /><br /><span style="color:#cc7832;">public </span><span style="color:#ffc66d;">DateDimension</span>(<span style="color:#cc7832;">int </span>year<span style="color:#cc7832;">, int </span>season<span style="color:#cc7832;">, int </span>month<span style="color:#cc7832;">, int </span>week<span style="color:#cc7832;">, int </span>day<span style="color:#cc7832;">, </span>String type<span style="color:#cc7832;">, </span>Date <br /><br /><br /><br /><a name="h2-5"></a><h2>PM</h2><br /><br /><img src="images\89-28.png" alt="images\89-28.png" /><br /><img src="images\89-29.png" alt="images\89-29.png" /><br /><img src="images\89-30.png" alt="images\89-30.png" /><br /><a name="h1-7"></a><h1>Day05  2017/12/14 - 09:23</h1>(存入hive表之后)<br /><br /><a name="h2-6"></a><h2>1 总结：</h2><br /><a name="h3-7"></a><h3>1.1 项目环境搭建：</h3><br />hadoop2.5.0-cdh5.3.6/<br />flume-cdh<br />hive-cdh<br />hbase-cdh<br />sqoop-cdh<br />mysql<br /><br /><a name="h2-7"></a><h2>1.2 数据收集</h2><br />    js买点数据、java后台数据<br /><a name="h2-8"></a><h2>1.3 etl:</h2><br />清洗两部分数据：<br />    * ip转换成国家、省、市<br />    * useragent转换成浏览器名称、浏览器版本、操作系统名称、操作系统版本<br />被过滤的数据：<br />    * servertime等于空的将会被过滤<br />分模块来过滤数据：<br /><br /><br /><br />项目架构的搭建：<br />    分析维度表、结果表和辅助分析表的设计：<br />基础维度实体bean的编写：（为了持久化到数据库当中）<br />输入输出维度bean的封装和类型的编写：<br />封装操作维度表的接口和实现：<br /><br /><br />缺点：<br /><br /><br />对比：<br /><br /><br /><a name="h3-8"></a><h3>1.4  新增用户 维度</h3><br /><img src="images\89-31.png" alt="images\89-31.png" /><br /><br /><img src="images\89-32.png" alt="images\89-32.png" /><br /><br /><img src="images\89-33.png" alt="images\89-33.png" /><br /><br /><br /><br />执行设置时间<br /><br /><br /><a name="h1-8"></a><h1>Day06  Day</h1><br /><br /><a name="h2-9"></a><h2>1  修改错误</h2><br /><img src="images\89-34.png" alt="images\89-34.png" /><br /><br /><img src="images\89-35.png" alt="images\89-35.png" /><br /><br /><img src="images\89-36.png" alt="images\89-36.png" /><br /><br /><img src="images\89-37.png" alt="images\89-37.png" /><br /><br /><img src="images\89-38.png" alt="images\89-38.png" /><br /><br />以上就是错误修改的地方：<br /><br /><a name="h2-10"></a><h2>2 查询语句</h2><br /><br /><img src="images\89-39.png" alt="images\89-39.png" /><br /><br />select <br />concat ('truncate ',t.TABLE_NAME,';')<br />from `tables` t <br />where t.TABLE_SCHEMA = 'report'<br /><br />deleteall 'event_logs','找一条rowkey'<br /><br />Hbase 数据集<br /><br /><br /><img src="images\89-40.png" alt="images\89-40.png" /><img src="images\89-41.png" alt="images\89-41.png" /><br /><br /><img src="images\89-42.png" alt="images\89-42.png" /><br /><br />truncate 'event_logs'<br /><br />scan 'event_logs',{COLUMNS=&gt;['info:u_ud','info:pl']}<br /><br />会把java server 的数据过滤掉，因为没有uuid<br /><br /><br />上传今天的数据<br /><br /><span style="background-color:#ff002b;">复制了两条数据改了什么地方？？</span><br /><br />数据两条<br /><br />hdfs dfs -mkdir /logs/12/15<br /><br /><br />hdfs dfs -put /aa.log  /logs/12/15<br /><br />然后执行写入到hbase 中<br />    执行本地文件：参数  -d 2017-12-15 <br />    <br />    <br /><a name="h1-9"></a><h1> 3 统计浏览器模块新增用户</h1><br /> 一个job跑两个指标的数据<br /> <br /><a name="h2-11"></a><h2>     3.1 浏览器维度新增用户指标</h2><br /> 跑浏览器维度：<br />stats_user表： <img src="images\89-43.png" alt="images\89-43.png" /><br /> <br />那一天、什么平台（web site 、java server ）、浏览器版本（3-谷歌的4.20版本，4代表所有浏览器版本）、当日活跃用户数、新增用户几个用户、总共用户数量<br />浏览器的维度<br /><a name="h2-12"></a><h2>        3.2 处理总共数据量：</h2><br />   dimension_date<br />        不需要mapreduce ，只需要两列值相加就可以，<br />        <img src="images\89-44.png" alt="images\89-44.png" /><br />        <br />根据日期获取时间维度id，也就是每天时间的id<br /><img src="images\89-45.png" alt="images\89-45.png" /><br />获取到时间id,抽取了一个方法，返回为-1则没有查找到时间id<br />~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><a name="h1-10"></a><h1>    4 新增活跃用户指标</h1><br />    新增的表中存储新增的<br />    以前万条数据，按照一个数字一个字节来算，占用300多MB,<br />    <br />    <br /><a name="h1-11"></a><h1>2017/12/18 - 09:19  周一 </h1><br /><br />uuid 指的是用户访问页面随机生成的一个uuid，<br />memberid：注册成功为会员<br /><br /><a name="h2-13"></a><h2>1.1 开始记录</h2><br /><br /><img src="images\89-46.png" alt="images\89-46.png" /><br />    <br />    获取一堆删除数据表的语句<br />    <br />    * 生成数据 用于测试<br />         1 vi /home/hadoop/16.log<br />            里面复制数据，吧时间戳改一下就行 <br />         2 然后上传到hdfs中<br />            hdfs dfs -mkdir -p /logs/12/16 <br />         3 把数据映射到hbase表中，ParserLogDataRunner  把数据简单清洗写入到hbase<br />          <br />目前项目中唯一的错误：如果当天没有数据你想查数据的话应该报错，或者提示，这里项目中出现了-1<br />           <img src="images\89-47.png" alt="images\89-47.png" /><br />           新增用户用launch、活跃用户只要存在uuid <br />    <br /><a name="h2-14"></a><h2>1.2  新增会员</h2><br /><br /><a name="h3-9"></a><h3>1 计算新增会员个数</h3>：新增会员的mapper，第一次访问会员id的去重个数<br />        <br />• 在util包里面创建MemberUtil 类用于判断memberid,是否是第一次访问<br />member是一个整数的数字，正则判断<br /><br />• 设一一张表 member_info 表存member的id，<br />CREATE TABLE IF NOT EXISTS meber_info (<br />   `member_id` varchar(10) not null ,<br />   `last_viewed_date` date  default null,<br />   `create` date default null ,<br />   primary key (`member_id`)<br />   <br />) engine=InnoDB default charset=utf8;<br />    <br />    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br /><a name="h1-12"></a><h1>2017/12/19 - 10:11 计算访问步长</h1><br />• 计算用户访问时长<br /><br /><img src="images\89-48.png" alt="images\89-48.png" /><br />SessionUtil 工具类用于计算session的时长<br />    这里定义了一个数组2:<img src="images\89-49.png" alt="images\89-49.png" /><br />    <br />计算用户访问时长：<br /><br /><br />• 小时指标<br /><br /><img src="images\89-50.png" alt="images\89-50.png" /><br /><img src="images\89-51.png" alt="images\89-51.png" /><br /><br /><img src="images\89-52.png" alt="images\89-52.png" /><br /><img src="images\89-53.png" alt="images\89-53.png" /><br /><br />为一个小时计算活跃用户<br /><br />au/ActiveUserReducer.java<br /><img src="images\89-54.png" alt="images\89-54.png" /><br /><br />key是小时数 value 是指标<br />这里使用setup 初始化时间，只执行一次<br /><br /><a name="h1-13"></a><h1>2017/12/20 - 09:44  浏览器 pv</h1><br />走到这里走了nu、nm、au、am、session、-》 这次是pv<br />nu指的时新增用户，新增用户里面又分为各种维度的新增用户，<br />    平台维度：比如所属平台，all平台，java_server平台还是什么平台；<br />    浏览器维度: 是ie还是google ，然后all维度，的新增用户数<br />    <br />    <img src="images\89-55.png" alt="images\89-55.png" /><br />nm 也是一样的，新增会员数<br /><br />au  指的时活跃用户数<br />am  指的时活跃会员数<br /><br />session  分析：<br /><br />pv:<br /><a name="h2-15"></a><h2>1 </h2>浏览器的pv也就是page view ，需求分析<br /><br /><img src="images\89-56.png" alt="images\89-56.png" /><br /><br /><img src="images\89-57.png" alt="images\89-57.png" /><br />跑出的数据：<br /><br /><img src="images\89-58.png" alt="images\89-58.png" /><br />14 号 1 平台，ie  所有版本的pv事件<br /><img src="images\89-59.png" alt="images\89-59.png" /><br />这条数据代表的是 ，<br />1 14号、2 15号 3 16号 <br />16号，所有平台，4 代表ie浏览器所有版本，有两个人访问了<br />3代表的是ie 某个版本，pv都是2，代表两条都是这个浏览器来的pv事件<br /><br /><br /><img src="images\89-60.png" alt="images\89-60.png" /><br /><br />到目前为止 前两个指标已经完成，接下来完成下面的两个也是用MR，最后三个指标用hive<br /><br />• 接下来来看地域信息分析规则<br /><img src="images\89-61.png" alt="images\89-61.png" /><br /><br /><img src="images\89-62.png" alt="images\89-62.png" /><br /><br />• 到目前还没有地域维度的维度类<br />创建LocationDimension extends BaseDimension  <br /><img src="images\89-63.png" alt="images\89-63.png" /><br /><br />LocationDimension extends BaseDimension  <br />    里面定义了三个字段：id,country,province,city <br />    并创建构造方法、write read （因为是继承BaseDimension类）<br />• getLocation（根据参数返回一个LocationDimension对象）<br />• 如果需要统计每一个省份所有市的话就需要：创建public List&lt;LocationDimension&gt; buildLocation(country,province,city)<br />{<br />    方法里面,判断是否国家、省份、城市为空，为空的就赋值nuknow <br />    如下图：<br />    <img src="images\89-64.png" alt="images\89-64.png" /><br />    <br />}<br /><br />• 之后重写了compare等。。。<br />• dimension_location <br /><img src="images\89-65.png" alt="images\89-65.png" /><br /><br /><a name="h2-16"></a><h2>2  地域维度公共类</h2><br />• <a name="h3-10"></a><h3>2.0 定义公共类<br /></h3>transform.model.dim.StatsCommonDimension<br />StatsCommonDimension extends StatsDimension <br />然后封装到 StatsLocationDimension  中，构建构造方法<br /><img src="images\89-66.png" alt="images\89-66.png" /><br />完善 colon（） write （） read()方法，重写hashCode(),comparreto（）方法<br /><a name="h3-11"></a><h3>• 2.1 定义地域维度创建输出类型</h3><br /><img src="images\89-67.png" alt="images\89-67.png" /><br />LocationReduceOutputValue extends BaseStatsValueWritable{<br />    KpiType kpi ;<br />    int au;<br />    int visits;<br />    bounceNum;<br />    重写了write(Dataoutput out ){<br />        out.writeInt(this.au); visits 、bounceNum<br />        ...<br />        WritableUtils.writenum(out,this.kpi);<br />    }<br />    <br />    重写了readfields(DataInput in){<br />        this.au=in.readInt();<br />        ...<br />        Writableutils.readEnum(in,KpiType.clcass);<br />    }<br />    <br />}<br /><br />不过貌似不行，需要再创建一个TextOutputValue extends BaseStatsValueWritable{<br />    String uuid;<br />    String usid;<br />    long time;<br />    写了write（）<br />    写了readFields()<br />    <br />}<br /><br /><a name="h3-12"></a><h3>• 2.2 定义location的mr</h3><br /><img src="images\89-68.png" alt="images\89-68.png" /><br /><br /><br /><br /><a name="h3-13"></a><h3>• 2.3 定义location的mr</h3><br />LocationMapper extends <br /><br />LocationReducer extends  Reducer &lt;...&gt;<br /><img src="images\89-69.png" alt="images\89-69.png" /><br /> session  等于1  那么bounch +1 ，代表的是跳出个数<br /><br /><a name="h1-14"></a><h1>2017/12/21 - 09:44 事件统计</h1><br /><a name="h2-17"></a><h2>1 需求</h2><br /><img src="images\89-70.png" alt="images\89-70.png" /><br />计算规则<br /><img src="images\89-71.png" alt="images\89-71.png" /><br />写hive<br /><img src="images\89-72.png" alt="images\89-72.png" /><br /><br /><img src="images\89-73.png" alt="images\89-73.png" /><br /><br /><a name="h2-18"></a><h2>2创建表：event_logs</h2><br />需要字段：<br /><br />s_time<br />p_l<br />ca (category)<br />ac （活跃用户）  <br />e_e (统计的事件)<br /> ca ac 分组  、s_time、p_l 、ca 、ac 维度;<br /> <img src="images\89-74.png" alt="images\89-74.png" /><br /> <br /> 创建表<br />create table if not exists event_logs(<br />s_time String,<br />pl String ,<br />ca String ,<br />ac String ,<br />e_e String <br />) <br /> row format serde 'org.apache.hadoop.hive.hbase.HbaseSerde'<br /> stored by 'org.apache.hadoop.hive.hbase.HbaseStorageHandler'<br /> whith serdeproperties('hbase.columns.mapping'=':key,info:s_time,info:pl)<br />tblproperties('hbase.table.name'='event_logs')<br /><br /><img src="images\89-75.png" alt="images\89-75.png" /><br /><br /><br /><img src="images\89-76.png" alt="images\89-76.png" /><br />如果hbase里面已经有数据了，只能创建外部表才能映射上去，否则不行因为你删除hive表的话可能吧我hbase数据表里面的数据也给删了<br />•  create external table if ...<br /><br />• hive --service metastore &amp; <br />• hive <br />创建一个数据库<br />• create database gp;<br />• use gp;<br />• 然后执行创建表的语句(注意external外表设定)<br /><img src="images\89-77.png" alt="images\89-77.png" /><br /><br /><a name="h2-19"></a><h2>• 3 然后汇过来写UDF</h2><br />•<img src="images\89-78.png" alt="images\89-78.png" /><br /><br /><img src="images\89-79.png" alt="images\89-79.png" /><br />写udf，这里咱们的pd(PlatformDimension) java_server 2/website 3<br /><br />写好之后打包上传<br /><a name="h2-20"></a><h2>4 制作临时函数</h2><br /><img src="images\89-80.png" alt="images\89-80.png" /><br />这里咱们吧jar包上传到hdfs上面，所以先创建一个文件夹在hdfs上面<br />create function date_convert as   'edu.qianfeng.transform.hive.EventDimensionUDF'  using jar 'hdfs://hadoop01:9000/qf/'<br /><br />create function platform_convert as   'edu.qianfeng.transform.hive.EventDimensionUDF'  using jar 'hdfs://hadoop01:9000/qf/';<br /><br />create function event_convert as   'edu.qianfeng.transform.hive.EventDimensionUDF'  using jar 'hdfs://hadoop01:9000/qf/';<br /><br /><img src="images\89-81.png" alt="images\89-81.png" /><br />select from_unixtime(cast ((151322390/1000) as int ),'yyyy-MM-dd');<br /><br /><img src="images\89-82.png" alt="images\89-82.png" /><br />select date_convert(from_unixtime(cast ((151322390/1000) as int ),'yyyy-MM-dd')),platform_convert(),event_convert(el.ca,el.ac) from event_logs el where s_time= 传过来一个时间戳<br /><br /><img src="images\89-83.png" alt="images\89-83.png" /><br /><br />• 创建mysql的结果表对应的hive表：对应的是mysql的stats_event表<br />    hive中没有所谓的主键，这就是一个临时表我们要不断覆盖的所以不需要创建一个<br /><img src="images\89-84.png" alt="images\89-84.png" /><br /><br />select unix_timestamp('2017-12-15','yyyy-MM-dd')  -- 获得的时间是秒数，如果要计算14好的数据，那么需要小于15号凌晨的时间<br />测试命令 select  unix_timestamp('2017-12-14','yyyy-MM-dd')*1000,unix_timestamp('2017-12-15','yyyy-MM-dd')*1000<br />编写指标的hql：<br />ca 事件类型、ac事件操作<br />select <br />el.pl,<br />from_unixtime(cast((el.s_time/1000) as int ) as `date`,<br />el.ca,<br />el.ac<br />from event_logs el <br />where el.en = 'e_e' <br />and el.pl is not null <br />and el.s_time &gt;=unix_timestamp('2017-12-14','yyyy-MM-dd')*1000<br />and el.s_time &lt; unix_timestamp('2017-12-15','yyyy-MM-dd')*1000<br /><br /><img src="images\89-85.png" alt="images\89-85.png" /><br />过滤出当天14号的。特定e_e 事件<br />只有一条数据，如果把上面语句中 el.en ='e_e' and 去掉<br /><br />查询出如下结果<br /><img src="images\89-86.png" alt="images\89-86.png" /><br /><br />• 把查询的结果存入 hive中的一张表（临时的）<br />with tmp as (<br />select <br />el.pl,<br />from_unixtime(cast((el.s_time/1000) as int ) as `date`,<br />el.ca,<br />el.ac<br />from event_logs el <br />where el.en = 'e_e' <br />and el.pl is not null <br />and el.s_time &gt;=unix_timestamp('2017-12-14','yyyy-MM-dd')*1000<br />and el.s_time &lt; unix_timestamp('2017-12-15','yyyy-MM-dd')*1000<br />)<br /><span style="background-color:#92e32c;">from (<br />//去重的话<br />//select pl as pl,`date`,ca as ca ,ac as ac ,count (distinct(列名字)) as times from tmp  group by pl ,`date`,ca,ac <br />select pl as pl,`date`,ca as ca ,ac as ac ,count (1) as times from tmp  group by pl ,`date`,ca,ac  union all <br />select 'all' as pl,`date`,ca as ca ,ac as ac ,count (1) as times from tmp  group by pl ,`date`,ca,ac  union all <br />select pl as pl,`date`,'all' as ca ,ac as ac ,count (1) as times from tmp  group by pl ,`date`,ca,ac  union all<br />select 'all' as pl,`date`,'all' as ca ,ac as ac ,count (1) as times from tmp  group by pl ,`date`,ca,ac  union all<br />select 'all' as pl,`date`,'all' as ca ,'all' as ac ,count (1) as times from tmp  group by pl ,`date`,ca,ac  union all    <br />select 'all' as pl,`date`,'all' as ca ,'all' as ac ,count (1) as times from tmp  group by pl ,`date`,ca,ac  union all    <br />) as tmp2 <br />insert overwrite table stats_event <br />select </span><br /><br /><a name="h2-21"></a><h2>• 5 编写sqoop语句</h2><br /><br />• 默认的分隔符'\\01'  就是一个小方块，<br />sqoop export --connect jdbc:mysql://hadoop01:3306/report \<br />--username root --password root --table stats_event \<br />--export-dir /hive/gp.db/stats_event/* \<br />--input-fields-terminated-by "\\01"--update-mode alowinsert \<br />--update-key `platform_dimension_id`,`date_dimension_id`,`event_dimension_id` \<br />--columns <br />;<br /><img src="images\89-87.png" alt="images\89-87.png" /><br /><br /><br /><br />————————————————————————————————————————————<br /><br /><br /><br /><br /><br /><a name="h1-15"></a><h1>N  Day</h1></div></div>
</body></html>