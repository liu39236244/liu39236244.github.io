<!doctype html><html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Hive东哥笔记</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="styles.css" type="text/css" />
</head>
<body><div class="main"><div class="tree">
<p><strong>Index</strong></p>
<p><a href="学习记录.html">学习记录</a></p>

<ol>
<li><a href="学习记录--Needto_Study.html">Needto Study</a></li>
<ol>
<li><a href="学习记录--Needto_Study--NeedStudy_01.html">NeedStudy_01</a></li>
</ol>
<li><a href="学习记录--Sqoop.html">Sqoop</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01.html">Sqoop_01</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop介紹.html">Sqoop介紹</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令.html">Sqoop命令</a></li>
<ol>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop常用命令.html">Sqoop常用命令</a></li>
<li><a href="学习记录--Sqoop--Sqoop_01--Sqoop命令--Sqoop测试命令.html">Sqoop测试命令</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--Hadoop.html">Hadoop</a></li>
<ol>
<li><a href="学习记录--Hadoop--Hadoop_介绍.html">Hadoop 介绍</a></li>
<li><a href="学习记录--Hadoop--Hadoop环境搭建.html">Hadoop环境搭建</a></li>
</ol>
<li><a href="学习记录--Linux.html">Linux</a></li>
<ol>
<li><a href="学习记录--Linux--linux_零散命令.html">linux 零散命令</a></li>
</ol>
<li><a href="学习记录--Spark.html">Spark</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_介绍.html">Spark 介绍</a></li>
<li><a href="学习记录--Spark--Spark_搭建.html">Spark 搭建</a></li>
<li><a href="学习记录--Spark--Spark_RDD.html">Spark RDD </a></li>
<li><a href="学习记录--Spark--SparkSQl.html">SparkSQl</a></li>
<li><a href="学习记录--Spark--Spark_other.html">Spark other</a></li>
<ol>
<li><a href="学习记录--Spark--Spark_other--Spark_累加器.html">Spark 累加器</a></li>
</ol>
<li><a href="学习记录--Spark--Spark_每日一记.html">Spark 每日一记</a></li>
</ol>
<li><a href="学习记录--Flume.html">Flume</a></li>
<ol>
<li><a href="学习记录--Flume--Flume_介绍、架构.html">Flume 介绍、架构</a></li>
<li><a href="学习记录--Flume--Flume使用.html">Flume使用</a></li>
<ol>
<li><a href="学习记录--Flume--Flume使用--Flume_1701笔记.html">Flume 1701笔记</a></li>
</ol>
</ol>
<li><a href="学习记录--Kafka.html">Kafka</a></li>
<ol>
<li><a href="学习记录--Kafka--Kafka介绍、运行机制.html">Kafka介绍、运行机制</a></li>
</ol>
<li><a href="学习记录--Hive.html">Hive</a></li>
<ol>
<li><a href="学习记录--Hive--Hive_介绍架构.html">Hive 介绍架构</a></li>
<li><a href="学习记录--Hive--Hive_QL_语句.html">Hive QL 语句</a></li>
<li><a href="学习记录--Hive--Hive_UDF.html">Hive UDF</a></li>
<li><a href="学习记录--Hive--Hive东哥笔记.html">Hive东哥笔记</a></li>
<li><a href="学习记录--Hive--hive_装在数据的几种方式.html">hive 装在数据的几种方式</a></li>
<ol>
<li><a href="学习记录--Hive--hive_装在数据的几种方式--01.html">01</a></li>
</ol>
<li><a href="学习记录--Hive--hive的几种join.html">hive的几种join</a></li>
<ol>
<li><a href="学习记录--Hive--hive的几种join--01.html">01</a></li>
</ol>
</ol>
<li><a href="学习记录--Hbase.html">Hbase </a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记.html">东哥笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--东哥笔记--hbase笔记总结.html">hbase笔记总结</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记.html">Hbase 命令笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_命令笔记--Hbase命令笔记_01.html">Hbase命令笔记_01</a></li>
</ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
<ol>
<li><a href="学习记录--Hbase--Hbase_命令笔记--Hbase_API笔记--Hbase_API笔记.html">Hbase API笔记</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--ELK.html">ELK</a></li>
<ol>
<li><a href="学习记录--ELK--ELK总结.html">ELK总结</a></li>
<li><a href="学习记录--ELK--ELK_博客.html">ELK 博客</a></li>
</ol>
<li><a href="学习记录--ETL.html">ETL</a></li>
<ol>
<li><a href="学习记录--ETL--ETL总结.html">ETL总结</a></li>
<li><a href="学习记录--ETL--ETL_博客.html">ETL 博客</a></li>
</ol>
<li><a href="学习记录--服务器云计算.html">服务器云计算</a></li>
<ol>
<li><a href="学习记录--服务器云计算--简单总结.html">简单总结</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置.html">linux 添加ss 的配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--我的.html">我的</a></li>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令.html">服务器阶段的命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令.html">下载安装命令</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--下载安装命令--wget.html">wget</a></li>
</ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置.html">其他配置</a></li>
<ol>
<li><a href="学习记录--服务器云计算--linux_添加ss_的配置--服务器阶段的命令--其他配置--配置命令.html">配置命令</a></li>
</ol>
</ol>
</ol>
</ol>
<li><a href="学习记录--面试方面.html">面试方面</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备.html">代码准备</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01.html">代码准备_01</a></li>
<ol>
<li><a href="学习记录--面试方面--代码准备--代码准备_01--worldCount准备.html">worldCount准备</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--每天记录.html">每天记录</a></li>
<ol>
<li><a href="学习记录--每天记录--今日整理待整理.html">今日整理待整理</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目).html">cdh(东哥hadoop项目)</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置.html">cdh配置</a></li>
<ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装.html">cdh安装 </a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--cdh配置--cdh安装集群模式.html">cdh安装集群模式</a></li>
</ol>
<li><a href="学习记录--cdh(东哥hadoop项目)--Hadoop项目笔记.html">Hadoop项目笔记</a></li>
<li><a href="学习记录--cdh(东哥hadoop项目)--项目公共日志.html">项目公共日志</a></li>
</ol>
<li><a href="学习记录--Hadoop、spark等进程.html">Hadoop、spark等进程</a></li>
<ol>
<li><a href="学习记录--Hadoop、spark等进程--Hadoop-spakr-zookeeper-等.html">Hadoop/spakr/zookeeper/等</a></li>
</ol>
<li><a href="学习记录--其他技术.html">其他技术</a></li>
<ol>
<li><a href="学习记录--其他技术--内存与磁盘问题.html">内存与磁盘问题</a></li>
<li><a href="学习记录--其他技术--flink.html">flink</a></li>
<li><a href="学习记录--其他技术--VPN搭建.html">VPN搭建</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas.html">服务器地址pas</a></li>
<ol>
<li><a href="学习记录--其他技术--VPN搭建--服务器地址pas--free_IP.html">free IP</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--人工智能.html">人工智能</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单.html">人工智能书单</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能书单--人工智能书单_01.html">人工智能书单_01</a></li>
</ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法.html">人工智能算法</a></li>
<ol>
<li><a href="学习记录--其他技术--人工智能--人工智能算法--人工智能算法——01.html">人工智能算法——01</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--下载地址.html">下载地址</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--创作社区.html">创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--it创作社区.html">it创作社区</a></li>
<li><a href="学习记录--其他技术--下载地址--源码解读.html">源码解读</a></li>
<li><a href="学习记录--其他技术--下载地址--复习语句总结.html">复习语句总结</a></li>
<ol>
<li><a href="学习记录--其他技术--下载地址--复习语句总结--复习语句总结.html">复习语句总结</a></li>
</ol>
</ol>
<li><a href="学习记录--其他技术--负载均衡.html">负载均衡</a></li>
<ol>
<li><a href="学习记录--其他技术--负载均衡--负载均衡_01.html">负载均衡_01</a></li>
</ol>
</ol>
<li><a href="学习记录--Phthon.html">Phthon</a></li>
<ol>
<li><a href="学习记录--Phthon--python_使用spark.html">python 使用spark</a></li>
</ol>
<li><a href="学习记录--数据结构.html">数据结构</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1.html">数据结构1</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--Hash_问题.html">Hash 问题</a></li>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树.html">数据结构中的树</a></li>
<ol>
<li><a href="学习记录--数据结构--数据结构1--数据结构中的树--数据结构树是怎么遍历的.html">数据结构树是怎么遍历的</a></li>
</ol>
</ol>
</ol>
<li><a href="学习记录--通信协议.html">通信协议</a></li>
<li><a href="学习记录--开发工具记录.html">开发工具记录</a></li>
<ol>
<li><a href="学习记录--开发工具记录--idea_的一些记录.html">idea 的一些记录</a></li>
<li><a href="学习记录--开发工具记录--博客地址.html">博客地址</a></li>
</ol>
<li><a href="学习记录--数据库.html">数据库</a></li>
<ol>
<li><a href="学习记录--数据库--mysql.html">mysql</a></li>
</ol></ol></div>
<div class="page"><h1><b><u>Hive东哥笔记</u></b></h1>1、hive怎么出现？？<br />facebook对海量数据进行分析和机器学习。<br />提供类sql工具对数据操作。学习方便使用方便。<br /><br />2、hive是什么？？<br />hive是一个基于hadoop的数据仓库。该仓库提供类sql语句进行读、写、管理海量数据。<br />hive可以使用类sql语句进行分析。<br /><br />hive本身没有存储功能，也没有分析功能。它只是一个套在hadoop只上的壳子。<br />依赖hdfs和hbase来做数据存储。<br />依赖mr、spark、tez，flink,麒麟，等来做数据分析。<br /><br />到底什么是Hive，我们先看看Hive官网Wiki是如何介绍Hive的(<a href="https://cwiki.apache.org/confluence/display/Hive/Home)：">https://cwiki.apache.org/confluence/display/Hive/Home)：</a><br />The Apache Hive data warehouse software facilitates querying and managing large datasets residing in distributed storage. Built on top of Apache HadoopTM, it provides:<br />　　(1)、Tools to enable easy data extract/transform/load (ETL)<br />　　(2)、A mechanism to impose structure on a variety of data formats<br />　　(3)、Access to files stored either directly in Apache HDFSTM or in other data storage systems such as Apache HBaseTM<br />　　(4)、Query execution via MapReduce<br />1、是一种易于对数据实现提取、转换、加载的工具(ETL)的工具。可以理解为数据清洗分析展现。<br />2、它有一种将大量格式化数据强加上结构的机制。<br />3、它可以分析处理直接存储在hdfs中的数据或者是别的数据存储系统中的数据，如hbase。<br />4、查询的执行经由mapreduce完成。<br />5、hive可以使用存储过程<br /><br />hive和hadoop的关系？？<br />依赖关系<br /><br />hive的架构图？？<br />cli：command line interface <br />jdbc/odbc: java连接驱动<br />web GUI：web操作<br />sql： structure query language<br />hql： hive query language<br />select <br />* <br />from u <br />where <br />chinese &lt; 60 <br />or math &lt; 60 <br />or english &lt; 60<br />;<br /><br />Driver：解释器，将hql语句生成一个表达式树。<br />Compiler：编译器，将hql语句的表达式树进行语法、语义、词法等的检测。(在它之后生成逻辑策略图)<br />Optmizer：优化器，选择最优的执行路径，在执行语句上做优化(减少不必要的列、join合并、减少不必要的分桶、减少不必要的分区)<br />Executer：执行器，将执行计划给mapreduce依次执行。<br /><br />metaStore：hive的元数据(库名、表名、字段、创建人、创建时间、分区、分桶、索引等信息)<br />hive元数据存储：<br />derby：默认存储在hive自带derby数据库里面。<br />mysql：hive可以将元数据存储关系型数据库中。如：mysql、orcal。<br /><br />derby：<br />优点：不用配置，简单快速<br />缺点：只支持单session；存储量稍微小。<br /><br />mysql：<br />优点：支持多session；存储量大。<br />缺点：需要配置。<br /><br /><br />hive 和 mysql 区别？？<br />1、hive使用mr执行引擎，mysql使用自己带的引擎。<br />2、hive是高延迟的，而mysql是低延迟<br />3、hive使用hdfs存储，而mysql使用磁盘。<br />4、hive数据类型偏向java，而mysql没有复杂数据类型。<br />5、hive不可以对局部数据进行增删改，而mysql是可以的。<br />6、hive的数据模型和mysql有区别。<br />7、hive可以大规模扩展，而mysql扩展性有约束。<br />8、hive的分区字段是表外，而mysql的分区字段是表内字段。<br /><br /><br /><br /><br />3、hive的安装？？<br />1、使用derby来做元数据存储安装：<br />解压、配置环境变量、启动(安装hadoop就需要先启动hadoop)<br />2、使用mysql做元数据存储：<br />解压配置环境变量<br />安装并配置mysql，启动mysql<br />配置hive配置文件：<br />将mysql的驱动包拷贝hive安装目录下的lib目录下<br />在mysql中手动创建hive的元数据库(hive)，并将其编码设置为latin-1。(hive对utf-8支持不是很友好)<br />在启动hive之前先把hadoop启动起来。<br />启动hive。<br /><br /><br />创建库：<br />create database [if not exists] qf1603 'comment this is mydatabase';<br /><br />创建表：默认创建到当前数据库(default是hive默认库)<br />create table if not exists u(<br />uname string,<br />chinese int,<br />math int,<br />english int<br />)<br />;<br /><br />create table if not exists qf1603.u(<br />uname string,<br />chinese int,<br />math int,<br />english int<br />)<br />;<br /><br />创建库的本质：创建一个目录<br />创建表的本质：在对应的库下面创建一个目录<br />加载数据的本质：将数据文件copy到对应的表目录下面(如果是hdfs上的目录，将是剪切)。<br />hive：使用的读时模式，写操作不检测数据，读数据有问题时，使用NULL代替。<br />mysql：使用写时模式，写操作检测数据。<br /><br /><br />切换库：<br />use qf1603;<br /><br />create table if not exists u1(<br />uname string,<br />chinese int,<br />math int,<br />english int<br />)<br />;<br /><br />create table if not exists u2(<br />uname string comment 'this is uname',<br />chinese int comment 'this is chinese',<br />math int,<br />english int<br />)<br />comment 'this table of u2'<br />row format delimited fields terminated by '\t'<br />lines terminated by '\n'<br />stored as textfile<br />;<br /><br />为hive表加载数据：<br />load data [local] inpath '/home/socre' [overwrite] into table u2;<br />load data inpath '/home/socre' into table u2;<br />load local data inpath '/home/u' into table u;<br /><br /><br />CREATE TABLE log2(<br />id             string COMMENT 'this is id column',<br />phonenumber     bigint,<br />mac             string,<br />ip               string,<br />url              string,<br />status1             string,<br />status2          string,<br />up           int,<br />down           int,<br />code            int,<br />dt         String<br />)<br />COMMENT 'this is log table'<br />ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '<br />LINES TERMINATED BY '\n'<br />stored as textfile<br />;<br /><br />#inser into 加载数据：<br />insert into table log1<br />select * from log where phonenumber = 15649428888<br />;<br /><br />加载数据还可以直接将数据copy到对应表目录下面。<br /><br />数据模型：<br /><br /><br />查看表：<br />show tables;<br /><br />删除表：<br />drop table if exists log2;<br />删除表本质：删除表目录<br /><br />查看表：<br />desc [extended] log1;<br />describe [extended] log1;<br />show create table log1;<br /><br />CREATE TABLE `log3`(<br />  `id` string COMMENT 'this is id column', <br />  `phonenumber` bigint, <br />  `mac` string, <br />  `ip` string, <br />  `url` string, <br />  `status1` string, <br />  `status2` string, <br />  `up` int, <br />  `down` int, <br />  `code` int, <br />  `dt` string)<br />COMMENT 'this is log table'<br />ROW FORMAT DELIMITED <br />  FIELDS TERMINATED BY ' ' <br />  LINES TERMINATED BY '\n' <br />STORED AS INPUTFORMAT <br />  'org.apache.hadoop.mapred.TextInputFormat' <br />OUTPUTFORMAT <br />  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'<br />LOCATION<br />  'hdfs://hadoop01:9000/user/hive/warehouse/qf1603.db/log1'<br />;<br /><br />加载数据可以使用location：但是localtion后面的路径一定是hdfs中的目录。<br /><br />hive有内部表和外部表：<br />内部表：hive默认创建内部表，也叫manager_table<br />外部表：需要使用external关键字创建<br />区别：<br />删除内部表将会删除元数据和真正的数据内容。<br />删除外部表将只会删除元数据，不删除真正的数据内容。<br /><br />CREATE external TABLE log2(<br />id             string COMMENT 'this is id column',<br />phonenumber     bigint,<br />mac             string,<br />ip               string,<br />url              string,<br />status1             string,<br />status2          string,<br />up           int,<br />down           int,<br />code            int,<br />dt         String<br />)<br />COMMENT 'this is log table'<br />ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '<br />LINES TERMINATED BY '\n'<br />stored as textfile<br />;<br />load local data inpath '/home/data.log.txt' into table log2;<br /><br />修改表属性：<br />修改表名：rename to<br />alter table log rename to log2;<br />修改列名：change column<br />alter table log4 change column ip myip String;<br />alter table log4 change column myip ip String comment 'this is myip' ;<br />alter table log4 change column myip ip String comment 'this is myip' after code;<br />alter table log4 change column ip myip int comment 'this is myip' first;<br /><br />添加列：add columns<br />alter table log4 add columns(<br />x int comment 'this x',<br />y int<br />)<br />;<br /><br />删除列：<br />alter table log4 replace columns(<br />x int,<br />y int<br />)<br />;<br />alter table log4 replace columns(<br />myip int,<br />id string, <br />phonenumber bigint,<br />mac string,<br />url string,<br />status1 string,<br />status2 string,<br />up int,<br />down int,  <br />code int,<br />dt string<br />)<br />;<br />将内部表转换为外部表：<br />alter table log4 set tblproperties(<br />'EXTERNAL' = 'TRUE'<br />);<br />alter table log4 set tblproperties(<br />'EXTERNAL' = 'false'<br />);<br />alter table log4 set tblproperties(<br />'EXTERNAL' = 'FALSE'<br />);<br /><br /><br /><br />-----------------------------hive 查询<br />select<br />*  <br />from<br />join ---map阶段||reduce阶段<br />where ---map阶段<br />group by ---reduce阶段<br />having  ----reduce阶段<br />sort by	---reduce阶段<br />distributed by --reduce阶段<br />order by ----reduce阶段<br />limit ---reduce阶段<br /> <br />union   <br />union all<br /><br />1、永远是小结果集驱动大结果集(小表驱动大表，小表放在左表)。<br />2、尽量不要使用join，但是join是难以避免的。<br /><br />create table if not exists user1(<br />uid int,<br />uname String<br />)<br />row format delimited fields terminated by '\t'<br />;<br /><br />create table if not exists sex(<br />sid int,<br />sname String<br />)<br />row format delimited fields terminated by '\t'<br />;<br /><br />create table if not exists login(<br />uid int,<br />sid String,<br />logintime String<br />)<br />row format delimited fields terminated by '\t'<br />;<br /><br /><br />load data local inpath '/home/user' into table user1;<br />load data local inpath '/home/sex' into table sex;<br />load data local inpath '/home/login' into table login;<br /><br /><br />join 查询： <br />左连接(常用)<br />left join 、 left outer join 、 left semi join(左半开连接，只显示左表信息)<br />hive在0.8版本以后开始支持left join<br />left join 和 left outer join 效果差不多<br />left semi join是left join 的一种优化，并且通常用于解决exists in<br />hive的join中的on只能跟等值连接 "=",不能跟 &gt; &lt; &gt;= &lt;= !=<br /><br />select <br />u.uid,<br />u.uname,<br />s.sname,<br />l.logintime<br />from user1 u<br />left join login l<br />on u.uid = l.uid<br />left join sex s<br />on l.sid = s.sid<br />;<br /><br />select <br />u.uid,<br />u.uname,<br />s.sname,<br />l.logintime<br />from login l<br />left outer join user1 u<br />on u.uid = l.uid<br />left outer join sex s<br />on l.sid = s.sid<br />;<br /><br />select <br />l.logintime<br />from login l<br />left semi join user1 u<br />on u.uid = l.uid<br />;<br /><br />将user1表中的uid不在login表中的uid数据查询出来：<br />select <br />u1.uid,<br />u1.uname<br />from user1 u1<br />left join login l<br />on u1.uid = l.uid<br />where l.uid is NULL<br />;<br /><br />left semi join来解决不存在的问题？？？<br /><br /><br />右连接(不常用)：right join 、 right outer join  <br />hive不支持：right semi join<br />select<br />u1.*<br />from login l<br />right join user1 u1<br />on l.uid = u1.uid<br />;<br /><br />select<br />u1.uid,<br />u1.uname,<br />s.sname,<br />l.logintime<br />from sex s<br />right join login l<br />on l.sid = s.sid<br />right join user1 u1<br />on u1.uid = l.uid<br />;<br /><br />#####right outer join???<br /><br /><br />select<br />l.uid,<br />s.sname,<br />l.logintime<br />from sex s<br />right semi join login l<br />on l.sid = s.sid<br />;<br /><br /><br />from 后面跟多个表名使用","分割 、 inner join 、join ：三种效果一样<br />select<br />u1.uid,<br />u1.uname,<br />s.sname,<br />l.logintime<br />from user1 u1,sex s,login l<br />where <br />u1.uid = l.uid<br />and l.sid = s.sid<br />;<br /><br />select <br />u1.uid,<br />u1.uname,<br />s.sname,<br />l.logintime<br />from user1 u1<br />inner join login l<br />on u1.uid = l.uid<br />join sex s<br />on l.sid = s.sid<br />;<br /><br /><br />join:不加where过滤，叫笛卡尔积<br />inner join ： 内连接<br />outer join ：外链接<br />full outer join ： 全外连接，寻找表中所有满足连接(包括where过滤)。<br /><br />select<br />l.uid,<br />s.sname,<br />l.logintime<br />from login l<br />full outer join sex s<br />on l.sid = s.sid<br />;<br /><br />hive提供一个小表标识，是hive提供的一种优化机制：/*+STREAMTABLE(l)*/<br />select<br />/*+STREAMTABLE(l)*/<br />u1.uid,<br />u1.uname,<br />s.sname,<br />l.logintime<br />from user1 u1<br />inner join login l<br />on u1.uid = l.uid<br />join sex s<br />on l.sid = s.sid<br />;<br /><br /><br />map-side join：<br />当有一大一小表的时候，适合用map-join。<br />会将小表文件缓存，放到内存中，在map端和内存中的数据一一进行匹配，连接查找关系。<br />hive-1.2.1 默认已经开启map-side join：hive.auto.convert.join=true<br />select<br />l.uid,<br />s.sname,<br />l.logintime<br />from login l<br />left join sex s<br />on l.sid = s.sid<br />;<br /><br />hive 0.7版本以前，需要hive提供的mapjoin()标识。来标识该join为map-side join。标识已经过时，但是写上任然识别<br />select<br />/*+MAPJOIN(s)*/<br />l.uid,<br />s.sname,<br />l.logintime<br />from login l<br />left join sex s<br />on l.sid = s.sid<br />;<br /><br />hive怎么知道将多大文件缓存？？ 23.8M<br /> &lt;property&gt;<br />    &lt;name&gt;hive.mapjoin.smalltable.filesize&lt;/name&gt;<br />    &lt;value&gt;25000000&lt;/value&gt;<br />  &lt;/property&gt;<br /><br /><br />group by：分组，通常和聚合函数搭配使用<br />规则：<br />使用group by后，查询的字段要么出现在聚合函数中，要么出现在group by 后面。<br />select<br />l.sid,<br />count(*)<br />from login l<br />where l.sid &lt;&gt; 0<br />group by l.sid<br />;<br /><br />select<br />l.uid,<br />count(*)<br />from login l<br />where l.sid &lt;&gt; 0<br />group by l.sid,l.uid<br />;<br /><br />DEPTNO DNAME  LOC<br />create table if not exists dept(<br />deptno int,<br />dname string,<br />loc string<br />)<br />row format delimited fields terminated by ','<br />;<br /><br />load data local inpath '/home/dept' into table dept;<br /><br />select<br />d.dname,<br />d.loc<br />from dept d<br />group by d.deptno,d.dname,d.loc<br />;<br /><br />##where后面不能跟聚合函数或者聚合函数的结果，能跟普通的查询值或者是方法<br /><br />select<br />l.uid userid,<br />count(*) gendercount<br />from login l<br />where l.sid &lt;&gt; 0 and count(*) &gt; 5<br />group by l.sid,l.uid<br />;<br /><br />having:对查询出来的结果进行过滤，通常和group by搭配使用。<br />select<br />l.sid,<br />count(*) gendercount<br />from login l<br />where l.sid &lt;&gt; 0<br />group by l.sid<br />having gendercount &gt; 5<br />;<br /><br />sort by ：排序，局部排序，只能保证单个reducer的结果排序。<br />order by: 排序，全局排序。保证整个job的结果排序。<br />当reducer只有1个的时候，sort by 和 order by 效果一样。建议使用sort by<br />通常和： desc asc .(默认升序)<br /><br />select <br />u1.uid,<br />u1.uname<br />from user1 u1<br />order by u1.uid <br />;<br /><br />设置reducer个数(等于1 或者 2)：<br />mapreduce.job.reduces=2<br /><br />select <br />u1.uid,<br />u1.uname<br />from user1 u1<br />sort by u1.uid desc<br />;<br /><br />distribute by：就是控制map中如何输出到reduce。<br />整个hive语句转换成job默认都有该过程，如果不写，<br />默认使用第一列的hash值来分。当只有一个reducer的时候不能体现出来。<br /><br />如果distribute by和sort by 一起出现的时候注意顺序问题？？distribute by在前面<br /><br /><br />clusterd by ： 它等价于distribute by和sort by(升序)。后面跟的字段名需要一样<br />clusterd by它既兼有distribute by，还兼有sort by (只能是升序)<br /><br />select <br />u1.uid,<br />u1.uname<br />from user1 u1<br />distribute by u1.uid<br />sort by u1.uid asc<br />;<br /><br />=====上和下等价<br />select <br />u1.uid,<br />u1.uname<br />from user1 u1<br />cluster by u1.uid<br />;<br /><br /><br />limit : 限制结果集的。<br /><br /><br />union all：将两个或者多个查询的结果集合并到一起，不去重每一个结果集排序<br />union：将两个或者多个查询结果集合并到一起，去重,合并后的数据排序<br />注意：<br /><br />select <br />u1.uid userid,<br />u1.uname username<br />from user1 u1<br />union all <br />select <br />s1.sid userid,<br />s1.sname username<br />from sex s1<br />;<br /><br />select <br />u1.uid userid,<br />u1.uname username<br />from user1 u1<br />union <br />select <br />s1.sid userid,<br />s1.sname username<br />from sex s1<br />;<br /><br /><br />select <br />u1.uid userid,<br />u1.uname username<br />from user1 u1<br />where u1.uid &gt; 10<br />union <br />select <br />s1.sid userid,<br />s1.sname username<br />from sex s1<br />sort by userid desc<br />limit 1<br />;<br />###不是很建议使用子查询<br />select <br />tmp.*<br />from<br />(<br />select <br />u1.uid userid,<br />u1.uname username<br />from user1 u1<br />where u1.uid &gt; 10<br />union <br />select <br />s1.sid userid,<br />s1.sname username<br />from sex s1<br />) tmp<br />sort by tmp.userid desc<br />;<br /><br />hive的hql语句中的where 后对子查询支持不是很好。通常使用in，不好。<br />select<br />l.uid<br />from login l<br />;<br /><br />select <br />u1.uid,<br />u1.uname<br />from user1 u1<br />where u1.uid in (<br />select<br />l.uid<br />from login l<br />)<br />;<br /><br />select <br />u1.uid,<br />u1.uname<br />from user1 u1<br />where u1.uid in (<br />select<br />l.uid<br />from login l<br />where l.uid = 1<br />)<br />;<br /><br /><br />--------------------------hive 分区<br />为什么要分区？？<br />当单个表数据量越来越大的时候，hive查询通常会全表扫描，这将会浪费我们不关心数据的扫描，浪费大量时间。从而hive引出分区概念<br />partition<br />怎么分区？？<br />看具体业务，能把一堆数据拆分成多个堆的数据就可以。<br />通常使用id 、 年 、 月 、天 、区域 、省份、<br />hive分区和mysql分区的区别？？<br />mysql的分区字段采用的表内字段。<br />hive的分区字段使用的是表外字段。<br />hive分区细节？？<br />1、分区本质是在该表下创建对应的目录。<br />2、分区名大小写不区分，建议不要使用中文。<br />3、可以查询分区信息。但是我们的分区字段相当于是一个伪字段，在元数据中存在，但是不真实存在数据内容中。<br /><br />创建一级分区表：<br /><br />create table if not exists day_part(<br />uid int,<br />uname string<br />)<br />partitioned by(year int)<br />row format delimited fields terminated by '\t'<br />;<br /><br />load data local inpath '/home/user' into table day_part partition(year=2017);<br />load data local inpath '/home/sex' into table day_part partition(year=2016);<br /><br />show partitions day_part;<br /><br /><br />create table if not exists day_part1(<br />uid int,<br />uname string<br />)<br />partitioned by(year int,month int)<br />row format delimited fields terminated by '\t'<br />;<br /><br />load data local inpath '/home/user' into table day_part partition(year=2017,month=04);<br />load data local inpath '/home/sex' into table day_part partition(year=2017,month=03);<br /><br />三级分区：<br />create table if not exists day_part2(<br />uid int,<br />uname string<br />)<br />partitioned by(year int,month int,day int)<br />row format delimited fields terminated by '\t'<br />;<br /><br /><br />对分区进行操作：<br />显示分区：show partitions day_part;<br />新增分区：空的<br />alter table day_part1 add partition(year=2017,month=2);<br />alter table day_part1 add partition(year=2017,month=1) partition(year=2016,month=12);<br />新增分区并加载数据：<br />alter table day_part1 add partition(year=2016,month=11) location "/user/hive/warehouse/qf1603.db/day_part1/year=2017/month=2";<br />修改分区的名字？？尝试<br />修改分区所对应的存储路径：<br />alter table day_part1 partition(year=2016,month=11) set location "hdfs://hadoop01:9000/user/hive/warehouse/qf1603.db/day_part1/year=2017/month=3";<br />删除分区：删除分区将会删除对应的分区目录<br />alter table day_part1 drop partition(year=2017,month=2);<br />alter table day_part1 drop partition(year=2017,month=3),partition(year=2017,month=4);<br /><br />静态分区、动态分区、混合分区<br />静态分区：新增分区或者是加载分区数据时，已经指定分区名。<br />动态分区：新增分区或者是加载分区数据时，分区名未知。<br />混合分区：静态分区和动态分区同时存在。<br /><br />动态分区的相关属性：<br />hive.exec.dynamic.partition=true :是否允许动态分区<br />hive.exec.dynamic.partition.mode=strict ：分区模式设置nostrict<br />strict：最少需要有一个是静态分区<br />nostrict：可以全部是动态分区<br />hive.exec.max.dynamic.partitions=1000 ：允许动态分区的最大数量<br />hive.exec.max.dynamic.partitions.pernode =100 ：单个节点上的mapper/reducer允许创建的最大分区<br /><br />创建临时表：<br />create table if not exists tmp(<br />uid int,<br />commentid bigint,<br />recommentid bigint,<br />year int,<br />month int,<br />day int<br />)<br />row format delimited fields terminated by '\t'<br />;<br /><br />load data local inpath '/home/comm' into table tmp;<br /><br />创建动态分区：<br />create table if not exists dyp1(<br />uid int,<br />commentid bigint,<br />recommentid bigint<br />)<br />partitioned by(year int,month int,day int)<br />row format delimited fields terminated by '\t'<br />;<br /><br />未动态分区加载数据：<br />insert into table dyp1 partition(year=2016,month,day)<br />select uid,commentid,recommentid,month,day from tmp<br />;<br /><br />create table if not exists dyp2(<br />uid int,<br />commentid bigint,<br />recommentid bigint<br />)<br />partitioned by(year int,month int,day int)<br />row format delimited fields terminated by '\t'<br />;<br /><br /><br />insert into table dyp2 partition(year,month,day)<br />select uid,commentid,recommentid,year,month,day from tmp<br />;<br /><br />hive提供我们一个严格模式：为咯阻止用户不小心提交恶意hql<br />hive.mapred.mode=nostrict  : strict   <br />如果该模式值为strict，将会阻止一下三种查询：<br />1、对分区表查询，where中过滤字段不是分区字段。<br />2、笛卡尔积join查询，join查询语句，不带on条件 或者 where条件。<br />select <br />u1.uid,<br />u1.uname,<br />l.logintime<br />from user1 u1<br />join login l<br />;<br /><br />可以：<br />select <br />u1.uid,<br />u1.uname,<br />l.logintime<br />from user1 u1<br />join login l<br />where u1.uid = l.uid<br />;<br />3、对order by查询，有order by的查询不带limit语句。<br />select<br />u1.*<br />from user1 u1<br />order by u1.uid desc<br />;<br /><br />注意：<br />1、尽量不要是用动态分区，因为动态分区的时候，将会为每一个分区分配reducer数量，当分区数量多的时候，reducer数量将会增加，对服务器是一种灾难。<br />2、动态分区和静态分区的区别，静态分区不管有没有数据都将会创建该分区，动态分区是有结果集将创建，否则不创建。<br />3、hive动态分区的严格模式和hive提供的hive.mapred.mode的严格模式。<br /><br /><br /><br />----------------分桶-----------------<br />为什么要分桶？？分区数据依然很大，对分区数据或者表数据更加细粒度的管理。<br />分桶关键字：<br />clustered by(uid) into n buckets 、bucket 、 分桶使用表内字段<br />怎么分桶？？<br />对分桶字段进行hash值，然后将hash值模于总的桶数，然后得到桶数<br />分桶的意义：<br />1、快速抽样查询。tablesample<br />2、减少查询扫描数据量，提高查询效率。<br /><br />create table if not exists bucket1(<br />uid int,<br />uname String<br />)<br />clustered by(uid) into 4 buckets<br />row format delimited fields terminated by '\t'<br />;<br /><br />为分桶表加载数据：<br />分桶不能使用load方式来加载数据，而需要iinsert into方式来加载<br />并且需要设置属性：<br />hive&gt; set hive.enforce.bucketing=true;<br /><br />load data local inpath '/home/user' into table bucket1;<br /><br />create table if not exists bucket7(<br />uid int,<br />uname String<br />)<br />clustered by(uid) into 4 buckets<br />row format delimited fields terminated by '\t'<br />;<br /><br />insert into table bucket7<br />select * from user1<br />;<br /><br />分桶查询：tablesample(bucket x out of y on uid)<br />注意：x不能大于y<br />x：所取桶的起始位置，<br />y：所取桶的总数，y是总桶数的因子。y大于源总桶数相当于拉伸，y小于源总桶数相当于压缩<br />1 out of 2<br />1 1+4/2<br />2 out of 2<br />2 2+4/2<br /><br />1 out of 4<br />1 1+4<br /><br />select * from bucket7;<br />select * from bucket7 tablesample(bucket 1 out of 4 on uid);<br />select * from bucket7 tablesample(bucket 2 out of 4 on uid);<br />select * from bucket7 tablesample(bucket 1 out of 2 on uid);<br />select * from bucket7 tablesample(bucket 2 out of 2 on uid);<br />select * from bucket7 tablesample(bucket 3 out of 2 on uid);<br />select * from bucket7 tablesample(bucket 1 out of 8 on uid);<br />select * from bucket7 tablesample(bucket 5 out of 8 on uid);<br /><br /><br /><br />分区+分桶：(qfstu) uid,uname,class,master gender分区  分桶uid 基偶分桶<br />查询女生中的学号为基数？？<br />create table if not exists qftmp(<br />uid int,<br />uname string,<br />class int,<br />gender int<br />)<br />row format delimited fields terminated by '\t'<br />;<br /><br />load data local inpath '/home/qf' into table qftmp;<br /><br />create table if not exists qf(<br />uid int,<br />uname string,<br />class int<br />)<br />partitioned by(gender int)<br />clustered by(uid) into 2 buckets<br />row format delimited fields terminated by '\t'<br />;<br /><br />insert into table qf partition(gender)<br />select uid,uname,class,gender from qftmp;<br /><br />####查询女生中的学号为基数？？<br />select * from qf where gender = 2 and uid%2 != 0;<br />select * from qf tablesample(bucket 2 out of 2 on uid) where gender = 2;<br /><br />##<br />分桶使用内部关键字，分区使用的是外部字段。<br />两者都是对hive的一个优化。<br />分区和分桶的数量都要合理设置，不是越多越好。<br /><br />抽样：<br />select * from user1 order by rand() limit 3;<br />select * from user1 limit 3;<br />select * from user1 tablesample(3 rows);<br />select * from user1 tablesample(20B); ##最小单位是B<br />select * from user1 tablesample(20 percent);<br /><br />--------------------------serde<br />serde:<br />serialize:序列化<br />deserialize:反序列化<br /><br />常见的serde：CSV(comma split value) 、  TSV(Tab ) 、 Json<br /><br />属性默认值：<br />with serdeproperties(<br />'separatorChar'="," ,   #指定分割符<br />'qutoChar'='"'			#指定字段的引号<br />'escapeChar'='\'		#指定转移符<br />)<br /><br />创建表csv：<br />create table if not exists csv1(<br />uid int,<br />uname string<br />)<br />row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'<br />;<br /><br />load data local inpath '/home/1603.csv' into table csv1;<br /><br />create table if not exists csv2(<br />uid int,<br />uname string<br />)<br />row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'<br />with serdeproperties(<br />'separatorChar'=',',<br />'qutoChar'='"',<br />'escapeChar'="\\"<br />)<br />store as textfile<br />;<br />load data local inpath '/home/1603.csv' into table csv2;<br /><br /><br />josn serde:<br /><br />在cli端添加所需jar:add jar /home/json-serde-1.3-jar-with-dependencies.jar;<br /><br />create table if not exists json1(<br />id int,<br />content string<br />)<br />row format serde 'org.openx.data.jsonserde.JsonSerDe'<br />;<br /><br />create table if not exists json2(<br />id int,<br />content string<br />)<br />row format delimited fields terminated by ','<br />;<br /><br />load data local inpath '/home/js1' into table json2;<br /><br />{"id":1,"content":"this is first content"}<br />{"id":2,"content":"this is second content"}<br /><br />create table if not exists json3(<br />provice string,<br />city Array&lt;String&gt;,<br />person map&lt;String,Array&lt;Int&gt;&gt;<br />)<br />row format serde 'org.openx.data.jsonserde.JsonSerDe'<br />;<br />load data local inpath '/home/js2' into table json3;<br />{"provice":"山西","city":["大同","临汾","太原"],"person":{"man":[100,200,50],"woman":[80,120,30]}}<br />{"provice":"河北","city":["石家庄","保定","唐山"],"person":{"man":[200,300,80],"woman":[100,160,89]}}<br /><br />select * from json3 where size(city) &gt;=3 and person["woman"][0] &gt; 80;<br /><br />serde hive提供一些默认serde，第三方也有提供，自己也可以自定义。<br /><br />###RegexSerDe????<br /><br /><br /><br />-------------------hive的数据类型<br />基础数据类型<br />复杂数据类型<br />：<br />tinyint			1字节(-128~127)<br />smallint		2<br />int				4<br />bigint			8<br />double			8<br />float			4<br />String			<br />boolean			1    (true/false)<br />binary					hive 0.8以后才有<br />timestamp				hive 0.8以后才有(2017-04-20 09:28:00)<br /><br />java中有的hive没有的：<br />char<br />short<br />byte<br />long<br /><br /><br />create table if not exists base(<br />id tinyint,<br />id1 smallint,<br />id2 int,<br />id3 bigint,<br />salary double,<br />salary1 float,<br />isok boolean,<br />content binary,<br />dt timestamp<br />)<br />row format delimited fields terminated by ' '<br />;<br /><br /><br />复杂数据类型：<br />Array ：数组。 arr Array&lt;dataType&gt;<br />map ：key-value集合。 map1 Map&lt;String,int&gt;<br />struct ： 结构体。 str1 Struct&lt;String,String,string,...&gt;<br /><br /><br /><br />create table if not exists arr1(<br />uname string,<br />ie Array&lt;String&gt;<br />)<br />row format delimited fields terminated by ' '<br />;<br /><br />create table if not exists arr2(<br />uname string,<br />ie Array&lt;String&gt;<br />)<br />row format delimited fields terminated by ' '<br />collection items terminated by ','<br />;<br /><br />zs 120.3,168.6<br />ww 121.3,158.2<br />ls 122.2,160.3<br />load data local inpath '/home/ie' into table arr2;<br /><br />数组长度大等于2 查询姓名和第一个元素<br />select <br />a.uname,<br />a.ie[0]<br />from arr2 a <br />where size(a.ie) &gt;=2<br />;<br /><br /><br />集合： 两个分隔符指定顺序不能颠倒<br />create table if not exists map1(<br />uname string,<br />mp1 Map&lt;String,Double&gt;<br />)<br />row format delimited fields terminated by ' '<br />collection items terminated by ','<br />map keys terminated by ':'<br />;<br /><br />zs high:180,weight:130<br />li high:177,weight:120<br />load data local inpath '/home/m1' into table map1;<br /><br />查询map长度大于等于2 身高大于178的用户的名字 、 升高 、体重<br />select<br />m.uname,<br />m.mp1["high"],<br />m.mp1["weight"]<br />from map1 m<br />where size(m.mp1) &gt;= 2 <br />and m.mp1["high"] &gt; 178<br />;<br /><br />Struct：结构体<br />create table if not exists struct1(<br />uname string,<br />str1 Struct&lt;chinese:String,cscore:double,math:String,mscore:String&gt;<br />)<br />row format delimited fields terminated by ' '<br />collection items terminated by ','<br />;<br /><br />create table if not exists struct2(<br />uname string,<br />str1 Struct&lt;chinese:double,math:double&gt;<br />)<br />row format delimited fields terminated by ' '<br />collection items terminated by ','<br />;<br /><br /><br />zs 80,88<br />ls 90,68<br />load data local inpath '/home/str' into table struct2;<br /><br />查询结构体长度大于等于2 语文大于85 的用户名 、语文 、数学<br />select <br />s.uname,<br />s.str1.chinese,<br />s.str1.math<br />from struct2 s<br />where <br />s.str1.chinese &gt; 85<br />;<br /><br />####map array的组合 ???????<br />create table if not exists map1(<br />uname string,<br />m1 Map&lt;string,Array&lt;int&gt;&gt;<br />)<br />row format delimited fields terminated by ' '<br />collection items terminated by ','<br />map keys terminated by ':'<br />;<br /><br />创建表：<br />create table if not exists comtax(<br />id int,<br />belone Array&lt;int&gt;,<br />tax Map&lt;String,int&gt;,<br />add Struct&lt;province:String,city:String,street:String&gt;<br />)<br />row format delimited fields terminated by '\t'<br />collection items terminated by " "<br />map keys terminated by ":"<br />stored as textfile<br />;<br />											<br />1	20,21,22,23	wuxian:300 gongjijin:600 北京 海淀 天丰利<br />2	30,31,32,33	wuxian:322 gongjijin:800 北京 大兴 天宫院<br />load data local inpath '/home/comtax' into table comtax;<br /><br /><br />----------------hive 内部函数<br />1、rand()<br />select rand();<br />2、split(str,splitor) #注意特殊分割符的转义<br />select split(5.0,"\\.")[0];<br />select split(rand()*100,"\\.")[0];<br />3、substr(str,x,y)  substring()<br />select substr(rand()*100,0,2);<br />select substring(rand()*100,0,2);<br />4、if(expr,truestament,falsestament)<br />select if(100&gt;10,"this is true","this is false");<br />select if(2=1,"男","女");<br />select if(1=1,"男",(if(1=2,"女","不知道")));<br />select if(3=1,"男",(if(3=2,"女","不知道")));<br />5、case i when v ...else  end; case when i&lt;expr when ..else  end;<br />select<br />case 6<br />when 1 then "100"<br />when 2 then "200"<br />when 3 then "300"<br />when 4 then "400"<br />else "others"<br />end<br />; <br /><br />create table if not exists cw(<br />flag int<br />)<br />;<br />load data local inpath '/home/flag' into table cw;<br />select<br />case c.flag<br />when 1 then "100"<br />when 2 then "200"<br />when 3 then "300"<br />when 4 then "400"<br />else "others"<br />end<br />from cw c<br />; <br /><br /><br />select<br />case<br />when 1=c.flag then "100"<br />when 2=c.flag then "200"<br />when 3=c.flag then "300"<br />when 4=c.flag then "400"<br />else "others"<br />end<br />from cw c<br />;<br /><br />6、regexp_replace(str,regex,"替换成什么样")<br />select regexp_replace("1.jsp",".jsp",".html");<br /><br />7、cast ： 类型转换 cast(str as double)<br />select 1;<br />select cast(1 as double);<br />select cast("12" as int);<br /><br />8、concat(str1,str2,...) concat_ws()<br /><br />select "千峰" + 1603 + "班级";<br />select concat("千峰",1603,"班级");<br />select concat_ws("|","千峰","1603","班级");<br /><br />9、排名函数：<br />row_number(): 名次不并列<br />rank():名次并列，但空位<br />dense_rank():名次并列，但不空位<br /><br />id	class score<br />1	1	90<br />2	1	85 <br />3	1	87 <br />4	1	60 <br />5	2	82 <br />6	2	70 <br />7	2	67 <br />8	2	88 <br />9	2	93 <br /><br />1	1	90	1<br />3	1	87	2<br />2	1	85	3<br />9	2	93	1<br />8	2	88	2<br />5	2	82	3<br /><br />create table if not exists uscore(<br />uid int,<br />classid int,<br />score double<br />)<br />row format delimited fields terminated by '\t'<br />;<br />load data local inpath '/home/uscore' into table uscore;<br /><br />select<br />u.uid,<br />u.classid,<br />u.score<br />from uscore u<br />group by u.classid,u.uid,u.score<br />limit 3<br />;<br /><br /><br />select<br />u.uid,<br />u.classid,<br />u.score,<br />row_number() over(distribute by u.classid sort by u.score desc) rn<br />from uscore u<br />;<br /><br />##取前三名<br />select<br />t.uid,<br />t.classid,<br />t.score<br />from<br />(<br />select<br />u.uid,<br />u.classid,<br />u.score,<br />row_number() over(distribute by u.classid sort by u.score desc) rn<br />from uscore u<br />) t<br />where t.rn &lt; 4<br />;<br /><br />####查看三个排名区别<br />select<br />u.uid,<br />u.classid,<br />u.score,<br />row_number() over(distribute by u.classid sort by u.score desc) rn,<br />rank()  over(distribute by u.classid sort by u.score desc) rank,<br />dense_rank()  over(distribute by u.classid sort by u.score desc) dr<br />from uscore u<br />;<br /><br />聚合函数：min() max() count() count(distinct ) sum() avg()<br />count(1):不管正行有没有值，只要出现就累计1<br />count(*):正行值只要有一个不为空就给类计1<br />count(col)：col列有值就累计1<br />count(distinct col)：col列有值并且不相同才累计1<br /><br /><br />######<br />几乎任何数和 NULL操作都返回NULL<br /><br /><br />&amp; ： 同真为真，其余全假<br />| ： 同假为假，其余全真<br /><br />&amp;&amp; 和 || 不行？？？<br /><br />size()<br />round()<br />select length("");<br />locat()<br />trim()<br />parse_url()<br />hash()<br /><br /><br /><br />select from_unixtime(timestamp,"yyyy-MM-dd")<br />select unix_timestamp();<br />select unix_timestamp("2017-04-19 00:00:00");<br />select year("2017-04-19 00:00:00");<br />select month("2017-04-19 00:00:00");<br />select day("2017-04-19 00:00:00");<br />select hour("2017-04-19 00:00:00");<br />select weekofyear("2017-04-19 00:00:00");<br /><br />------------------------udf 函数<br />为什么需要udf？<br />1、因为内部函数没法满足需求。<br />2、hive它本身就是一个灵活框架，允许用自定义模块功能，如可以自定义udf、serde、输入输出等。<br />udf是什么？<br />udf：user difine function，用户自定义函数，一对一。常用<br />udaf:user define aggregate function,用户自定义聚合函数，多对一。<br />udtf：user define table_generate function，用户自定义表生成函数，一对多。<br />怎么变写udf函数？？<br />1、该类需要继承UDF,重写evaluate(),允许该方法重载。<br />2、也可以继承 generic UDF,需要重写 initliaze() 、 getDisplay() 、 evaluate()<br /><br />udf的使用？？<br />第一种方法：(当前session有效)<br />1、添加自定udf的jar包<br />hive&gt;add jar /home/h2h.jar;<br />2、创建临时函数<br />hive&gt;create temporary function myfunc as "edu.qianfeng.udf.FirstUdf";<br />3、测试是否添加好：<br />show functions;<br />select myfunc("1603");<br />4、确定无用时可以删除：<br />drop temporary function myfunc;<br /><br />第二种：(当前session有效)<br />1、添加自定udf的jar包(hive.aux.jars.path在该目录下的jar会在hive启动时自动加载)<br />&lt;property&gt;<br />	&lt;name&gt;hive.aux.jars.path&lt;/name&gt;<br />	&lt;value&gt;$HIVE_HOME/auxlib&lt;/value&gt;<br />&lt;/property&gt;<br />2、创建临时函数<br />hive&gt;create temporary function ktv as "edu.qianfeng.udf.KeyToValue";<br />3、测试是否添加好：<br />show functions;<br />select myfunc("1603");<br />4、确定无用时可以删除：<br />drop temporary function myfunc;<br /><br />第三种：(当前session有效)<br />1、创建一个初始化文件：<br />vi ./init-hive<br />add jar /home/h2h.jar;<br />create temporary function ktv1 as "edu.qianfeng.udf.KeyToValue";<br />2、启动使用命令：hive -i ./init-hive<br /><br />3、测试是否添加好：<br />show functions;<br />select myfunc("1603");<br />4、确定无用时可以删除：<br />drop temporary function myfunc;<br /><br /><br />第四种：(做成永久性)<br />需要对源码编译。<br />1)将写好的Jave文件拷贝到~/install/hive-0.8.1/src/ql/src/java/org/apache/hadoop/hive/ql/udf/ <br />cd  ~/install/hive-0.8.1/src/ql/src/java/org/apache/hadoop/hive/ql/udf/<br />ls -lhgt |head<br />2)修改~/install/hive-0.8.1/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java，增加import和RegisterUDF<br /><br />import com.meilishuo.hive.udf.UDFIp2Long;   //添加import<br /><br />registerUDF("ip2long", UDFIp2Long.class, false); //添加register<br />3)在~/install/hive-0.8.1/src下运行ant -Dhadoop.version=1.0.1 package<br />cd ~/install/hive-0.8.1/src<br />ant -Dhadoop.version=1.0.1 package<br />4)替换exec的jar包，新生成的包在/hive-0.8.1/src/build/ql目录下,替换链接    <br />cp hive-exec-0.8.1.jar /hadoop/hive/lib/hive-exec-0.8.1.jar.0628<br />rm hive-exec-0.8.1.jar<br />ln -s hive-exec-0.8.1.jar.0628 hive-exec-0.8.1.jar<br />5)重启进行测试<br /><br /><br /><br />4、数据：根据key找value：<br />{"code":0,"success":true,"message":"请求成功","data":{"supplier":{"isSupportMultipleJourney":0,"rows":[{"resId":1683581010,"departureDates":null,"agencyId":28600,"companyName":"春之旅（呼和浩特）/尾货","aggregationAgencyId":9002459,"aggregationCompanyName":"内蒙春之旅","fullName":"内蒙古春之旅旅行社有限公司","groundOperatorsId":0,"groundOperatorsName":null,"satisfactionInfo":[{"satisfactionNum":0.8689,"satisfactionDifference":-0.0454,"targetAreaCode":8,"targetSecondGroupCode":209}]}],"displayDestGroupName":"本州"}}}<br /><br /><br />5、根据key找到value。<br />如：sex=1&amp;hight=180&amp;weight=130&amp;sal=28000<br />select fun1("sex=1&amp;hight=180&amp;weight=130&amp;sal=28000","weight") 130<br /><br /><br />6、字符串反转：如：<a href="http://www.baidu.com">www.baidu.com</a><br />select fun1("<a href="http://www.baidu.com");">www.baidu.com");</a> moc.udibd.www<br /><br />7、域名反转：如：<a href="http://www.baidu.com">www.baidu.com</a><br />select fun1("<a href="http://www.baidu.com");">www.baidu.com");</a> com.baidu.www<br /><br />8、根据生日获取星座：<br />select fun1("2016-12-16"); 摩羯座<br /><br />"摩羯座","水瓶座","双鱼座" ,"白羊座","金牛座","双子座","巨蟹座","狮子座","处女座","天秤座","天蝎座","射手座" <br /><br /><br /><br />9、获取字符串拆分后指定位置的字符：<br />select fun1("北京市 海淀区 中关村"," ",1); 海淀区<br /><br />10、正则表达式解析日志：<br />解析前：<br />220.181.108.151 - - [31/Jan/2012:00:02:32 +0800] \"GET /home.php?mod=space&amp;uid=158&amp;do=album&amp;view=me&amp;from=space HTTP/1.1\" 200 8784 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +<a href="http://www.baidu.com/search/spider.html)\"">http://www.baidu.com/search/spider.html)\"</a><br />解析后：<br />220.181.108.151	20120131 120232	GET	/home.php?mod=space&amp;uid=158&amp;do=album&amp;view=me&amp;from=space	HTTP	200	Mozilla<br /><br />5、根据key找到value。<br />如：sex=1&amp;hight=180&amp;weight=130&amp;sal=28000<br />select fun1("sex=1&amp;hight=180&amp;weight=130&amp;sal=28000","weight") 130<br /><br /><br />6、字符串反转：如：<a href="http://www.baidu.com">www.baidu.com</a><br />select fun1("<a href="http://www.baidu.com");">www.baidu.com");</a> moc.udibd.www<br /><br />7、域名反转：如：<a href="http://www.baidu.com">www.baidu.com</a><br />select fun1("<a href="http://www.baidu.com");">www.baidu.com");</a> com.baidu.www<br /><br />8、根据生日获取星座：<br />select fun1("2016-12-16"); 摩羯座<br /><br />"摩羯座","水瓶座","双鱼座" ,"白羊座","金牛座","双子座","巨蟹座","狮子座","处女座","天秤座","天蝎座","射手座" <br /><br />22,20,19,21,21,21,22,23,23,23,23,22<br /><br />0.22&lt;1.20 <br />1.20-2.19<br />2.19-3.21<br />      d  s   m-1<br />1.11  11 22  0<br />2.18  18 20  1<br />3.22  22 19  3<br />5.12  12 21  4<br /><br /><br />9、获取字符串拆分后指定位置的字符：<br />select fun1("北京市 海淀区 中关村"," ",1); 海淀区<br /><br />10、正则表达式解析日志：<br />解析前：<br />220.181.108.151 - - [31/Jan/2012:00:02:32 +0800] \"GET /home.php?mod=space&amp;uid=158&amp;do=album&amp;view=me&amp;from=space HTTP/1.1\" 200 8784 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +<a href="http://www.baidu.com/search/spider.html)\"">http://www.baidu.com/search/spider.html)\"</a><br />解析后：<br />220.181.108.151	20120131 120232	GET	/home.php?mod=space&amp;uid=158&amp;do=album&amp;view=me&amp;from=space	HTTP	200	Mozilla<br /><br />##<br />^()()()$<br />^([0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}) - - \[(.*)\] (.*) (.*) .*$<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />String regex = "^([0-9.]+\\d+) - - \\[(.* \\+\\d+)\\] .+(GET|POST) (.+) (HTTP)\\S+ (\\d+) .+ \"([A-Za-z]+).+$";<br />String regex = "^([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}) - - \\[(.* \\+\\d+)\\] .+(GET|POST) (.+) (HTTP)\\S+ (\\d+) .+ \"([A-Za-z]+).+ \\+(.*)\\).+$";<br /><br /><br />-----------UDAF------------<br />1、求一列的最大值<br />public class myMax extends UDAF{<br />	public static class MaximumIntUDAFEvaluator implements UDAFEvaluator{<br />		private IntWritable result;<br />		@Override<br />		public void init() {<br />			result = null;<br />		}<br />		public boolean iterate(IntWritable value){<br />			if(value==null){<br />				return true;<br />			}<br />			if(result==null){<br />				result = new IntWritable(value.get());<br />			} else {<br />				result.set(Math.max(result.get(), value.get()));<br />			}<br />			return true;<br />		}<br />		public IntWritable teminatePartial(){<br />			return result;<br />		}<br />		public boolean merge(IntWritable other){<br />			return iterate(other);<br />		}<br />		public IntWritable terminate(){<br />			return result;<br />		}<br />	}<br />}<br /><br /><br />---------------------hive 数据导入导出-----------<br />hive没有严格数据格式。<br />hive不能像mysql一样进行局部数据的插入、修改、删除。<br />数据导入：<br />1、从linux中导入到hive表中  <br />2、从hdfs中导入的hive表中<br />3、从hive一张表中导入到另外一张表中<br />4、手动copy到hive表目录下<br />5、location hdfs的目录导入<br />6、like location (克隆表)<br />7、CTAS 导入hive表<br />8、from 多表导入<br /><br /><br />load data local inpath '/home/hd' overwirte into table hd;<br />load data inpath '/home/hd' overwirte into table hd;<br />insert into table hd<br />select * from hdtmp [where];<br />create table if not exists user2(<br />uid bigint,<br />uname string<br />)<br />row format delimited fields terminated by '\t'<br />location 'hdfs://hadoop01:9000/user/hive/warehouse/qf1603.db/user1'<br />;<br /><br />create table log5 like log4;  ###克隆表结构<br />###克隆表结构并加载数据<br />create table log6 like log4 location "hdfs://hadoop01:9000/user/hive/warehouse/qf1603.db/log4";<br /><br />##ctas<br />create table if not exists log7<br />as <br />select myip,id,phonenumber,dt<br />from log4<br />where id = 15<br />;<br />####CTAS 没有结果集任然会创建表<br />create table if not exists log8<br />as <br />select myip,id,phonenumber,dt<br />from log4<br />where id = 15<br />;<br /><br /><br />create table if not exists log9(<br />myip string,<br />id bigint<br />)<br />row format delimited fields terminated by '\t'<br />;<br /><br />create table if not exists log10(<br />myip string,<br />id bigint,<br />phonenumber string<br />)<br />row format delimited fields terminated by '\t'<br />;<br />##from 一次扫描多次导入<br />from log4<br />insert into log9<br />select myip,id<br />where id=15649428888<br />insert into log10<br />select myip,id,phonenumber<br />where id = 15652106622<br />;<br /><br /><br />数据导出：<br />1、将hive中的数据导出到linux目录中<br />2、将hive表中的数据导出到hdfs目录中<br />3、将hive表中的数据到处去hive的一张表中#######<br />4、将hive表中的数据导出到linux的文件中<br />insert overwrite [local] directory   #####不是insert into<br />insert overwrite local directory '/home/out/00'<br />select * from log9<br />;<br /><br />insert overwrite directory '/home/out/00'<br />select * from log9<br />;<br />###解决导出后数据字段分隔符问题：<br />insert overwrite local directory '/home/out/01'<br />row format delimited fields terminated by '\t'<br />select * from log9<br />;<br /><br />hive -e "use qf1603;select * from log9" &gt;&gt; /home/out/02;<br />###  -S  静音模式，不会输出和结果集无关信息<br />hive -S -e "use qf1603;select * from user1" &gt; /home/out/03;<br /><br />---------------------hive 的视图(view)<br />视图是什么？？<br />视图相当于一个表。<br />hive目前只支持逻辑视图，而不支持物理视图。<br />视图的优点：<br />1、降低复杂查询。<br />2、可以将数据很好过滤(局部暴露)。<br /><br />1、创建视图 CVAS<br />create view if not exists v1 as select * from user1;<br />create view if not exists v2 as select * from u;<br />????视图是否可以克隆 like???<br />create view if not exists v2 like v1;<br /><br />查询视图：<br />select * from v1 where id = 10000;<br /><br />2、显示视图：<br />show tables;<br />###<br />DDL：数据结构定义语言(操作表结构的命令)。create 、 drop 、create database 、alter 、<br />DML：数据操作语言(操作数据的命令)。insert 、 select 、drop 、<br />3、查看视图<br />desc v1;<br />desc extended v1;<br />show create table v1;<br /><br />4、删除视图<br />drop view if exists v2;<br /><br />###<br />1、视图不能用insert into 或者load方式来加载数据。<br />2、切忌将表删除后查询视图。<br /><br /><br />-----------------hive 的索引<br />索引是数据库的标准技术，它可以提高数据查询效率。<br /><br />hive是0.7版本后开始支持索引，但是hive对索引效果并不是很理想。<br /><br />索引的优点：先扫描索引文件，将阻止全表扫描；提高查询效率。<br />索引的缺点：冗余存储；对数据写入将变慢。<br />索引文件本身排序，再加上索引文件较小，索引扫描索引文件比较快速。<br /><br />先创建表：<br />create table if not exists idx(<br />id bigint,<br />url string,<br />kv string,<br />dt bigint<br />)<br />row format delimited fields terminated by '001'<br />;<br />load data local inpath '/home/bhv_14139.txt' overwrite into table idx;<br /><br />创建索引前查询：<br />select count(id)  from idx;   32.558 seconds<br />创建索引后查询：<br />select count(id)  from idx;   21.381 seconds<br /><br /><br />创建索引：<br />create index idx_idx_id<br />on table idx(id)<br />as 'compact'<br />with deferred rebuild<br />;<br /><br />创建联合索引：<br />create index idx_idx_id_dt<br />on table idx(id,dt)<br />as 'compact'<br />with deferred rebuild<br />;<br /><br />###bitmap类型：也可以创建联合索引：<br />create index idx_idx_idbm<br />on table idx(id)<br />as 'bitmap'<br />with deferred rebuild<br />;<br /><br />修改索引：(相当于重建索引，不可以修改索引的名字？？尝试是否可以修改)<br />alter index idx_idx_idbm<br />on idx rebuild<br />;<br /><br /><br />显示索引：<br />show index on 表名;<br /><br />删除索引：<br />drop index idx_idx_idbm on idx;<br /><br />关于索引属性：<br /><br />&lt;property&gt;<br />  &lt;name&gt;hive.optimize.index.filter&lt;/name&gt;<br />  &lt;value&gt;false&lt;/value&gt;<br />  &lt;description&gt;Whether to enable automatic use of indexes&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />  &lt;name&gt;hive.optimize.index.autoupdate&lt;/name&gt;<br />  &lt;value&gt;false&lt;/value&gt;<br />  &lt;description&gt;Whether to update stale indexes automatically&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />  &lt;name&gt;hive.index.compact.binary.search&lt;/name&gt;<br />  &lt;value&gt;true&lt;/value&gt;<br />  &lt;description&gt;Whether or not to use a binary search to find the entries in an index table that match the filter, where possible&lt;/description&gt;<br />&lt;/property&gt;<br />  &lt;property&gt;<br />  &lt;name&gt;hive.optimize.index.groupby&lt;/name&gt;<br />  &lt;value&gt;false&lt;/value&gt;<br />  &lt;description&gt;Whether to enable optimization of group-by queries using Aggregate indexes.&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br /><br /><br />-------------------------hive 的log----------------<br />hive的分为系统log、query log<br />系统log：启动停止hive服务、hive运行产生的log日志<br />query log：hive语句执行产生的log日志<br /><br />系统log的配置：<br />hive.log.dir=${java.io.tmpdir}/${user.name}<br />hive.log.file=hive.log<br />log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender<br /><br />log4j.appender.DRFA.File=${hive.log.dir}/${hive.log.file}<br /><br />log4j.appender.DRFA.DatePattern=.yyyy-MM-dd<br /><br />querylog配置：？？<br />&lt;property&gt;<br />   &lt;name&gt;hive.querylog.location&lt;/name&gt;<br />   &lt;value&gt;${system:java.io.tmpdir}/${system:user.name}&lt;/value&gt;<br />   &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;<br />&lt;/property&gt;<br /><br /><br />----------------hive的列的分割符------------------<br />##hive&gt;! cmd;  可以执行<br />hive&gt;! pwd;<br />##操作hdfs文件系统：<br />hive&gt;dfs -ls /user/hive/warehouse;<br /><br />hive的默认字段分割符： ^A  (ctrl+v,ctrl+A) 。而不是tab<br />tab<br />,<br />|<br /><br />^A 001  \001 \u0001 (001不能写成00001、01、1、0001)<br />^B 002<br />^C 003<br /><br />create table if not exists ua1(<br />id int,<br />uname string<br />)<br />row format delimited fields terminated by '\001'<br />;<br /><br />create table if not exists ua2(<br />id int,<br />uname string<br />)<br />row format delimited fields terminated by '\0001'<br />;<br /><br />load data local inpath '/home/ua2' into table ua2;<br /><br /><br /><br />#####---------hive 压缩 -----------------<br />map端的压缩：<br />mapreduce.map.output.compress=true;  #是否对map输出进行压缩<br />mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec;<br />hive.exec.compress.intermediate=false #是否开启中间压缩<br />hive.intermediate.compression.codec=;<br />hive.intermediate.compression.type=NONE|RECORD|BLOCK;<br /><br />insert overwrite local directory '/home/out/01'<br />row format delimited fields terminated by '\t'<br />select * from log9<br />;<br /><br /><br />reduce端的压缩：<br />hive.exec.compress.output=false; #默认最终输出不压缩true<br />mapred.output.compression.codec=org.apache.hadoop.io.compress.DefaultCodec;<br />设置压缩类型？？<br /><br /><br />1.hql<br />hive.exec.compress.output=true;<br />insert overwrite local directory '/home/out/03'<br />row format delimited fields terminated by '\t'<br />select * from user1<br />;<br />hive.exec.compress.output=false;<br /><br /><br />#通过squenceFile压缩：<br />创建表的时候保存文件类型为sequencefile:<br /><br />局部压缩：(通过在hql文件中或者是cli端设置属性)<br /><br /><br />全局压缩：(直接通过配置文件hive-site.xml)<br /><br /><br /><br />------------------hive 存储格式<br />hive的存储格式通常是三种：textfile 、 sequencefile 、 rcfile 、 orc 、自定义<br />set hive.default.fileformat=TextFile;<br />默认存储格式为：textfile<br />textFile:普通文本存储，不进行压缩。查询效率较低。<br /><br />sequencefile:hive提供的二进制序列文件存储，天生压缩。<br /><br />##sequeceFile 和 rcfile都不允许使用load方式加载数据。需要使用insert 方式插入<br /><br /><br />##默认支付压缩、分割，使用便捷、写和查询较快。sequencefile和压缩属性可以搭配使用。<br />create table if not exists seq1(<br />id int,<br />name string<br />)<br />row format delimited fields terminated by '\t'<br />lines terminated by '\n'<br />stored as sequencefile<br />;<br /><br />###加载数据 不ok<br />load data local inpath '/home/user' into table seq1;<br />###加载数据 ok<br />insert into table seq1<br />select * from user1<br />;<br /><br /><br />rcfile：rcfile可以进行行列混合压缩，将附近的列和行的数据尽量保存到相同的块里面，该存储格式会提高查询效率，但是写数据较慢。该方式和gzcodeC压缩属性结合不是很好()<br />set mapred.output.compression=true;<br />set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;<br /><br />##创建rcfile表：<br />create table if not exists rc1(<br />id int,<br />name string<br />)<br />row format delimited fields terminated by '\t'<br />stored as rcfile<br />;<br /><br />create table if not exists rc2(<br />id int,<br />name string<br />)<br />row format delimited fields terminated by '\t'<br />stored as rcfile<br />;<br /><br />###加载数据 不ok<br />load data local inpath '/home/user' into table rc1;<br />###加载数据 ok<br />insert into table rc2<br />select * from user1<br />;<br /><br /><br />存储自定义：<br />数据：<br />seq_yd元数据文件：<br />aGVsbG8saGl2ZQ==<br />aGVsbG8sd29ybGQ=<br />aGVsbG8saGFkb29w<br />seq_yd文件为base64编码后的内容，decode后数据为：<br /><br />hello,hive<br />hello,world<br />hello,hadoop<br /><br /><br />create table cus(str STRING)  <br />stored as  <br />inputformat 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat'  <br />outputformat 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat';  <br /><br />LOAD DATA LOCAL INPATH '/home/cus' INTO TABLE cus;<br /><br />###通常是使用 defaultCodec + rcfile搭配效率最好<br /><br />---------------------hive的运行方式--------------<br />hive的属性设置：<br />1、在cli端设置  (只针对当前的session)<br />3、在java代码中设置  (当前连接)<br />2、在配置文件中设置  (所有session有效)<br /><br />设置属性的优先级依次降低。<br />cli端只能设置非hive启动需要的属性。(log属性,元数据连接属性)<br /><br /><br />查找所有属性：<br />hive&gt;set;<br />查看当前属性的值：通常是hadoop<br />hive&gt; set -v;<br />模糊查找属性：<br />hive -S -e "set" | grep current;<br />hive -S -e "set" | grep index;<br /><br /><br />###hive变量：system 、 env 、hivevar 、hiveconf<br />system ：系统级别环境变量(jvm、hadoop等)，可读可写<br />hive&gt; set system:min.limit = 3;<br />hive&gt; set system:min.limit;<br />system:min.limit=3<br /><br />env：环境变量 (HADOOP_HOME)，只读不能写。<br />hive&gt; set env:PWD;<br />env:PWD=/usr/local/hive-1.2.1<br /><br />hivevar：自定义临时变量(可读可写)<br />hive&gt; set hivevar:min.limit=3;<br />hive&gt; set hivevar:min.limit;<br />hivevar:min.limit=3<br />hive&gt; set hivevar:min.limit=2;<br />hive&gt; set hivevar:min.limit;<br />hivevar:min.limit=2<br /><br />hiveconf:自定义临时属性变量(可读可写)<br />hive&gt; set hiveconf:max.limit=10;<br />hive&gt; set hiveconf:max.limit;<br />hiveconf:max.limit=10<br />hive&gt; set hiveconf:max.limit=6;<br />hive&gt; set hiveconf:max.limit;<br />hiveconf:max.limit=6<br /><br />hive的运行方式：<br />1、cli端运行 (临时统计、开发)<br />2、hive -S -e "hql 语句";  (适合单个hql的query语句)<br />3、hive -S -f /hql文件;  (hql文件的脚本) <br /><br />#不带参数<br />hive -S -e "use qf1603;select * from user1;"<br />hive -S -f /home/su.hql;<br /><br />hive在0.9版本以前是不支持的-f 带参数的执行：<br />hive --hivevar min_limit=3 -hivevar -hivevar t_n=user1 -e 'use qf1603;select * from ${hive:t_n} limit ${hivevar:min_limit};'<br />hive --hiveconf min_lit=3 -e "use qf1603;select * from user1 limit ${hiveconf:min_lit};"<br />hive -S --hiveconf t_n=user1 --hivevar min_limit=3 -f ./su.hql<br /><br /><br />hive中注释：<br />--注释内容<br /><br />insert overwrite local directory '/home/out/05'<br />select * from user1 limit 3;<br /><br /><br /><br />###----------------------hive 优化--------------<br />1、环境优化(linux 句柄数、应用内存分配、是否负载等)<br />2、应用配置属性方面的优化。<br />3、代码优化(hql，尝试换一种hql的写法)。<br /><br /><br />1、学会看explain<br />explain :显示hql查询的计划。<br />explain extended :显示hql查询的计划。还会显示hql的抽象表达式树。(就是解释器干得事)<br /><br />explain select * from user1;<br />explain extended select * from user1;<br /><br />一个hql语句将会有一个或者多个stage构成。每一个stage相当于一个mr的job，<br />stage可以是一个Fetch 、 map join 、 limit 等操作。<br />每一个stage都会按照依赖关系依次执行，没有依赖关系的可以并行执行。<br /><br /><br /><br />2、对limit的优化：<br />hive.limit.row.max.size=100000<br />hive.limit.optimize.limit.file=10<br />hive.limit.optimize.enable=false<br /><br />3、对join的优化：<br />永远是小表驱动大表(小结果集驱动大结果集)<br />必要时候使用小表标识 /*+STREAMTABLE(小表别名)*/<br />将业务调整为能尽量使用map-side join:<br />hive.auto.convert.join:<br />smalltable: <br />尽量避免笛卡尔积的join查询，即便有咯也需要使用on 或者where 来过滤。<br />hive目前的join 只支持等值连接(= and)。其它的不行<br /><br />4、使用hive本地模式(在一个jvm里面运行)<br />hive.exec.mode.local.auto=false<br />hive.exec.mode.local.auto.inputbytes.max=134217728<br />hive.exec.mode.local.auto.input.files.max=4<br /><br />5、hive并行执行(stage之间没有相互依赖关系的可以并行执行)<br />hive.exec.parallel=false <br />hive.exec.parallel.thread.number=8<br /><br />6、严格模式：<br />hive提供的严格模式阻挡三种查询：<br />1、<br />2、<br />3、join查询语句，不带on条件 或者 where条件。<br /><br />7、设置mapper 和 reduce个数<br />mapper个数太多，启动耗时，个数太少，资源利用不充分<br />reducer个数太多，启动耗时，个数太少，资源利用不充分<br /><br /><br />mapper个数：<br />手动设置：<br />set mapred.map.tasks=2;<br /><br />适当调整块大小，从而改变分片数，来改变mapper个数：<br /><br />通过合并文件小文件来减少mapper个数：<br />set mapred.max.split.size=25600000; 256M<br />set mapred.min.split.per.node=1<br />set mapred.min.split.per.rack=1<br />set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;<br /><br /><br /><br />reducer个数(通常手动设置)：<br />set mapreduce.job.reduces=-1;<br /><br /><br />8、hive使用jvm重用<br />mapreduce.job.jvm.numtasks=1 <br />set mapred.job.reuse.jvm.num=8;  ##jvm里运行task的任务数<br /><br /><br />9、数据倾斜(查看：Hive优化.docx文档)<br />数据倾斜：数据某列的值分布不均匀。<br />造成数据倾斜的原因：<br />1、原本数据就倾斜<br />2、hql语句可能造成<br />3、join 极容易造成<br />4、count(distinct col)<br />5、group by语句也容易<br /><br />解决方法：<br />1、如果数据本身倾斜，看数据能否直接分离(找到倾斜的数据)<br />2、将倾斜的数据单独出来计算，然后和正常的数据进行union all<br />3、将倾斜的数据赋予随机数来进行join查询，均衡每个task的任务量。<br />4、试图不变需求改写hql语句。<br /><br />倾斜解决的几个属性设置：<br />hive.map.aggr=true<br />hive.groupby.skewindata=false <br />hive.optimize.skewjoin=false  <br /><br /><br />10、job数量的控制<br />连接查询的on中的连接字段类型尽可能相同。<br />通常是一个简单hql语句生成一个job，有join 、limit 、group by 都将有可能会生成一个独立job。<br /><br />select<br />u.uid,<br />u.uname<br />from user1 u<br />where u.uid in (select l.uid from login l where l.uid=1 limit 1)<br />;<br /><br />select<br />u.uid,<br />u.uname<br />from user1 u<br />join login l<br />on u.uid = l.uid<br />where l.uid = 1<br />;<br /><br /><br />分区 、分桶 、索引 这些本身就是hive的一种优化。<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></div></div>
</body></html>